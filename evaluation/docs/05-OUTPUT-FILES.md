# Output Files Reference

This document provides a detailed breakdown of every file generated by the agent evaluation pipeline.

---

## Output Folder Structure

Each evaluation run creates a single timestamped folder. All commands add to this folder:

```
eval/results/
└── 20260114_143022/                      # One evaluation run
    ├── eval_summary.json                 # Aggregated metrics (from evaluate)
    ├── question_answer_log.md            # Q&A transcript (from analyze)
    ├── gemini_analysis.md                # AI root cause analysis (from analyze)
    └── raw/
        ├── processed_interaction_*.csv   # Interaction data (from convert/interact)
        ├── evaluation_results_*.csv      # Full eval results (from evaluate)
        └── gemini_prompt.txt             # Debug: prompt sent to Gemini
```

---

## Phase 1: Data Collection

### From `agent-eval convert` (Path A: Simulation)

| File | Description |
| :--- | :--- |
| `processed_interaction_sim.csv` | Converted data from ADK simulator traces |

### From `agent-eval interact` (Path B: Live API)

| File | Description |
| :--- | :--- |
| `processed_interaction_<app_name>.csv` | Interaction data from live agent API |

### Processed CSV Columns

| Column | Description | Used By |
| :--- | :--- | :--- |
| `question_id` | Unique ID for the test case | All metrics |
| `user_inputs` | The prompt(s) sent to the agent (JSON list) | LLM metrics as `prompt` |
| `final_response` | The agent's final text response | LLM metrics as `response` |
| `reference_data` | Ground truth for grading (from Golden Dataset) | Reference-based metrics |
| `session_id` | UUID of the conversation session | Debugging |
| `extracted_data` | JSON with `state_variables`, `tool_interactions` | Custom metrics |
| `session_trace` | Full OpenTelemetry trace | Deterministic metrics |
| `trace_summary` | Simplified list of agent/tool calls | Trajectory analysis |

---

## Phase 2: Evaluation

### From `agent-eval evaluate`

| File | Description |
| :--- | :--- |
| `eval_summary.json` | Aggregated statistics (mean scores, latencies, costs) |
| `evaluation_results_*.csv` | Full spreadsheet with all data + evaluation scores |

### `eval_summary.json` Structure

```json
{
  "experiment_id": "eval-20260114_143022",
  "run_type": "baseline",
  "test_description": "Customer Service Agent v1.0",
  "overall_summary": {
    "deterministic_metrics": {
      "latency_metrics.total_latency_seconds": 12.5,
      "tool_success_rate.tool_success_rate": 0.92,
      ...
    },
    "llm_based_metrics": {
      "response_correctness": 4.2,
      "tool_usage_accuracy": 3.8,
      ...
    }
  },
  "per_question_summary": [
    {
      "question_id": "q_billing_01",
      "runs": 1,
      "deterministic_metrics": { ... },
      "llm_metrics": {
        "response_correctness": {
          "score": 4.0,
          "explanation": "The response correctly addressed...",
          "input": { "prompt": "...", "response": "..." }
        }
      }
    }
  ]
}
```

### Deterministic Metrics (Calculated Automatically)

| Metric Category | Fields |
| :--- | :--- |
| `latency_metrics` | `total_latency_seconds`, `average_turn_latency_seconds`, `time_to_first_response_seconds` |
| `cache_efficiency` | `cache_hit_rate`, `total_cached_tokens`, `total_fresh_prompt_tokens` |
| `thinking_metrics` | `reasoning_ratio`, `total_thinking_tokens`, `total_output_tokens` |
| `tool_utilization` | `total_tool_calls`, `unique_tools_used`, `tool_counts` |
| `tool_success_rate` | `tool_success_rate`, `failed_tool_calls`, `failed_tools_list` |
| `context_saturation` | `max_total_tokens`, `peak_usage_span` |
| `agent_handoffs` | `total_handoffs`, `unique_agents_count`, `agents_invoked_list` |

---

## Phase 3: Analysis

### From `agent-eval analyze`

| File | Description |
| :--- | :--- |
| `question_answer_log.md` | Human-readable transcript showing Question → Expected → Actual → Score |
| `gemini_analysis.md` | AI-generated root cause analysis identifying code-level issues |
| `gemini_prompt.txt` | Debug file showing the exact prompt sent to Gemini |

### `question_answer_log.md` Example

```markdown
# Evaluation Results: Customer Service Agent

## Question: q_billing_01
**User Input:** "I have a billing question about my last invoice"

**Expected Response:** "I can help you with billing inquiries..."

**Actual Response:** "Let me look up your billing information..."

### Metrics
| Metric | Score | Explanation |
|--------|-------|-------------|
| response_correctness | 4/5 | Response addressed the query but... |
| tool_usage_accuracy | 5/5 | Correctly used billing_lookup tool |
```

### `gemini_analysis.md` Example

```markdown
# Root Cause Analysis

## Executive Summary
The agent scored 3.2/5 on response correctness due to...

## Critical Issues
1. **Tool Selection Error** (affects 3 test cases)
   - File: `customer_service/tools/billing.py:45`
   - Issue: The `lookup_invoice` tool returns incomplete data when...

## Recommendations
1. Update the billing tool to include...
```

---

## Debug Artifacts

These files help troubleshoot issues without opening large CSVs:

| File Pattern | Location | Purpose |
| :--- | :--- | :--- |
| `session_<q_id>_<s_id>.json` | `raw/` | Standalone dump of agent's Session State |
| `trace_<q_id>_<s_id>.json` | `raw/` | Standalone dump of Execution Trace |
| `temp_consolidated_questions.json` | `raw/` | Questions after merging/sampling |
| `temp_consolidated_metrics.json` | `raw/` | Metrics used for the run |

---

## File Relationships

```
                    ┌─────────────────────────┐
                    │   Input Data            │
                    │   (scenarios or golden) │
                    └───────────┬─────────────┘
                                │
                    ┌───────────▼─────────────┐
                    │   processed_*.csv       │ ◄── Phase 1
                    │   (interaction data)    │
                    └───────────┬─────────────┘
                                │
              ┌─────────────────┼─────────────────┐
              │                 │                 │
   ┌──────────▼──────────┐ ┌────▼────┐ ┌─────────▼─────────┐
   │ evaluation_*.csv    │ │ eval_   │ │ deterministic     │ ◄── Phase 2
   │ (scores + data)     │ │ summary │ │ metrics           │
   └─────────────────────┘ │ .json   │ └───────────────────┘
                           └────┬────┘
                                │
              ┌─────────────────┼─────────────────┐
              │                 │                 │
   ┌──────────▼──────────┐ ┌────▼────────────┐   │
   │ question_answer_    │ │ gemini_         │   │ ◄── Phase 3
   │ log.md              │ │ analysis.md     │   │
   └─────────────────────┘ └─────────────────┘   │
```

---

## Related Documentation

- [01-GETTING-STARTED.md](01-GETTING-STARTED.md) - Quick start guide
- [03-METRICS-GUIDE.md](03-METRICS-GUIDE.md) - Defining metrics
- [04-CLI-REFERENCE.md](04-CLI-REFERENCE.md) - Command reference
