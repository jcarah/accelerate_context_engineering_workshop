# Refactoring Report: ADK Evaluation Framework

**Date:** January 12, 2026
**Status:** Phase 1 (Structural Refactoring) Complete

---

## 1. Mission & Objectives

The primary goal of our current refactoring efforts is to transform the `evaluation/` directory from a collection of loose scripts into a robust, installable Python package with a unified CLI (`adk-eval`).

### Key Drivers:
*   **Usability:** Replacing numbered scripts (`01_agent_interaction.py`, etc.) with semantic CLI commands (`adk-eval run`, `adk-eval evaluate`) makes the tool easier to use and document.
*   **Decentralization:** Moving evaluation assets (datasets, metrics) out of the central `evaluation/` folder and into the specific agent directories (`customer-service/eval/`, etc.). This ensures that agents are self-contained units.
*   **Maintainability:** Separating core business logic (`core/`) from interface logic (`cli/`) and configuration allows for better testing and future extensibility.
*   **Workflow Unification:** Bridging the gap between "Live/Remote" testing (hitting an API) and "Simulation" testing (using ADK `eval_history`) by standardizing the data format.

---

## 2. Deep Dive: Dual Interaction Workflows

A critical part of this refactor was establishing a unified pipeline for two distinct interaction methods. The goal is that the **Evaluator** (Step 2) should never care *how* the conversation happened, only that it has a standardized dataset to grade.

### Route A: The Live/Remote Path (`adk-eval run`)
*   **Mechanism:** The script acts as a client, sending HTTP requests to a running agent service (Localhost, Cloud Run, etc.).
*   **Data Collection:** It captures responses and explicitly polls the agent's Trace API to get execution details.
*   **Output:** Generates `processed_interaction_*.csv` with full session state and traces.

### Route B: The Simulator Path (`adk eval` -> `adk-eval convert`)
*   **Context:** The ADK CLI has a native `adk eval` command that runs conversations locally using a user simulator. However, out-of-the-box, it only supports a limited set of metrics (Safety, Hallucination).
*   **The Gap:** We want to apply our full suite of "Golden Path" metrics (Deterministic latency/cost + Custom LLM Business Logic) to these simulations.
*   **The Solution (`converters.py`):** We ported the logic from `convert_adk_history_to_dataset.py` into `evaluation/core/converters.py`.
    *   **Function:** It reads the `.adk/eval_history` JSON files generated by the simulator.
    *   **Adaptation:** It reshapes this data into the *exact same* DataFrame structure as Route A. This involves synthesizing OpenTelemetry-style traces from the flat event log so that our deterministic metrics (which expect spans) can still function.
*   **Result:** This unlocks "Offline Evaluation" where developers can test agents locally without deploying a server, yet still get the full depth of the Evaluation Report.

---

## 3. Data Standardization Strategy: The Golden Dataset

While the **Evaluator** demands a rich "Golden Dataset" schema (containing reference trajectories, tool expectations, and metadata) to perform robust grading, many teams start with simpler assets:
*   **Raw Test Logs:** Simple JSON lists of user queries and expected tool calls (often used in unit tests).
*   **ADK Native Formats:** Simple turn-based representations.

To bridge this gap, we introduced the **`create-dataset`** command.
*   **The Problem:** The `adk-eval run` command requires a specific input format to correctly orchestrate interactions and map reference data. Manually migrating hundreds of simple test cases into this format is error-prone and tedious.
*   **The Solution:** The `create-dataset` command ingests these simpler formats (specifically raw test turns) and "upgrades" them into the standardized Golden Dataset schema required by the pipeline.
*   **Why it matters:** This lowers the barrier to entry significantly. Teams can take their existing `test_cases.json` or ADK-native logs, run `adk-eval create-dataset`, and immediately start using the full power of the evaluation pipeline without rewriting their data.

---

## 4. Architectural Changes

### Before
*   **Root:** Flat list of scripts (`01_...`, `02_...`, `03_...`).
*   **Logic:** Scattered across `scripts/` and root files.
*   **Data:** Centralized `datasets/` and `metrics/` folders requiring manual path management.
*   **Execution:** `python -m evaluation.01_agent_interaction ...`

### After (New Structure)
The `evaluation` folder is now a proper package structure:

```
evaluation/
├── cli/
│   └── main.py            # Unified Entry Point (argparse)
├── core/
│   ├── interactions.py    # Async API interaction logic (Path A)
│   ├── processor.py       # Log enrichment & trace analysis
│   ├── converters.py      # ADK History -> Evaluation Dataset conversion (Path B)
│   ├── evaluator.py       # Deterministic & LLM metrics engine
│   ├── analyzer.py        # Gemini-powered reporting
│   ├── agent_client.py    # HTTP Client for Agent API
│   ├── deterministic_metrics.py
│   └── data_mapper.py
├── __main__.py            # Module entry point
└── pyproject.toml         # Defines 'adk-eval' script
```

---

## 5. Asset Migration Strategy

We moved away from a monolithic `evaluation/datasets` folder. Assets are now co-located with the agents they test.

*   **Customer Service Agent:**
    *   Datasets: `customer-service/eval/datasets/golden_dataset.json`
    *   Metrics: `customer-service/eval/metrics/metric_definitions.json`
*   **Retail Strategy Agent:**
    *   Datasets: `retail-ai-location-strategy/eval/datasets/golden_dataset.json`
    *   Metrics: `retail-ai-location-strategy/eval/metrics/metric_definitions.json`

**Reasoning:** This allows developers to modify an agent and its specific tests/metrics in the same PR without touching the shared evaluation framework.

---

## 6. The New CLI: `adk-eval`

We implemented a unified CLI with five primary commands:

1.  **`run`**: Executes questions against a live agent URL.
    *   *Usage:* `adk-eval run --app-name customer_service ...`
2.  **`convert`**: Converts ADK simulation logs (`.adk/eval_history`) into the standard CSV format used for evaluation.
    *   *Usage:* `adk-eval convert --agent-dir ...`
3.  **`evaluate`**: Runs the scoring engine (Deterministic + LLM Judges) on the output of `run` or `convert`.
    *   *Usage:* `adk-eval evaluate --interaction-file ...`
4.  **`analyze`**: Generates human-readable Markdown logs and AI-powered root cause analysis reports.
    *   *Usage:* `adk-eval analyze --results-dir ...`
5.  **`create-dataset`**: Converts raw JSON logs (lists of turns) into the standardized "Golden Dataset" format required by `run`.
    *   *Usage:* `adk-eval create-dataset --input raw_log.json --output golden.json ...`

---

## 7. Completed Tasks Log

1.  **Core Logic Migration:** Moved all logic from `scripts/` and root files into `evaluation/core/`.
2.  **CLI Implementation:** Created `evaluation/cli/main.py` and registered it in `pyproject.toml`.
3.  **Cleanup:** Deleted old scripts (`01`, `02`, `03`), `utils/` folder, and `scripts/` folder.
4.  **Asset Move:** Moved JSON files to agent directories and removed `evaluation/datasets` & `evaluation/metrics`.
5.  **CI/CD Update:** Rewrote `evaluation.yml` to use the new `adk-eval` command structure and removed hardcoded references to specific projects where possible.
6.  **Test Stubbing:** Created `tests/core/` with import checks to ensure package integrity (full logic migration pending).
7.  **Dataset Converter:** Integrated `convert_test_to_golden.py` into `core/converters.py` and exposed it via the `create-dataset` CLI command.

---

## 8. Future Roadmap & Next Steps

### Immediate Actions Required
1.  **Documentation Update:** The files `DATASETS_GUIDE.md` and `METRICS_GUIDE.md` still reference the old folder structures and scripts. They need to be updated to reflect the `adk-eval` CLI and agent-specific paths.
2.  **Unit Test Migration:** The files in `evaluation/tests/core/` are currently stubs. We need to port the actual test logic from the deleted `tests/` files to ensure we haven't broken the deterministic metrics or data mappers.
3.  **End-to-End Verification:** Run a full cycle (`run` -> `evaluate` -> `analyze`) for the Customer Service agent to verify the refactor works in practice.

### Long-term Goals
*   **PyPI Publishing:** Package the tool so it can be installed via `pip install adk-eval` in other repos.
*   **Plugin System:** Allow agents to define custom deterministic metrics without modifying the core package.
*   **Dashboarding:** Build a simple Streamlit or web view for the `eval_summary.json` results.