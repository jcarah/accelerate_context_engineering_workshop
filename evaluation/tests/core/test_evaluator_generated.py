import json
import shutil
import tempfile
import unittest
from pathlib import Path
from unittest.mock import MagicMock, patch

import pandas as pd

# Import the class under test
from evaluation.core.evaluator import Evaluator


@patch("evaluation.core.evaluator.CONFIG")
@patch("evaluation.core.evaluator.aiplatform")
@patch("evaluation.core.evaluator.Client")
class TestEvaluator(unittest.TestCase):
    def setUp(self):
        self.test_dir = tempfile.mkdtemp()
        self.results_dir = Path(self.test_dir) / "results"
        self.config = {
            "metric_filters": {},
            "input_label": "test_label",
            "test_description": "test_desc"
        }

    def tearDown(self):
        shutil.rmtree(self.test_dir)

    def test_initialization(self, mock_client, mock_aiplatform, mock_config):
        mock_config.GOOGLE_CLOUD_PROJECT = "test-project"
        mock_config.GOOGLE_CLOUD_LOCATION = "us-central1"
        
        Evaluator(self.config)
        
        # Verify the mocked library was initialized
        mock_aiplatform.init.assert_called_with(project="test-project", location="us-central1")

    @patch("evaluation.core.evaluator.load_and_consolidate_metrics")
    @patch("evaluation.core.evaluator.evaluate_deterministic_metrics")
    @patch("evaluation.core.evaluator.save_metrics_summary")
    def test_evaluate_adk_score_integration_jsonl(
        self, 
        mock_save_summary, 
        mock_det_metrics, 
        mock_load_metrics,
        mock_client,
        mock_aiplatform,
        mock_config
    ):
        """
        Validates that ADK scores are correctly extracted from JSONL input
        and passed to the summary generation.
        """
        mock_config.GOOGLE_CLOUD_PROJECT = "test-project"
        mock_config.MAX_WORKERS = 1
        
        # Setup Input Data with ADK Scores in JSONL format
        input_data = [
            {
                "question_id": "q1",
                "final_session_state": {},
                "session_trace": [],
                "agents_evaluated": ["agent1"],
                "adk_score.hallucination": 0.1,
                "adk_score.safety": 1.0
            }
        ]
        input_file = Path(self.test_dir) / "input.jsonl"
        
        # Write as JSONL
        with open(input_file, "w") as f:
            for record in input_data:
                f.write(json.dumps(record) + "\n")

        # Mock Dependencies
        mock_load_metrics.return_value = {} 
        mock_det_metrics.return_value = {}

        # Run
        evaluator = Evaluator(self.config)
        evaluator.evaluate(input_file, ["metrics.json"], self.results_dir)

        # Verify
        args, _ = mock_save_summary.call_args
        result_df = args[0]
        
        # Parse the JSON string in the 'eval_results' column
        first_row_results = json.loads(result_df.iloc[0]["eval_results"])
        
        # Assert ADK scores are present
        self.assertIn("hallucination", first_row_results)
        self.assertEqual(first_row_results["hallucination"]["score"], 0.1)
        self.assertEqual(first_row_results["safety"]["score"], 1.0)

    @patch("evaluation.core.evaluator.load_and_consolidate_metrics")
    @patch("evaluation.core.evaluator.concurrent.futures.as_completed")
    @patch("evaluation.core.evaluator.concurrent.futures.ThreadPoolExecutor")
    def test_evaluate_llm_metrics_execution_jsonl(
        self,
        mock_executor_cls,
        mock_as_completed,
        mock_load_metrics,
        mock_client,
        mock_aiplatform,
        mock_config
    ):
        """Test LLM metrics execution using JSONL input."""
        mock_config.GOOGLE_CLOUD_PROJECT = "test-project"
        mock_config.MAX_WORKERS = 2
        
        # Input data
        input_file = Path(self.test_dir) / "input.jsonl"
        input_data = [
            {
                "question_id": "q1", 
                "agents_evaluated": ["agent1"],
                "request": "req", 
                "response": "res"
            }
        ]
        with open(input_file, "w") as f:
            for record in input_data:
                f.write(json.dumps(record) + "\n")
        
        # Mock Metrics
        mock_load_metrics.return_value = {
            "test_metric": {
                "metric_type": "llm",
                "agents": ["agent1"],
                "template": "test prompt"
            }
        }
        
        # Setup Future Mock
        mock_future = MagicMock()
        mock_parsed_df = pd.DataFrame([{"original_index": 0, "test_metric/score": 5.0}])
        mock_input_df = pd.DataFrame(input_data)
        # return (parsed_results_df, metric_name, input_dataset_df)
        mock_future.result.return_value = (mock_parsed_df, "test_metric", mock_input_df)

        # Setup Executor Mock
        mock_executor_instance = mock_executor_cls.return_value.__enter__.return_value
        mock_executor_instance.submit.return_value = mock_future
        mock_as_completed.return_value = [mock_future]
        
        evaluator = Evaluator(self.config)
        evaluator.evaluate(input_file, [], self.results_dir)
        
        # Verify executor was used
        mock_executor_instance.submit.assert_called()
        mock_as_completed.assert_called()

if __name__ == "__main__":
    unittest.main()