# Evaluation Pipeline Outputs

This document provides a detailed breakdown of every file generated by the agent evaluation pipeline. All results are stored in: `evaluation/results/<app_name>/<YYYYMMDD_HHMMSS>/`.

---

## üìÇ 1. Data Collection (`01_agent_interaction.py`)

This step produces the foundational data for everything that follows. It runs in two sub-phases (Run and Process).

### **Core Data Files**

| File Name Pattern | Created By | Description |
| :--- | :--- | :--- |
| `interaction_*.csv` | `run_interactions.py` | **Raw Runtime Log.** Contains basic session info and the user prompts. It does NOT have traces or state yet. |
| **`processed_interaction_*.csv`** | `process_interactions.py` | **Enriched Log.** The final output of Step 1. Contains full traces, state variables, and latency data. |

### **Processed CSV Column Reference**

| Column | Source | Description |
| :--- | :--- | :--- |
| **`question_id`** | Input JSON | Unique ID for the test case (e.g., `q_billing_01`). |
| **`user_inputs`** | Input JSON | The prompt(s) sent to the agent. |
| **`reference_data`** | Input JSON | Ground truth for grading (e.g., expected SQL query, required tools). |
| **`session_id`** | Agent Runtime | The UUID of the conversation session. |
| **`final_session_state`** | Agent Runtime | **Crucial.** The full JSON dump of the agent's memory at the end of the run. |
| **`extracted_data`** | Processing | Specific variables parsed from `final_session_state` (e.g., `customer_profile`). |
| **`session_trace`** | Trace API | The full OpenTelemetry trace. Source for deterministic metrics. |
| **`trace_summary`** | Processing | A simplified list of `[AgentName, ToolName]` calls. |
| **`latency_data`** | Processing | Breakdown of time spent in LLM vs. Tools vs. Network. |

---

## üìä 2. Evaluation (`02_agent_run_eval.py`)

This step adds a new column, `eval_results`, to the CSV and generates a summary.

### **Core Evaluation Files**

| File Name Pattern | Description |
| :--- | :--- |
| **`evaluation_results_*.csv`** | The final spreadsheet including all interaction data PLUS the evaluation scores and LLM explanations. |
| **`eval_summary.json`** | Machine-readable aggregated statistics (mean scores, costs, latencies). |

---

## üìù 3. Insight Phase (`03_analyze_eval_results.py`)

### **Final Report Files**

| File Name Pattern | Description |
| :--- | :--- |
| **`question_answer_log.md`** | Human-readable transcript. Shows Question -> Expected -> Actual -> Score for every test case. |
| **`gemini_analysis.md`** | Technical diagnosis generated by Gemini 2.0. Identifies code-level root causes for failures. |

---

## üêû 4. Support & Debugging Artifacts

When things go wrong, these files help you isolate the issue without opening the massive CSVs.

| File Name Pattern | Source | Purpose |
| :--- | :--- | :--- |
| `session_<q_id>_<s_id>.json` | `process_interactions.py` | A standalone dump of the agent's **Session State**. Open this to check if the agent saved variables correctly. |
| `trace_<q_id>_<s_id>.json` | `process_interactions.py` | A standalone dump of the **Execution Trace**. Open this to see exactly which tools or models were called. |
| `temp_consolidated_questions.json` | `01_agent_interaction.py` | The set of questions after merging multiple input files and applying sampling. |
| `temp_consolidated_metrics.json` | `02_agent_run_eval.py` | The set of metrics (prompts/rubrics) used for the run after merging JSON files. |

---

## üõ†Ô∏è Summary of Metric Categories

| Metric Category | Source | Description |
| :--- | :--- | :--- |
| **Deterministic** | Local Trace Analysis | Latency, Cost, Tool Success Rate. Defined in `scripts/deterministic_metrics.py`. |
| **Managed** | Vertex AI SDK | Predefined Google rubrics like `TOOL_USE_QUALITY` or `GENERAL_QUALITY`. |
| **Custom LLM** | Your JSON Rubric | Specific business logic like `state_management_fidelity`. Defined in `metrics/*.json`. |
