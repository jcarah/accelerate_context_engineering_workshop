# Guide to Evaluation Datasets

This document explains the schema and structure of the **Golden Datasets** used to evaluate agents. These JSON files serve as the "Source of Truth" for your test cases.

## ðŸ“„ File Location & Naming

*   **Location:** `evaluation/datasets/`
*   **Naming Convention:** `<agent_name>_golden.json` (e.g., `customer_service_golden.json`)

---

## ðŸ› ï¸ Schema Reference

The root object must contain a `golden_questions` list. Each item represents a single test session.

```json
{
  "golden_questions": [
    {
      "id": "q_billing_01",
      "user_inputs": ["Hi", "I have a billing question"],
      "agents_evaluated": ["customer_service"],
      "metadata": { ... },
      "reference_data": { ... }
    }
  ]
}
```

### Field Definitions

| Field | Type | Required | Description |
| :--- | :--- | :--- | :--- |
| **`id`** | `string` | Yes | Unique identifier for the test case. Used in reports and logs. |
| **`user_inputs`** | `list[str]` | Yes | The sequence of messages the user sends to the agent. Simulates a multi-turn conversation. |
| **`agents_evaluated`** | `list[str]` | Yes | List of agent names expected to be involved (for routing checks). |
| **`metadata`** | `dict` | No | Arbitrary tags for filtering/analysis (e.g., `complexity: "high"`, `topic: "refund"`). |
| **`reference_data`** | `dict` | Yes | The ground truth used by metrics to grade the agent. |

---

## ðŸŽ¯ Reference Data Structure

The `reference_data` object contains the expected behaviors. You can add custom keys here, but the standard ones are:

### 1. `reference_tool_interactions`
A list of tools the agent *must* call to be considered correct.

```json
"reference_tool_interactions": [
    {
        "tool_name": "access_cart_information",
        "input_arguments": { "customer_id": "123" }
    },
    {
        "tool_name": "generate_qr_code",
        "input_arguments": { "discount_value": 10 }
    }
]
```

### 2. `reference_trajectory`
The expected sequence of agents or major steps. Used by the `trajectory_accuracy` metric.

```json
"reference_trajectory": [
    "GreetingAgent",
    "BillingSupportAgent",
    "PaymentTool"
]
```

### 3. `reference_state_variables`
The expected values in the agent's memory (session state) at the end of the run. Used by the `state_management_fidelity` metric.

```json
"reference_state_variables": {
    "customer_intent": "refund",
    "refund_amount": 50.00,
    "target_date": "2024-01-01"
}
```

---

## ðŸ› ï¸ How to Create a Dataset

### Option A: Manual Creation
Copy the structure above and write your JSON file manually. Best for small, precise test suites.

### Option B: Convert from Logs
If you have existing conversation logs, use the helper script:

```bash
uv run python evaluation/scripts/convert_test_to_golden.py \
  --input raw_logs.json \
  --output datasets/my_new_golden.json \
  --agent my_agent
```

*Note: The input `raw_logs.json` must be a list of dicts with `query` and optional `expected_tool_use` fields.*

---

## ðŸš€ Simulation Path Dataset (CSV)

When using the **Simulation Path** (ADK Eval -> Convert -> Run), the dataset is a CSV file generated by `convert_adk_history_to_dataset.py`. This serves as the input for the evaluation script.

### Key Columns

| Column | Description | Usage in Metrics |
| :--- | :--- | :--- |
| **`user_inputs`** | JSON list of user strings (e.g., `["Hi", "Help"]`). | Used as the `prompt` for LLM judges. |
| **`final_response`** | The text of the agent's last message. | Used as the `response` for managed metrics (Safety, Quality). |
| **`sub_agent_trace`** | JSON list of agent turns `{name, text}`. | Used for `response_correctness` (checking consistency). |
| **`trace_summary`** | Plain text log of the conversation. | Used for `trajectory_accuracy` (checking flow). |
| **`extracted_data`** | JSON dict containing `state_variables` and `tool_interactions`. | Used for `state_management_fidelity` and `tool_usage_accuracy`. |
| **`session_trace`** | JSON OpenTelemetry-style span list. | Used for all **Deterministic Metrics** (Cost, Latency). |

**Note:** `02_agent_run_eval.py` automatically detects and handles both the JSON (Golden) and CSV (Simulation) formats.
