# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Evaluate Agent Interactions using Vertex AI Evaluation.

This script takes the output from the `01_agent_interaction.py` script (a CSV file
of agent interactions) and runs evaluations on it based on a set of metric
definitions. It supports both LLM-based and deterministic metrics.

Key functionalities:
- Loads agent interaction data from a CSV file.
- Consolidates metric definitions from one or more JSON files, applying prefixes.
- Filters metrics based on user-provided criteria (e.g., by metric type or agent).
- Prepares the data for evaluation by normalizing nested JSON and mapping columns.
- Runs evaluations using the Vertex AI SDK's EvalTask.
- Calculates deterministic metrics for objective, reproducible measurements.
- Aggregates results, generates a summary, and saves them to CSV and JSON files.
- Optionally, uploads the final results to a BigQuery table.

This script is designed to be modular and configurable, allowing for flexible
evaluation of different agents and scenarios.

How it connects with other components:
- `01_agent_interaction.py`: This script consumes the CSV output generated by
  `01_agent_interaction.py`.
- `schemas/eval_state_variables_schema.json`: This schema is used by
  `01_agent_interaction.py` to define which variables to extract from the agent's
  final session state into the `extracted_data` column of the interaction CSV.
- `evaluation/metrics/*.json`: These files define the metrics to be run. This
  script uses the `dataset_mapping` within these files to find the necessary data
  (from columns like `extracted_data.sql_explorer:generated_sql`) and map it to
  the placeholders in the evaluation prompt templates.
"""

import argparse
import json
import math
import os
import sys
import time
import uuid
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from google.cloud import aiplatform, bigquery
from google.cloud.exceptions import NotFound
from vertexai.evaluation import EvalTask, PointwiseMetric

# Add project root to Python path to allow importing from other modules.
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# Import deterministic metrics calculator and registry
from evaluation.scripts.deterministic_metrics import evaluate_deterministic_metrics, DETERMINISTIC_METRICS


def robust_json_loads(x: Any) -> Optional[Dict | List | str]:
    """
    Safely parse a JSON string, returning None for invalid or empty inputs.

    Args:
        x: The input to parse.

    Returns:
        The parsed JSON object (dict or list), the original input if it's already
        an object or not a valid JSON string, or None for empty inputs.
    """
    if x is None:
        return None
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str) or not x:
        return None
    try:
        return json.loads(x)
    except (json.JSONDecodeError, TypeError):
        return x


def extract_metadata_field(df: pd.DataFrame, field_name: str) -> Optional[Any]:
    """
    Extract a specific field from the 'question_metadata' JSON column.

    Args:
        df: The DataFrame containing the 'question_metadata' column.
        field_name: The name of the field to extract (e.g., 'tenant', 'tier').

    Returns:
        The extracted field value or None if not found or on error.
    """
    if df.empty or "question_metadata" not in df.columns:
        return None

    try:
        metadata_str = df.iloc[0]["question_metadata"]
        metadata = json.loads(metadata_str)
        return metadata.get(field_name, None)
    except (json.JSONDecodeError, KeyError, IndexError):
        return None


def load_and_consolidate_metrics(metric_files: List[str]) -> Dict[str, Any]:
    """
    Load and consolidate metric definitions from multiple JSON files.

    This function reads multiple metric definition files, extracts their metrics,
    and combines them into a single dictionary. It also handles a 'metric_prefix'
    key in the JSON files to prepend a prefix to each metric name, avoiding
    naming collisions.

    Args:
        metric_files: A list of file paths to the metric definition JSON files.

    Returns:
        A dictionary containing all consolidated metric definitions.
    """
    consolidated_metrics = {}
    print("--- Loading and Consolidating Metric Definitions ---")
    for file_path in metric_files:
        try:
            with open(file_path, "r") as f:
                data = json.load(f)
                prefix = data.get("metric_prefix", "")
                metrics = data.get("metrics", {})
                print(f"Loaded {len(metrics)} metrics from '{file_path}' with prefix '{prefix}'.")
                for name, definition in metrics.items():
                    # Prepend prefix, ensuring no leading underscore if prefix is empty.
                    new_name = f"{prefix}_{name}".lstrip("_")
                    if new_name in consolidated_metrics:
                        print(f"Warning: Duplicate metric name '{new_name}' found. Overwriting.")
                    consolidated_metrics[new_name] = definition
        except FileNotFoundError:
            print(f"Error: Metric file not found: {file_path}")
            sys.exit(1)
        except json.JSONDecodeError:
            print(f"Error: Could not parse JSON from file: {file_path}")
            sys.exit(1)
    print(f"Total consolidated metrics: {len(consolidated_metrics)}")
    return consolidated_metrics


def parse_metric_filters(filter_strings: Optional[List[str]]) -> Dict[str, List[str]]:
    """
    Parse metric filter strings from command-line arguments into a dictionary.

    Args:
        filter_strings: A list of filter strings in the format "key:value1,value2".
                        Example: ["metric_type:deterministic", "agents:sql_explorer"].

    Returns:
        A dictionary of parsed filters.
    """
    filters = {}
    if not filter_strings:
        return filters

    for filter_string in filter_strings:
        if ":" not in filter_string:
            print(f"Warning: Invalid filter format '{filter_string}'. Expected 'key:value1,value2'")
            continue

        key, values_str = filter_string.split(":", 1)
        values = [v.strip() for v in values_str.split(",")]

        if key in filters:
            filters[key].extend(values)
        else:
            filters[key] = values

    return filters


def filter_metrics_by_criteria(
    metric_definitions: Dict[str, Any], filters: Dict[str, List[str]]
) -> Dict[str, Any]:
    """
    Filter metric definitions based on specified criteria.

    Args:
        metric_definitions: The dictionary of all metric definitions.
        filters: A dictionary of filter criteria, where keys are attributes to
                 filter on (e.g., 'metric_type', 'agents') and values are lists
                 of accepted values.

    Returns:
        A dictionary of metric definitions that match the filter criteria.
    """
    if not filters:
        return metric_definitions

    filtered_metrics = {}
    for metric_name, metric_info in metric_definitions.items():
        matches_all_filters = True
        for filter_key, filter_values in filters.items():
            # Special handling for top-level keys like 'metric_type' and 'agents'
            if filter_key == "metric_type":
                metric_value = metric_info.get("metric_type", "llm")
            elif filter_key == "agents":
                metric_value = metric_info.get("agents", ["data_explorer_agent"])
            elif filter_key == "metrics":
                # Allow filtering by a list of specific metric names
                if metric_name not in filter_values:
                    matches_all_filters = False
                continue # Continue to next filter key
            else:
                metric_value = metric_info.get(filter_key)

            # Check for match
            if metric_value is None:
                matches_all_filters = False
                break
            
            # Standardize to list for comparison
            if not isinstance(metric_value, list):
                metric_value = [str(metric_value)]

            if not any(str(v) in filter_values for v in metric_value):
                matches_all_filters = False
                break

        if matches_all_filters:
            filtered_metrics[metric_name] = metric_info

    return filtered_metrics


def generate_summary_table(per_metric_summary: List[Dict[str, Any]]) -> str:
    """Formats a per-metric summary into a Markdown table."""
    headers = ["Metric Name", "Average Score", "Conversation Count"]
    # Calculate column widths
    max_name = max(len(h) for h in headers)
    max_score = len(headers[1])
    max_count = len(headers[2])

    for summary in per_metric_summary:
        max_name = max(max_name, len(summary["metric_name"]))
        max_score = max(max_score, len(f"{summary['average_score']:.4f}"))
        max_count = max(max_count, len(str(summary["conversation_count"])))

    # Create table
    header_line = f"| {headers[0]:<{max_name}} | {headers[1]:<{max_score}} | {headers[2]:<{max_count}} |"
    separator = f"| {'-' * max_name} | {'-' * max_score} | {'-' * max_count} |"
    rows = [header_line, separator]
    for summary in per_metric_summary:
        row = f"| {summary['metric_name']:<{max_name}} | {summary['average_score']:<{max_score}.4f} | {summary['conversation_count']:<{max_count}} |"
        rows.append(row)
    return "\n".join(rows)


def save_metrics_summary(
    df: pd.DataFrame,
    results_dir: Path,
    experiment_id: str,
    run_type: str,
    test_description: str,
) -> None:
    """
    Calculate and save a summary of metrics and latency.

    This function groups results by question, calculates average metrics and
    latency for each question, and then computes an overall summary across all
    questions. The final summary is saved to a JSON file.

    Args:
        df: The DataFrame containing the full evaluation results.
        results_dir: The directory to save the summary file in.
        experiment_id: The unique ID for this evaluation experiment.
        run_type: The type of run ('manual' or 'scheduled').
        test_description: The user-provided description of the test.
    """
    print("--- Generating Metrics Summary ---")
    # Group by question_id to calculate metrics per question
    grouped = df.groupby("question_id")

    all_question_summaries = []
    for question_id, group in grouped:
        # Average all metric scores for this group
        group["eval_results"] = group["eval_results"].apply(robust_json_loads)
        
        # Dictionary to store score and explanation for each metric
        metrics_data = {}

        for result_dict in group["eval_results"].dropna():
            if not isinstance(result_dict, dict):
                continue
            for metric_name, values in result_dict.items():
                if isinstance(values, dict):
                    # We assume one run per question as per instructions.
                    # We store the score and explanation directly.
                    # If multiple runs exist, this will effectively take the last one,
                    # but the logic prioritizes the structure: {metric: {score: x, explanation: y}}
                    
                    metric_entry = {}
                    
                    # Process Score
                    if "score" in values and values["score"] is not None:
                        try:
                            score = float(values["score"])
                            if not math.isnan(score):
                                metric_entry["score"] = score
                        except (ValueError, TypeError):
                            pass
                    
                    # Process Explanation
                    if "explanation" in values:
                        metric_entry["explanation"] = values["explanation"]

                    # Process Details (flatten them into the top level for consistency with overall summary if needed, 
                    # but for per-question, keeping them in details or just explanation is cleaner. 
                    # However, to maintain the flattening logic requested previously for 'average_metrics':
                    if "details" in values and isinstance(values["details"], dict):
                        for det_key, det_val in values["details"].items():
                            if isinstance(det_val, (int, float)) and not isinstance(det_val, bool):
                                # Add flattened details as separate metrics for visibility
                                metrics_data[f"{metric_name}.{det_key}"] = {
                                    "score": det_val,
                                    "explanation": f"Detail {det_key} for {metric_name}"
                                }
                    
                    if metric_entry:
                        metrics_data[metric_name] = metric_entry

        # Extract all metadata dynamically
        metadata = robust_json_loads(group.iloc[0].get("question_metadata", "{}"))
        if not isinstance(metadata, dict):
            metadata = {}

        # Create summary for this question
        question_summary = {
            "question_id": question_id,
            "runs": len(group),
            "metrics": metrics_data,
        }
        # Merge metadata into the summary
        question_summary.update(metadata)
        
        all_question_summaries.append(question_summary)

    # --- Calculate Overall and Per-Metric Summaries ---
    per_metric_scores = defaultdict(list)
    if all_question_summaries:
        for qs in all_question_summaries:
            for metric, data in qs["metrics"].items():
                if "score" in data:
                    per_metric_scores[metric].append(data["score"])

    final_overall_metrics = {
        metric: sum(scores) / len(scores) if scores else 0
        for metric, scores in per_metric_scores.items()
    }
    
    per_metric_summary = [
        {
            "metric_name": metric,
            "average_score": sum(scores) / len(scores) if scores else 0,
            "conversation_count": len(scores),
        }
        for metric, scores in per_metric_scores.items()
    ]
    # Sort by name for consistent output
    per_metric_summary.sort(key=lambda x: x["metric_name"])


    # --- Create and Print Summary Table ---
    summary_table = generate_summary_table(per_metric_summary)
    print("\n--- Per-Metric Evaluation Summary ---")
    print(summary_table)
    print("-------------------------------------\n")


    # Create the final summary JSON object
    output_json = {
        "experiment_id": experiment_id,
        "run_type": run_type,
        "test_description": test_description,
        "interaction_datetime": (
            df.iloc[0]["interaction_datetime"] if not df.empty else datetime.now().isoformat()
        ),
        "USER": df.iloc[0]["USER"] if not df.empty else None,
        "ADK_USER_ID": df.iloc[0]["ADK_USER_ID"] if not df.empty else None,
        "base_url": df.iloc[0]["base_url"] if not df.empty else None,
        "overall_summary": {
            "average_metrics": final_overall_metrics,
        },
        "per_metric_summary": per_metric_summary,
        "per_question_summary": all_question_summaries,
    }

    summary_path = results_dir / "eval_summary.json"
    with open(summary_path, "w") as f:
        json.dump(output_json, f, indent=4)
    print(f"Metrics summary saved to {summary_path}")


def main():
    """Main function to orchestrate the evaluation pipeline."""
    parser = argparse.ArgumentParser(
        description="Run evaluation on agent interaction results.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--interaction-results-file",
        type=Path,
        required=True,
        help="Path to the CSV file with agent interaction results.",
    )
    parser.add_argument(
        "--results-dir",
        type=Path,
        help="Directory to save the evaluation results. Defaults to the directory of the input file.",
    )
    parser.add_argument(
        "--metrics-files",
        type=str,
        nargs="+",
        required=True,
        help="List of paths to the metric definition JSON files.",
    )
    parser.add_argument(
        "--input-label",
        type=str,
        default="manual",
        help="Label for the run (e.g., 'manual', 'scheduled', 'ci').",
    )
    parser.add_argument(
        "--test-description",
        type=str,
        default="Evaluation run from 02_agent_run_eval.py.",
        help="A brief description of what is being tested.",
    )
    parser.add_argument(
        "--metric-filter",
        action="append",
        dest="metric_filters",
        help="Filter metrics by criteria. Format: 'key:value1,value2'. Can be used multiple times.",
    )
    args = parser.parse_args()

    # --- Initialization ---
    project_id = os.environ.get("GOOGLE_CLOUD_PROJECT")
    if not project_id:
        print("Error: GOOGLE_CLOUD_PROJECT environment variable not set.")
        sys.exit(1)
    aiplatform.init(project=project_id, location="us-central1")

    # --- Load Data and Metrics ---
    if not args.interaction_results_file.exists():
        print(f"Error: The interaction results file was not found at '{args.interaction_results_file}'")
        sys.exit(1)
        
    try:
        interaction_results = pd.read_csv(args.interaction_results_file, dtype={"question_id": str})
    except Exception as e:
        print(f"Error reading CSV: {e}")
        sys.exit(1)

    # Determine results directory
    results_dir = args.results_dir if args.results_dir else args.interaction_results_file.parent
    results_dir.mkdir(parents=True, exist_ok=True)
    print(f"Results will be saved to: {results_dir}")

    metric_definitions = load_and_consolidate_metrics(args.metrics_files)

    # Save the consolidated metrics to the results directory for the next step.
    consolidated_metrics_path = results_dir / "temp_consolidated_metrics.json"
    with open(consolidated_metrics_path, "w") as f:
        json.dump(metric_definitions, f, indent=4)
    print(f"Saved consolidated metrics to {consolidated_metrics_path}")

    # --- Apply Metric Filtering ---
    if args.metric_filters:
        filters = parse_metric_filters(args.metric_filters)
        print(f"--- Applying Metric Filters: {filters} ---")
        original_count = len(metric_definitions)
        metric_definitions = filter_metrics_by_criteria(metric_definitions, filters)
        print(f"Filtered to {len(metric_definitions)}/{original_count} metrics.")
        if not metric_definitions:
            print("Warning: No metrics match the specified filters. Exiting.")
            sys.exit(0)
        print("Selected metrics:")
        for name, info in metric_definitions.items():
            print(f"  - {name} (Type: {info.get('metric_type', 'llm')}, Agents: {info.get('agents', ['data_explorer_agent'])})")
    else:
        print(f"Running evaluation with all {len(metric_definitions)} available metrics.")

    # --- Data Preparation ---
    original_df = interaction_results.copy()
    # Safely parse all JSON-like columns.
    for col in [
        "extracted_data", "reference_data", "latency_data", "agents_evaluated",
        "user_inputs", "trace_summary", "session_trace", "final_session_state"
    ]:
        if col in interaction_results.columns:
            interaction_results[col] = interaction_results[col].apply(robust_json_loads)

    # Normalize nested JSON data into separate columns for easier access.
    columns_to_drop = []
    dfs_to_concat = [interaction_results]
    
    if "extracted_data" in interaction_results.columns:
        extracted_data_df = pd.json_normalize(interaction_results["extracted_data"]).add_prefix("extracted_data.")
        dfs_to_concat.append(extracted_data_df)
        # Don't drop original column, we need it for nested dict access
        # columns_to_drop.append("extracted_data") 
    
    if "reference_data" in interaction_results.columns:
        reference_data_df = pd.json_normalize(interaction_results["reference_data"]).add_prefix("reference_data.")
        dfs_to_concat.append(reference_data_df)
        # Don't drop original column
        # columns_to_drop.append("reference_data")
    
    if columns_to_drop:
        dfs_to_concat[0] = interaction_results.drop(columns=columns_to_drop)
    
    interaction_results = pd.concat(dfs_to_concat, axis=1)

    # --- Run Evaluations ---
    evaluation_datetime = datetime.now()
    base_run_id = f"eval-{evaluation_datetime.strftime('%Y%m%d%H%M%S')}"
    all_llm_results = []
    deterministic_results_by_row = defaultdict(dict)

    # --- Run ALL Deterministic Metrics (Phase 1) ---
    print("\n--- Running Deterministic Metrics (Global) ---")
    for index, row in original_df.iterrows():
        try:
            if "final_session_state" not in row.index or pd.isna(row["final_session_state"]) or not row["final_session_state"]:
                print(f"Skipping deterministic metrics for row {index}: missing final_session_state")
                continue
            
            session_state = robust_json_loads(row["final_session_state"])
            session_trace_val = row.get("session_trace") if "session_trace" in row.index else None
            session_trace = robust_json_loads(session_trace_val) or []
            agents_evaluated_val = row.get("agents_evaluated", "[]") if "agents_evaluated" in row.index else "[]"
            agents_evaluated = robust_json_loads(agents_evaluated_val) or []
            reference_data_val = row.get("reference_data") if "reference_data" in row.index else None
            reference_data = robust_json_loads(reference_data_val) or {}
            question_metadata_val = row.get("question_metadata", "{}") if "question_metadata" in row.index else "{}"
            question_metadata = robust_json_loads(question_metadata_val) or {}
            latency_data_val = row.get("latency_data") if "latency_data" in row.index else None
            latency_data = robust_json_loads(latency_data_val) or []
            
            # Run ALL registered deterministic metrics
            det_results = evaluate_deterministic_metrics(
                session_state=session_state,
                session_trace=session_trace,
                agents_evaluated=agents_evaluated,
                reference_data=reference_data,
                question_metadata=question_metadata,
                metrics_to_run=list(DETERMINISTIC_METRICS.keys()),
                latency_data=latency_data
            )
            
            # Merge results into row storage
            deterministic_results_by_row[index].update(det_results)
            
        except Exception as e:
            print(f"Error calculating deterministic metrics for row {index}: {e}")

    # Group metrics by the agent they are intended for.
    metrics_by_agent = defaultdict(list)
    for metric_name, metric_info in metric_definitions.items():
        for agent in metric_info.get("agents", ["data_explorer_agent"]):
            metrics_by_agent[agent].append((metric_name, metric_info))

    for agent, metrics in metrics_by_agent.items():
        print(f"\n--- Running Metrics for Agent: {agent} ---")

        # Filter the interaction data for the current agent.
        # 'data_explorer_agent' is the root agent, so its metrics apply to all rows.
        # For sub-agents, we filter rows where that agent was explicitly evaluated.
        if agent == "data_explorer_agent":
            agent_df = interaction_results.copy()
        else:
            # This logic is crucial. It checks if the agent name from the metric
            # definition is present in the 'agents_evaluated' list for each row.
            # If agent names in the CSV (e.g., 'sql_explorer_agent') differ from
            # metric files (e.g., 'sql_explorer'), this filtering will fail.
            # Ensure data consistency between interaction logging and metric definitions.
            mask = interaction_results["agents_evaluated"].apply(
                lambda x: agent in (x if isinstance(x, list) else [x]) if x else False
            )
            agent_df = interaction_results[mask].copy()

        if agent_df.empty:
            print(f"Warning: No applicable interaction rows found for agent '{agent}'. Skipping.")
            continue
        print(f"Found {len(agent_df)} applicable rows for agent '{agent}'.")

        agent_experiment_id = f"{base_run_id}-{agent.replace('_', '-')}"
        aiplatform.init(experiment=agent_experiment_id)

        eval_tasks = []
        for metric_name, metric_info in metrics:
            metric_type = metric_info.get("metric_type", "llm")

            # Skip deterministic metrics as they are now run globally in Phase 1.
            # We identify them by type OR by checking if they exist in the registry (with/without prefix).
            is_deterministic = False
            if metric_type == "deterministic":
                is_deterministic = True
            elif metric_name in DETERMINISTIC_METRICS:
                is_deterministic = True
            else:
                 # Check for prefixes (e.g. prefix__metric_name)
                 for key in DETERMINISTIC_METRICS:
                     if metric_name.endswith(f"_{key}"):
                         is_deterministic = True
                         break
            
            if is_deterministic:
                continue

            # --- Prepare and run LLM-based metrics ---
            eval_dataset = pd.DataFrame()
            mapping = metric_info.get("dataset_mapping", {})
            
            # Dynamically build the evaluation dataset based on the metric's mapping.
            # This is where the connection to `extracted_data` and `reference_data` happens.
            all_source_columns = []
            for details in mapping.values():
                source_cols = details.get("source_column") or details.get("source_columns")
                if isinstance(source_cols, str):
                    source_cols = [source_cols]
                
                for col in source_cols:
                    found_col = False
                    # 1. Try explicit mapping first (e.g. reference_data:field -> reference_data.field)
                    if ":" in col:
                        prefix, field = col.split(":", 1)
                        dot_col = f"{prefix}.{field}"
                        if dot_col in agent_df.columns:
                            all_source_columns.append(dot_col)
                            found_col = True
                    
                    if not found_col:
                        # 2. Check for normalized column names with standard prefixes
                        for prefix in ["extracted_data.", "reference_data."]:
                            if f"{prefix}{col}" in agent_df.columns:
                                all_source_columns.append(f"{prefix}{col}")
                                found_col = True
                                break
                    
                    if not found_col:
                         # 3. Check for original column name
                        if col in agent_df.columns:
                            all_source_columns.append(col)

            metric_df = agent_df.dropna(subset=all_source_columns).copy()
            if metric_df.empty:
                print(f"No valid rows for metric '{metric_name}' after dropping NaNs. Skipping.")
                continue

            for placeholder, details in mapping.items():
                if "source_column" in details:
                    col = details["source_column"]
                    # Find the correct column name
                    candidates = []
                    # 1. Try explicit dot notation if colon is used
                    if ":" in col:
                        candidates.append(col.replace(":", "."))
                    # 2. Try standard prefixes
                    candidates.extend([f"extracted_data.{col}", f"reference_data.{col}", col])
                    
                    # 3. Fallback: Check if the *root* of a colon-separated path exists (e.g. "reference_data")
                    root_col = None
                    field_path = None
                    if ":" in col:
                        parts = col.split(":", 1)
                        if parts[0] in metric_df.columns:
                            root_col = parts[0]
                            field_path = parts[1]

                    source_col_name = next(
                        (c for c in candidates if c in metric_df.columns),
                        None
                    )
                    
                    if source_col_name:
                        # Direct column match (flattened or simple)
                        eval_dataset[placeholder] = metric_df[source_col_name].apply(
                            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)
                        )
                    elif root_col and field_path:
                        # Nested extraction from a root dict column
                        def extract_nested_field(row_data, path):
                            if isinstance(row_data, str):
                                row_data = robust_json_loads(row_data)
                            if isinstance(row_data, dict):
                                return row_data.get(path)
                            return None

                        eval_dataset[placeholder] = metric_df[root_col].apply(
                            lambda x: extract_nested_field(x, field_path)
                        ).apply(
                            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x) if x is not None else ""
                        )
                elif "template" in details:
                    def format_template(row):
                        template_vars = {}
                        for col in details["source_columns"]:
                            # Same lookup logic as above
                            candidates = []
                            if ":" in col:
                                candidates.append(col.replace(":", "."))
                            candidates.extend([f"extracted_data.{col}", f"reference_data.{col}", col])
                            
                            source_col_name = next(
                                (
                                    c
                                    for c in candidates
                                    if c in row and row[c] is not None and (not isinstance(row[c], list) or len(row[c]) > 0)
                                ),
                                None,
                            )
                            template_vars[
                                col.replace(":", "_")
                            ] = row[source_col_name] if source_col_name else ""
                        return details["template"].format(**template_vars)
                    eval_dataset[placeholder] = metric_df.apply(format_template, axis=1)

            if eval_dataset.empty:
                print(f"Evaluation dataset for '{metric_name}' is empty. Skipping.")
                continue

            metric = PointwiseMetric(metric=metric_name, metric_prompt_template=metric_info["template"])
            eval_task = EvalTask(dataset=eval_dataset, metrics=[metric])
            eval_tasks.append((eval_task, metric_df, metric_name))

        # Execute all prepared LLM eval tasks for the current agent.
        for task, metric_df, metric_name in eval_tasks:
            max_retries = 3
            base_delay = 5  # seconds
            for attempt in range(max_retries):
                try:
                    print(f"Running LLM evaluation for metric: {metric_name} (Attempt {attempt + 1}/{max_retries})")
                    result = task.evaluate()
                    result.metrics_table["original_index"] = metric_df.index
                    all_llm_results.append((result, metric_name))
                    break  # Success, exit retry loop
                except Exception as e:
                    print(f"An error occurred during evaluation for metric '{metric_name}': {e}")
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)
                        print(f"Retrying in {delay} seconds...")
                        time.sleep(delay)
                    else:
                        print(f"All {max_retries} retries failed for metric '{metric_name}'.")

    # --- Aggregate and Save Results ---
    # results_dir already created above

    final_results_df = original_df.copy()
    final_results_df["evaluation_datetime"] = evaluation_datetime.isoformat()
    final_results_df["input_label"] = args.input_label
    final_results_df["experiment_id"] = base_run_id

    # Create a placeholder for evaluation results for each row.
    eval_results_data = [{} for _ in range(len(final_results_df))]

    # Merge deterministic results into the placeholder.
    for index, results in deterministic_results_by_row.items():
        if index < len(eval_results_data):
            eval_results_data[index].update(results)

    # Merge LLM-based results into the placeholder.
    for result, metric_name in all_llm_results:
        for _, row in result.metrics_table.iterrows():
            original_index = row["original_index"]
            if original_index < len(eval_results_data):
                eval_results_data[original_index][metric_name] = {
                    "score": row[f"{metric_name}/score"],
                    "explanation": row[f"{metric_name}/explanation"],
                }

    final_results_df["eval_results"] = [json.dumps(res) if res else None for res in eval_results_data]

    # Dynamically create the output filename based on the input filename.
    input_basename = args.interaction_results_file.name
    output_basename = input_basename.replace(
        "processed_interaction", "evaluation_results"
    )
    # Add a fallback in case the input filename doesn't match the expected pattern
    if "processed_interaction" not in input_basename:
        output_basename = f"evaluation_results_{input_basename}"

    output_csv_path = results_dir / output_basename
    final_results_df.to_csv(output_csv_path, index=False)
    print(f"\nEvaluation complete. Full results saved to {output_csv_path}")

    save_metrics_summary(
        final_results_df,
        results_dir,
        base_run_id,
        args.input_label,
        args.test_description,
    )


if __name__ == "__main__":
    main()