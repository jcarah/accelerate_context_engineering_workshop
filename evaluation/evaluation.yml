stages:
  - evaluation

# Base template for evaluation jobs
.eval_base:
  stage: evaluation
  image: ghcr.io/astral-sh/uv:python3.12-trixie
  tags:
    - gke-runners-dematic
  variables:
    GOOGLE_CLOUD_PROJECT: ${GCP_PROJECT_ID}
    SERVICE_ACCOUNT_NAME: "opendataqna-edp.json"
    UV_SYSTEM_PYTHON: "1"
  before_script:
    # Install dependencies
    - uv sync --frozen --no-install-project
    - uv sync --frozen
    # Setup GCP authentication
    - apt-get update && apt-get install -y --no-install-recommends curl gnupg
    - echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
    - curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
    - apt-get update && apt-get install -y google-cloud-sdk
    # Download secure files
    - curl -s https://gitlab.com/gitlab-org/incubation-engineering/mobile-devops/download-secure-files/-/raw/main/installer | bash
    - cp .secure_files/$SERVICE_ACCOUNT_NAME .
    - export GOOGLE_APPLICATION_CREDENTIALS=$(pwd)/$SERVICE_ACCOUNT_NAME
    - gcloud auth activate-service-account --key-file $SERVICE_ACCOUNT_NAME
  allow_failure: true

# Evaluation job for Merge Requests (10 questions sample)
eval-mr:
  extends: .eval_base
  script:
    - echo "Step 1/3 - Running agent interactions on 10 sample questions..."
    # Note: Replace 'your-app-name' with your actual agent name
    - uv run adk-eval run --app-name data-explorer --questions-file evaluation/datasets/su_levelG_only.json --runs 1 --num-questions 10 --results-dir evaluation/results/mr_${CI_MERGE_REQUEST_IID}
    - echo "Step 2/3 - Running evaluation metrics..."
    - uv run adk-eval evaluate --interaction-file evaluation/results/mr_${CI_MERGE_REQUEST_IID}/processed_interaction_data-explorer.csv --results-dir evaluation/results/mr_${CI_MERGE_REQUEST_IID} --metrics-files evaluation/metrics/metric_definitions.json evaluation/metrics/metric_definitions_harmful_language.json --test-description "Eval Run on MR"
    - echo "Step 3/3 - Analyzing results..."
    - uv run adk-eval analyze --results-dir evaluation/results/mr_${CI_MERGE_REQUEST_IID}
  artifacts:
    name: "eval-results-mr-${CI_MERGE_REQUEST_IID}"
    paths:
      - evaluation/results/mr_${CI_MERGE_REQUEST_IID}/
    expire_in: 7 days
    when: always
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

# Daily scheduled evaluation (all questions)
eval-daily:
  extends: .eval_base
  script:
    - TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    - echo "Step 1/3 - Running agent interactions on all Level G questions (3 runs each)..."
    - uv run adk-eval run --app-name data-explorer --questions-file evaluation/datasets/su_levelG_only.json --runs 3 --results-dir evaluation/results/daily_${TIMESTAMP}
    - echo "Step 2/3 - Running evaluation metrics..."
    - uv run adk-eval evaluate --interaction-file evaluation/results/daily_${TIMESTAMP}/processed_interaction_data-explorer.csv --results-dir evaluation/results/daily_${TIMESTAMP} --metrics-files evaluation/metrics/metric_definitions.json --test-description "Daily Eval Run"
    - echo "Step 3/3 - Analyzing results..."
    - uv run adk-eval analyze --results-dir evaluation/results/daily_${TIMESTAMP}
  artifacts:
    name: "eval-results-daily-${CI_COMMIT_SHORT_SHA}"
    paths:
      - evaluation/results/daily_*/
    expire_in: 30 days
    when: always
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_PIPELINE_SOURCE == "web"
      when: manual