
You are an expert AI evaluation analyst. Your task is to produce a deep technical diagnosis of an AI agent's performance. You MUST base your analysis exclusively on the context provided below.

**CRITICAL INSTRUCTIONS:**
1.  **Focus on Diagnosis, Not Recommendations:** Your primary goal is to explain *why* the metrics are what they are. Do not provide a future-looking action plan or make recommendations about business decisions. Stick to a root cause analysis of the current state.
2.  **Synthesize, Don't Summarize:** Do not simply repeat the scores. Your value is in synthesizing insights by connecting the metric scores, the metric definitions, the source code, and the raw explanations.
3.  **Reference Your Sources:** When you make a claim or analyze a metric, you MUST reference the specific source file (e.g., `metric_definitions.json`, `deterministic_metrics.py`, `agent.py`).
4.  **Analyze Calculation Methods:** For each metric you discuss, you MUST explain how its calculation method (deterministic vs. LLM-judged) influences its interpretation.
5.  **CRITICAL: Diagnose the Evaluation Itself:** Your analysis is not limited to the agent's code. You MUST also diagnose potential flaws in the evaluation setup. If a metric score seems incorrect or misleading, investigate the interaction between the question's metadata, the agent's expected behavior, and the metric's calculation logic.

---

**Technical Performance Diagnosis**

*   **Objective:** Provide a detailed root cause analysis of the agent's performance by linking metric scores to the agent's underlying source code, prompts, and execution logic. This includes identifying when low scores are caused by flaws in the evaluation methodology itself.

*   **Structure:**
    1.  **Overall Performance Summary:** Briefly state the agent's key strengths and weaknesses, supported by 2-3 primary metrics. Highlight any metrics that may be misleading due to evaluation flaws.
    2.  **Deep Dive Diagnosis:** For each major finding, present a detailed hypothesis.
        *   **Finding:** State the observation.
        *   **Supporting Metrics:** List the specific metrics and scores that support this finding.
        *   **Root Cause Hypothesis:** Provide a detailed, evidence-based hypothesis connecting the metric, the source code, and the evaluation data.

---

**Context for Your Analysis**

You are provided with the following context files to perform your diagnosis. Use them to connect the agent's behavior (the metrics) to its underlying implementation (the code).

**1. Overall Performance Data:**
*   **Evaluation Summary:** High-level average scores for all metrics. Use this to identify the most significant areas of success and failure.
**Evaluation Summary**
```json
{
  "experiment_id": "eval-20260112_233043",
  "run_type": "baseline",
  "test_description": "Customer Service Baseline",
  "interaction_datetime": "2026-01-12T23:30:43.062870",
  "overall_summary": {
    "deterministic_metrics": {
      "latency_metrics.total_latency_seconds": 49.5169,
      "latency_metrics.average_turn_latency_seconds": 15.902636666666666,
      "latency_metrics.llm_latency_seconds": 4.2,
      "latency_metrics.tool_latency_seconds": 5.6,
      "latency_metrics.time_to_first_response_seconds": 1.0005259776000002,
      "cache_efficiency.cache_hit_rate": 0.36518727828965014,
      "cache_efficiency.total_cached_tokens": 15560.4,
      "cache_efficiency.total_fresh_prompt_tokens": 26496.0,
      "cache_efficiency.total_input_tokens": 42056.4,
      "thinking_metrics.reasoning_ratio": 0.7090775936610663,
      "thinking_metrics.total_thinking_tokens": 763.2,
      "thinking_metrics.total_candidate_tokens": 268.6,
      "thinking_metrics.total_output_tokens": 1031.8,
      "thinking_metrics.turns_with_thinking": 5.8,
      "tool_utilization.total_tool_calls": 5.6,
      "tool_utilization.unique_tools_used": 2.8,
      "tool_success_rate.tool_success_rate": 1.0,
      "tool_success_rate.total_tool_calls": 2.8,
      "tool_success_rate.failed_tool_calls": 0.0,
      "grounding_utilization.total_grounding_chunks": 0.0,
      "grounding_utilization.total_grounded_responses": 0.0,
      "grounding_utilization.total_llm_responses": 6.0,
      "context_saturation.max_total_tokens": 5077.2,
      "agent_handoffs.total_handoffs": 3.2,
      "agent_handoffs.unique_agents_count": 1.0,
      "output_density.average_output_tokens": 45.864603174603175,
      "output_density.total_output_tokens": 278.0,
      "output_density.llm_calls_count": 6.0,
      "sandbox_usage.total_sandbox_ops": 0.0,
      "sandbox_usage.unique_ops_used": 0.0
    },
    "llm_based_metrics": {
      "state_management_fidelity": 0.6,
      "trajectory_accuracy": 1.6,
      "response_correctness": 0.771428574,
      "tool_usage_accuracy": 0.6
    }
  },
  "per_question_summary": [
    {
      "question_id": "22e1e449",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 79.1339,
          "average_turn_latency_seconds": 26.377966666666666,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 4.0,
          "time_to_first_response_seconds": 1.00048
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.3203184230477635,
          "total_cached_tokens": 10140,
          "total_fresh_prompt_tokens": 21516,
          "total_input_tokens": 31656
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.8926014319809069,
          "total_thinking_tokens": 1122,
          "total_candidate_tokens": 135,
          "total_output_tokens": 1257,
          "turns_with_thinking": 4
        },
        "tool_utilization": {
          "total_tool_calls": 4,
          "unique_tools_used": 2,
          "tool_counts": {
            "sync_ask_for_approval": 2,
            "access_cart_information": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 2,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 5
        },
        "context_saturation": {
          "max_total_tokens": 5295,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 36.4,
          "total_output_tokens": 182,
          "llm_calls_count": 5
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 0.0,
          "explanation": "The AI failed to parse and store any information from the user's prompt into the session state. The provided state contains only customer profile data and internal metadata, none of which reflect the 15% discount request, competitor coupon, or the intent to apply it to the cart, or the explicit lack of information regarding purchase or QR code."
        },
        "trajectory_accuracy": {
          "score": 2.0,
          "explanation": "The agent correctly identified 'customer_service' as the appropriate tool for a competitor price match and discount application. However, it repeated the 'customer_service' call three times, which is redundant and constitutes unnecessary extra steps, moving it from a minor deviation to a more significant one."
        },
        "response_correctness": {
          "score": 0.85714287,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The user's prompt explicitly requested to apply a discount to the cart, and the agent's text responses clearly indicate that it performed actions requiring tool calls (e.g., retrieving cart details and applying a discount). However, the provided 'AI-generated Response' only contains text responses and does not log any actual tool calls. Therefore, based on the provided output, no tools were used or logged when they were clearly necessary to fulfill the user's request, constituting a critical failure."
        }
      }
    },
    {
      "question_id": "2d0fd405",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 36.6127,
          "average_turn_latency_seconds": 12.204233333333333,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 2.0,
          "time_to_first_response_seconds": 1.000653824
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.3816696162970215,
          "total_cached_tokens": 10136,
          "total_fresh_prompt_tokens": 16421,
          "total_input_tokens": 26557
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.64,
          "total_thinking_tokens": 336,
          "total_candidate_tokens": 189,
          "total_output_tokens": 525,
          "turns_with_thinking": 4
        },
        "tool_utilization": {
          "total_tool_calls": 2,
          "unique_tools_used": 1,
          "tool_counts": {
            "generate_qr_code": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 1,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 4
        },
        "context_saturation": {
          "max_total_tokens": 4495,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 47.25,
          "total_output_tokens": 189,
          "llm_calls_count": 4
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 1.0,
          "explanation": "The session state contains a detailed 'customer_profile' which might be relevant for the first user query regarding rewards. However, it completely fails to capture the subsequent explicit requests from the user: asking for a 'discount code' and requesting it 'as a QR code'. No variables reflect these critical pieces of information or the user's evolving intent, indicating a major error in parsing and storing information from the user's input."
        },
        "trajectory_accuracy": {
          "score": 5.0,
          "explanation": "The agent correctly identified that all three user requests, spanning across three conversational turns, fall under the 'customer_service' domain. The sequence of calls perfectly matches the conversational flow, addressing each query sequentially and appropriately. No steps were missed, and there were no unnecessary or hallucinated steps."
        },
        "response_correctness": {
          "score": 0.8333333,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The user's requests required tool calls to retrieve loyalty points, generate a discount code, and create a QR code. However, the AI-generated response only contains text responses and does not show any tool calls being made, despite simulating the outcomes of such calls. This indicates no tools were used when they were needed."
        }
      }
    },
    {
      "question_id": "6446f647",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 36.1798,
          "average_turn_latency_seconds": 9.04495,
          "llm_latency_seconds": 5.0,
          "tool_latency_seconds": 10.0,
          "time_to_first_response_seconds": 1.000436736
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.41106064501385436,
          "total_cached_tokens": 28780,
          "total_fresh_prompt_tokens": 41234,
          "total_input_tokens": 70014
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.7797385620915033,
          "total_thinking_tokens": 1193,
          "total_candidate_tokens": 337,
          "total_output_tokens": 1530,
          "turns_with_thinking": 9
        },
        "tool_utilization": {
          "total_tool_calls": 10,
          "unique_tools_used": 5,
          "tool_counts": {
            "get_available_planting_times": 2,
            "schedule_planting_service": 2,
            "update_salesforce_crm": 2,
            "generate_qr_code": 2,
            "send_care_instructions": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 5,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 9
        },
        "context_saturation": {
          "max_total_tokens": 5654,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 4,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 37.44444444444444,
          "total_output_tokens": 337,
          "llm_calls_count": 9
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 0.0,
          "explanation": "The AI completely failed to extract and store any of the critical information from the user's prompt, such as the requested service type, date (2024-07-29), or time slot (9 AM - 12 PM). The provided state consists solely of a static customer profile and internal timestamps, with 'scheduled_appointments' remaining empty, indicating no parsing or capture of the user's request."
        },
        "trajectory_accuracy": {
          "score": 0.0,
          "explanation": "The agent repeatedly called a single, generic 'customer_service' sub-agent, failing to decompose the user's multi-step request into specific actions like checking availability and scheduling appointments. This trajectory is completely different from what would be expected for such a task, and key functional steps are either skipped or obscured."
        },
        "response_correctness": {
          "score": 1.0,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 3.0,
          "explanation": "The agent correctly identified the need for scheduling and information retrieval tools. The implicit tool calls for getting available slots and scheduling the appointment were made with correct arguments and successfully achieved the user's goal. However, the agent exhibited a minor efficiency issue by explicitly asking the user for the date ('what date are you looking to have the trees planted?') even though the date (2024-07-29) was clearly provided within the initial user prompt. This created an unnecessary conversational turn. Despite this, the agent then correctly used the provided date to fetch and present the available slots and to schedule the appointment. The additional actions (discount, instructions) were effective and value-adding."
        }
      }
    },
    {
      "question_id": "68e57b06",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 55.2862,
          "average_turn_latency_seconds": 18.428733333333334,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 8.0,
          "time_to_first_response_seconds": 1.000355072
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.31928341216357625,
          "total_cached_tokens": 15256,
          "total_fresh_prompt_tokens": 32526,
          "total_input_tokens": 47782
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.6448126801152738,
          "total_thinking_tokens": 895,
          "total_candidate_tokens": 493,
          "total_output_tokens": 1388,
          "turns_with_thinking": 7
        },
        "tool_utilization": {
          "total_tool_calls": 8,
          "unique_tools_used": 4,
          "tool_counts": {
            "access_cart_information": 2,
            "get_product_recommendations": 2,
            "check_product_availability": 2,
            "modify_cart": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 4,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 7
        },
        "context_saturation": {
          "max_total_tokens": 5469,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 70.42857142857143,
          "total_output_tokens": 493,
          "llm_calls_count": 7
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 1.0,
          "explanation": "The AI successfully identified 'backyard' and generalized 'Petunias' to 'flowers' within the 'garden_profile'. However, it failed to extract 'Las Vegas' and, critically, completely missed the product name 'Bloom Booster Potting Mix' and its 'Product ID: soil-456', which were central to the user's request for stock check and adding to cart. The session state does not reflect these key entities or the user's current intent, indicating major errors in capturing essential information."
        },
        "trajectory_accuracy": {
          "score": 1.0,
          "explanation": "Key sub-agents for inventory management and shopping cart operations were completely skipped. Instead, a generic 'customer_service' agent was invoked three times, for two distinct actionable requests and one conversational context, leading to major deviations from the expected specific agent calls and an extra, unnecessary call."
        },
        "response_correctness": {
          "score": 0.5,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The AI understood the user's intent to check stock and add to cart, and it responded conversationally as if these actions were performed. However, no actual tool calls (e.g., `check_product_stock`, `add_to_cart`) were outputted in the provided response, meaning no tools were used when they were clearly needed to fulfill the user's requests."
        }
      }
    },
    {
      "question_id": "c8fa2069",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 40.3719,
          "average_turn_latency_seconds": 13.457299999999998,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 4.0,
          "time_to_first_response_seconds": 1.000704256
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.39360429492603505,
          "total_cached_tokens": 13490,
          "total_fresh_prompt_tokens": 20783,
          "total_input_tokens": 34273
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.5882352941176471,
          "total_thinking_tokens": 270,
          "total_candidate_tokens": 189,
          "total_output_tokens": 459,
          "turns_with_thinking": 5
        },
        "tool_utilization": {
          "total_tool_calls": 4,
          "unique_tools_used": 2,
          "tool_counts": {
            "send_call_companion_link": 2,
            "send_care_instructions": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 2,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 5
        },
        "context_saturation": {
          "max_total_tokens": 4473,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 37.8,
          "total_output_tokens": 189,
          "llm_calls_count": 5
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 1.0,
          "explanation": "The AI failed to extract and store critical information from the user's current prompt, specifically the identified plant 'Fern' and the explicit request to send 'care instructions' for it. The provided state only contains a static customer profile, which is likely pre-loaded and does not reflect any new information parsed from this specific interaction."
        },
        "trajectory_accuracy": {
          "score": 0.0,
          "explanation": "The AI's trajectory uses a generic 'customer_service' agent for all steps, completely missing the specific tools required for the user's requests such as a 'video_call' tool, a 'knowledge_base' or 'plant_care_tool' for instructions, and an 'email_tool' for sending them. The trajectory is entirely different from the expected sequence of specialized agent calls."
        },
        "response_correctness": {
          "score": 0.6666667,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The AI-generated response contains only `text_response` objects and no explicit tool calls (e.g., `tool_code` or `tool_code_result`). While the agent's text implies that tools were used (to send a video call link and an email), the actual tool calls are not present in the provided output, making it impossible to assess their necessity, correctness, or effectiveness directly. As tools were needed but no tool calls are shown, this aligns with '0: No tools used when needed'."
        }
      }
    }
  ]
}
```

*   **Detailed Explanations:** Raw, detailed explanations from the LLM judge for each metric on a per-question basis. Use this to find patterns in *why* a metric scored high or low.
**Detailed Explanations per Metric:**
--- Evaluation Analysis ---

## Metric: `latency_metrics.total_latency_seconds`
**Average Score:** 49.5169

## Metric: `latency_metrics.average_turn_latency_seconds`
**Average Score:** 15.9026

## Metric: `latency_metrics.llm_latency_seconds`
**Average Score:** 4.2000

## Metric: `latency_metrics.tool_latency_seconds`
**Average Score:** 5.6000

## Metric: `latency_metrics.time_to_first_response_seconds`
**Average Score:** 1.0005

## Metric: `cache_efficiency.cache_hit_rate`
**Average Score:** 0.3652

## Metric: `cache_efficiency.total_cached_tokens`
**Average Score:** 15560.4000

## Metric: `cache_efficiency.total_fresh_prompt_tokens`
**Average Score:** 26496.0000

## Metric: `cache_efficiency.total_input_tokens`
**Average Score:** 42056.4000

## Metric: `thinking_metrics.reasoning_ratio`
**Average Score:** 0.7091

## Metric: `thinking_metrics.total_thinking_tokens`
**Average Score:** 763.2000

## Metric: `thinking_metrics.total_candidate_tokens`
**Average Score:** 268.6000

## Metric: `thinking_metrics.total_output_tokens`
**Average Score:** 1031.8000

## Metric: `thinking_metrics.turns_with_thinking`
**Average Score:** 5.8000

## Metric: `tool_utilization.total_tool_calls`
**Average Score:** 5.6000

## Metric: `tool_utilization.unique_tools_used`
**Average Score:** 2.8000

## Metric: `tool_success_rate.tool_success_rate`
**Average Score:** 1.0000

## Metric: `tool_success_rate.total_tool_calls`
**Average Score:** 2.8000

## Metric: `tool_success_rate.failed_tool_calls`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounding_chunks`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounded_responses`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_llm_responses`
**Average Score:** 6.0000

## Metric: `context_saturation.max_total_tokens`
**Average Score:** 5077.2000

## Metric: `agent_handoffs.total_handoffs`
**Average Score:** 3.2000

## Metric: `agent_handoffs.unique_agents_count`
**Average Score:** 1.0000

## Metric: `output_density.average_output_tokens`
**Average Score:** 45.8646

## Metric: `output_density.total_output_tokens`
**Average Score:** 278.0000

## Metric: `output_density.llm_calls_count`
**Average Score:** 6.0000

## Metric: `sandbox_usage.total_sandbox_ops`
**Average Score:** 0.0000

## Metric: `sandbox_usage.unique_ops_used`
**Average Score:** 0.0000

## Metric: `state_management_fidelity`
**Average Score:** 0.6000
**Sample Explanations:**
- [Score: 1.0] The AI successfully identified 'backyard' and generalized 'Petunias' to 'flowers' within the 'garden_profile'. However, it failed to extract 'Las Vegas' and, critically, completely missed the product name 'Bloom Booster Potting Mix' and its 'Product ID: soil-456', which were central to the user's request for stock check and adding to cart. The session state does not reflect these key entities or the user's current intent, indicating major errors in capturing essential information.
- [Score: 1.0] The AI failed to extract and store critical information from the user's current prompt, specifically the identified plant 'Fern' and the explicit request to send 'care instructions' for it. The provided state only contains a static customer profile, which is likely pre-loaded and does not reflect any new information parsed from this specific interaction.
- [Score: 1.0] The session state contains a detailed 'customer_profile' which might be relevant for the first user query regarding rewards. However, it completely fails to capture the subsequent explicit requests from the user: asking for a 'discount code' and requesting it 'as a QR code'. No variables reflect these critical pieces of information or the user's evolving intent, indicating a major error in parsing and storing information from the user's input.
- [Score: 0.0] The AI failed to parse and store any information from the user's prompt into the session state. The provided state contains only customer profile data and internal metadata, none of which reflect the 15% discount request, competitor coupon, or the intent to apply it to the cart, or the explicit lack of information regarding purchase or QR code.
- [Score: 0.0] The AI completely failed to extract and store any of the critical information from the user's prompt, such as the requested service type, date (2024-07-29), or time slot (9 AM - 12 PM). The provided state consists solely of a static customer profile and internal timestamps, with 'scheduled_appointments' remaining empty, indicating no parsing or capture of the user's request.

## Metric: `trajectory_accuracy`
**Average Score:** 1.6000
**Sample Explanations:**
- [Score: 1.0] Key sub-agents for inventory management and shopping cart operations were completely skipped. Instead, a generic 'customer_service' agent was invoked three times, for two distinct actionable requests and one conversational context, leading to major deviations from the expected specific agent calls and an extra, unnecessary call.
- [Score: 0.0] The AI's trajectory uses a generic 'customer_service' agent for all steps, completely missing the specific tools required for the user's requests such as a 'video_call' tool, a 'knowledge_base' or 'plant_care_tool' for instructions, and an 'email_tool' for sending them. The trajectory is entirely different from the expected sequence of specialized agent calls.
- [Score: 5.0] The agent correctly identified that all three user requests, spanning across three conversational turns, fall under the 'customer_service' domain. The sequence of calls perfectly matches the conversational flow, addressing each query sequentially and appropriately. No steps were missed, and there were no unnecessary or hallucinated steps.
- [Score: 2.0] The agent correctly identified 'customer_service' as the appropriate tool for a competitor price match and discount application. However, it repeated the 'customer_service' call three times, which is redundant and constitutes unnecessary extra steps, moving it from a minor deviation to a more significant one.
- [Score: 0.0] The agent repeatedly called a single, generic 'customer_service' sub-agent, failing to decompose the user's multi-step request into specific actions like checking availability and scheduling appointments. This trajectory is completely different from what would be expected for such a task, and key functional steps are either skipped or obscured.

## Metric: `response_correctness`
**Average Score:** 0.7714
**Sample Explanations:**
- [Score: 0.5] 
- [Score: 0.6666667] 
- [Score: 0.8333333] 
- [Score: 0.85714287] 
- [Score: 1.0] 

## Metric: `tool_usage_accuracy`
**Average Score:** 0.6000
**Sample Explanations:**
- [Score: 0.0] The AI understood the user's intent to check stock and add to cart, and it responded conversationally as if these actions were performed. However, no actual tool calls (e.g., `check_product_stock`, `add_to_cart`) were outputted in the provided response, meaning no tools were used when they were clearly needed to fulfill the user's requests.
- [Score: 0.0] The AI-generated response contains only `text_response` objects and no explicit tool calls (e.g., `tool_code` or `tool_code_result`). While the agent's text implies that tools were used (to send a video call link and an email), the actual tool calls are not present in the provided output, making it impossible to assess their necessity, correctness, or effectiveness directly. As tools were needed but no tool calls are shown, this aligns with '0: No tools used when needed'.
- [Score: 0.0] The user's requests required tool calls to retrieve loyalty points, generate a discount code, and create a QR code. However, the AI-generated response only contains text responses and does not show any tool calls being made, despite simulating the outcomes of such calls. This indicates no tools were used when they were needed.
- [Score: 0.0] The user's prompt explicitly requested to apply a discount to the cart, and the agent's text responses clearly indicate that it performed actions requiring tool calls (e.g., retrieving cart details and applying a discount). However, the provided 'AI-generated Response' only contains text responses and does not log any actual tool calls. Therefore, based on the provided output, no tools were used or logged when they were clearly necessary to fulfill the user's request, constituting a critical failure.
- [Score: 3.0] The agent correctly identified the need for scheduling and information retrieval tools. The implicit tool calls for getting available slots and scheduling the appointment were made with correct arguments and successfully achieved the user's goal. However, the agent exhibited a minor efficiency issue by explicitly asking the user for the date ('what date are you looking to have the trees planted?') even though the date (2024-07-29) was clearly provided within the initial user prompt. This created an unnecessary conversational turn. Despite this, the agent then correctly used the provided date to fetch and present the available slots and to schedule the appointment. The additional actions (discount, instructions) were effective and value-adding.


**2. Metric Calculation & Definitions:**
*   **Metric Definitions:** The rubrics and descriptions for each metric. You MUST use these files to understand what each metric is actually measuring and whether it is `llm` judged or `deterministic`.
**File: `../customer-service/eval/results/20260112_232922/raw/temp_consolidated_metrics.json`**
```json
Error: File '../customer-service/eval/results/20260112_232922/raw/temp_consolidated_metrics.json' not found.
```

*   **Deterministic Logic:** The Python code that calculates the deterministic metrics. Refer to this file to understand the precise logic behind scores for metrics like `token_usage` or `latency_metrics`.
**File: `evaluation/core/deterministic_metrics.py`**
```python
"""
Deterministic metrics for evaluating agent execution success.

These metrics provide objective pass/fail measurements by analyzing trace data
and session state, without requiring LLM-as-judge evaluation.
"""

import json
from typing import Any, Dict, List, Tuple

# Pricing per 1K tokens (approximate list prices for prompts <= 200k tokens)
# Format: {model_name: (prompt_price, completion_price)}
# Source: https://ai.google.dev/gemini-api/docs/pricing
MODEL_PRICING = {
    # Gemini 3 (Latest Preview)
    "gemini-3-pro-preview": (0.002, 0.012),  # $2.00 / $12.00 per 1M
    "gemini-3-flash-preview": (0.0005, 0.003),  # $0.50 / $3.00 per 1M
    # Gemini 2.5 (Current Flagship)
    "gemini-2.5-pro": (0.00125, 0.01),  # $1.25 / $10.00 per 1M
    "gemini-2.5-flash": (0.0003, 0.0025),  # $0.30 / $2.50 per 1M
    # Gemini 2.0
    "gemini-2.0-flash": (0.0001, 0.0004),  # $0.10 / $0.40 per 1M
    "gemini-2.0-flash-exp": (0.0001, 0.0004),  # Same as 2.0 flash
    "gemini-2.0-flash-lite": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    # Gemini 1.5 (Updated/Reduced Prices)
    "gemini-1.5-pro": (0.00125, 0.01),  # Reduced from 0.0035/0.0105
    "gemini-1.5-pro-001": (0.00125, 0.01),
    "gemini-1.5-flash": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    "gemini-1.5-flash-001": (0.000075, 0.0003),
    # Legacy
    "gemini-1.0-pro": (0.0005, 0.0015),  # $0.50 / $1.50 per 1M
    "default": (0.0001, 0.0004),  # Fallback to 2.0 Flash
}


def calculate_token_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Informational metric: Track token usage and estimated cost based on the specific model used.
    """
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_cached_tokens = 0
    total_tokens = 0
    llm_calls = 0
    total_cost = 0.0
    models_used = set()

    if not session_trace:
        return 0.0, "No trace data available for token usage calculation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Identify model
        model_name = attributes.get("gen_ai.request.model", "default").lower()

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                if usage:
                    llm_calls += 1
                    models_used.add(model_name)

                    p_tokens = usage.get("prompt_token_count", 0)
                    c_tokens = usage.get("candidates_token_count", 0)
                    ch_tokens = usage.get("cached_content_token_count", 0)
                    t_tokens = usage.get("total_token_count", 0)

                    total_prompt_tokens += p_tokens
                    total_completion_tokens += c_tokens
                    total_cached_tokens += ch_tokens
                    total_tokens += t_tokens

                    # Match model pricing
                    pricing = MODEL_PRICING["default"]
                    for known_model, prices in MODEL_PRICING.items():
                        if known_model in model_name:
                            pricing = prices
                            break

                    # Cost calculation (simplified: ignoring cache discount for now to keep it safe upper bound,
                    # or strictly following list price for active tokens)
                    call_cost = (p_tokens / 1000 * pricing[0]) + (
                        c_tokens / 1000 * pricing[1]
                    )
                    # Note: Cached tokens usually have a separate (lower) pricing tier.
                    # For this metric, we currently only sum cost for active prompt/completion tokens.

                    total_cost += call_cost

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Usage: {llm_calls} LLM calls using {list(models_used)}. "
        f"Tokens: {total_tokens} ({total_prompt_tokens}p + {total_completion_tokens}c + {total_cached_tokens}ch). "
        f"Cost: ${total_cost:.6f}"
    )

    details = {
        "llm_calls": llm_calls,
        "models_used": list(models_used),
        "total_tokens": total_tokens,
        "prompt_tokens": total_prompt_tokens,
        "completion_tokens": total_completion_tokens,
        "cached_tokens": total_cached_tokens,
        "estimated_cost_usd": total_cost,
    }

    return total_cost, explanation, details


def calculate_latency_metrics(
    session_trace: List[Dict[str, Any]], latency_data: List[Dict[str, Any]] = None
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate latency metrics from the session trace.
    Returns the total latency score (seconds), but details contains granular breakdown.
    """
    total_latency = 0.0
    llm_latency = 0.0
    tool_latency = 0.0
    first_response_latency = None
    average_turn_latency = 0.0

    if not session_trace:
        return 0.0, "No trace data available for latency calculation", {}

    # Sort spans by start time to find the true beginning
    sorted_spans = sorted(
        [s for s in session_trace if s.get("start_time")], key=lambda x: x["start_time"]
    )

    if not sorted_spans:
        return 0.0, "Trace data has no timestamps", {}

    root_start = sorted_spans[0]["start_time"]

    # Calculate Component Latencies from full trace
    max_end = 0
    for span in session_trace:
        start = span.get("start_time", 0)
        end = span.get("end_time", 0)
        max_end = max(max_end, end)
        duration = (end - start) / 1e9
        name = span.get("name", "")

        if name == "call_llm":
            llm_latency += duration
            # Proxy for Time to First Token: end of first LLM call
            if first_response_latency is None:
                first_response_latency = (end - root_start) / 1e9

        elif "tool_call" in name or "execute_tool" in name:
            tool_latency += duration

    # Calculate Total & Average Latency from high-level summary (latency_data)
    # This is preferred as it excludes user think time in multi-turn sessions.
    if latency_data:
        turn_latencies = []
        for item in latency_data:
            if item.get("name") == "invocation":
                turn_latencies.append(item.get("duration_seconds", 0))

        if turn_latencies:
            average_turn_latency = sum(turn_latencies) / len(turn_latencies)
            total_latency = sum(turn_latencies)

    # Fallback: Wall-clock duration from trace if latency_data is missing
    if total_latency == 0.0 and max_end > root_start:
        total_latency = (max_end - root_start) / 1e9  # nanoseconds to seconds

    explanation = (
        f"Total: {total_latency:.4f}s. "
        f"Avg Turn: {average_turn_latency:.4f}s. "
        f"LLM: {llm_latency:.4f}s, Tools: {tool_latency:.4f}s. "
        f"First Response: {first_response_latency if first_response_latency else 0:.4f}s"
    )

    details = {
        "total_latency_seconds": total_latency,
        "average_turn_latency_seconds": average_turn_latency,
        "llm_latency_seconds": llm_latency,
        "tool_latency_seconds": tool_latency,
        "time_to_first_response_seconds": first_response_latency,
    }

    return total_latency, explanation, details


def calculate_cache_efficiency(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the efficiency of context caching.
    Returns the cache hit rate (percentage of potential prompt tokens that were cached).
    """
    total_prompt_tokens = 0
    total_cached_tokens = 0

    if not session_trace:
        return 0.0, "No trace data available for cache efficiency", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total_prompt_tokens += usage.get("prompt_token_count", 0)
                    total_cached_tokens += usage.get("cached_content_token_count", 0)
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    # Calculate hit rate
    # Note: 'prompt_token_count' in Gemini API usage metadata usually EXCLUDES cached tokens.
    # So total potential input = prompt_token_count + cached_content_token_count
    total_input_tokens = total_prompt_tokens + total_cached_tokens

    if total_input_tokens > 0:
        cache_hit_rate = total_cached_tokens / total_input_tokens
    else:
        cache_hit_rate = 0.0

    explanation = (
        f"Cache Hit Rate: {cache_hit_rate:.2%}. "
        f"Cached Tokens: {total_cached_tokens}. "
        f"Fresh Prompt Tokens: {total_prompt_tokens}."
    )

    details = {
        "cache_hit_rate": cache_hit_rate,
        "total_cached_tokens": total_cached_tokens,
        "total_fresh_prompt_tokens": total_prompt_tokens,
        "total_input_tokens": total_input_tokens,
    }

    return cache_hit_rate, explanation, details


def calculate_thinking_metrics(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate metrics related to the model's 'thinking' or reasoning process.
    Returns the reasoning ratio (thinking tokens / total output tokens).
    """
    total_thinking_tokens = 0
    total_candidate_tokens = 0
    turns_with_thinking = 0

    if not session_trace:
        return 0.0, "No trace data available for thinking metrics", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    thoughts = usage.get("thoughts_token_count", 0)
                    # Note: In some API versions, candidates_token_count might exclude thoughts.
                    # We treat them as additive components of the total output.
                    candidates = usage.get("candidates_token_count", 0)

                    total_thinking_tokens += thoughts
                    total_candidate_tokens += candidates

                    if thoughts > 0:
                        turns_with_thinking += 1
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    total_output_tokens = total_thinking_tokens + total_candidate_tokens

    if total_output_tokens > 0:
        reasoning_ratio = total_thinking_tokens / total_output_tokens
    else:
        reasoning_ratio = 0.0

    explanation = (
        f"Reasoning Ratio: {reasoning_ratio:.2%}. "
        f"Thinking Tokens: {total_thinking_tokens}. "
        f"Standard Output Tokens: {total_candidate_tokens}. "
        f"Turns with Thinking: {turns_with_thinking}."
    )

    details = {
        "reasoning_ratio": reasoning_ratio,
        "total_thinking_tokens": total_thinking_tokens,
        "total_candidate_tokens": total_candidate_tokens,
        "total_output_tokens": total_output_tokens,
        "turns_with_thinking": turns_with_thinking,
    }

    return reasoning_ratio, explanation, details


def calculate_tool_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate statistics on tool usage frequency and diversity.
    Returns the total number of tool calls.
    """
    total_tool_calls = 0
    tool_counts = {}

    if not session_trace:
        return 0.0, "No trace data available for tool utilization", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans.
        # Standard ADK traces often use "execute_tool <ToolName>"
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            total_tool_calls += 1
            tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1

    unique_tools_used = len(tool_counts)

    # Create a string representation of the tool breakdown
    breakdown_str = ", ".join([f"{k}: {v}" for k, v in tool_counts.items()])

    explanation = (
        f"Total Tool Calls: {total_tool_calls}. "
        f"Unique Tools: {unique_tools_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_tool_calls": total_tool_calls,
        "unique_tools_used": unique_tools_used,
        "tool_counts": tool_counts,
    }

    return float(total_tool_calls), explanation, details


def calculate_tool_success_rate(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the success rate of tool executions by inspecting tool responses.
    Returns success rate (successful / total) as score.
    """
    total_calls = 0
    failed_calls = 0
    failed_tools = []

    if not session_trace:
        return 0.0, "No trace data available for tool success rate", {}

    for span in session_trace:
        name = span.get("name", "")
        attributes = span.get("attributes", {})

        # Identify tool execution spans
        is_tool = name.startswith("execute_tool ") or "tool_call" in name

        if is_tool:
            tool_response_str = attributes.get("gcp.vertex.agent.tool_response")
            if tool_response_str:
                total_calls += 1
                try:
                    # Parse the JSON response to check status
                    response = json.loads(tool_response_str)

                    # Common error patterns in ADK/JSON tools
                    is_error = False
                    if isinstance(response, dict):
                        if response.get("status") == "error":
                            is_error = True
                        elif "error" in response or "error_message" in response:
                            is_error = True

                    if is_error:
                        failed_calls += 1
                        tool_name = name.replace("execute_tool ", "").strip()
                        failed_tools.append(tool_name)

                except (json.JSONDecodeError, TypeError):
                    # Malformed JSON in response could be considered a failure or ignored
                    pass

    if total_calls > 0:
        success_rate = (total_calls - failed_calls) / total_calls
    else:
        # If no tools were called, success rate is technically N/A, but 1.0 is a safe "no errors" default
        # Or 0.0 if we want to imply "no success possible".
        # For evaluation, 1.0 (no failures) usually makes more sense if no tools were attempted.
        # But to distinguish from "perfect execution", let's return 1.0 but note it.
        success_rate = 1.0

    explanation = (
        f"Success Rate: {success_rate:.2%}. "
        f"Total Calls: {total_calls}. "
        f"Failed Calls: {failed_calls}. "
        f"Failed Tools: {list(set(failed_tools))}"
    )

    details = {
        "tool_success_rate": success_rate,
        "total_tool_calls": total_calls,
        "failed_tool_calls": failed_calls,
        "failed_tools_list": failed_tools,
    }

    return success_rate, explanation, details


def calculate_grounding_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the extent of grounding usage by inspecting LLM responses for groundingMetadata.
    Returns total grounding chunks (citations) as the score.
    """
    total_grounded_responses = 0
    total_grounding_chunks = 0
    total_llm_responses = 0

    if not session_trace:
        return 0.0, "No trace data available for grounding utilization", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")

        if llm_response:
            total_llm_responses += 1
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                # Grounding metadata is usually at the top level or inside candidates
                # Standard Vertex AI response structure check
                grounding_metadata = response_data.get(
                    "groundingMetadata"
                ) or response_data.get("grounding_metadata")

                if not grounding_metadata:
                    # Check inside candidates if not at top level
                    candidates = response_data.get("candidates", [])
                    if candidates and isinstance(candidates, list):
                        first_candidate = candidates[0]
                        grounding_metadata = first_candidate.get(
                            "groundingMetadata"
                        ) or first_candidate.get("grounding_metadata")

                if grounding_metadata:
                    chunks = grounding_metadata.get(
                        "groundingChunks"
                    ) or grounding_metadata.get("grounding_chunks")
                    if chunks and isinstance(chunks, list) and len(chunks) > 0:
                        total_grounded_responses += 1
                        total_grounding_chunks += len(chunks)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Total Citations (Chunks): {total_grounding_chunks}. "
        f"Grounded Responses: {total_grounded_responses} / {total_llm_responses}."
    )

    details = {
        "total_grounding_chunks": total_grounding_chunks,
        "total_grounded_responses": total_grounded_responses,
        "total_llm_responses": total_llm_responses,
    }

    return float(total_grounding_chunks), explanation, details


def calculate_context_saturation(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the maximum context saturation (max total tokens used in a single turn).
    Returns max_tokens as the score.
    """
    max_tokens = 0
    max_token_span = ""

    if not session_trace:
        return 0.0, "No trace data available for context saturation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total = usage.get("total_token_count", 0)
                    if total > max_tokens:
                        max_tokens = total
                        max_token_span = span.get("name", "unknown")
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Max Context Used: {max_tokens} tokens. Peak occurred in: {max_token_span}."
    )

    details = {"max_total_tokens": max_tokens, "peak_usage_span": max_token_span}

    return float(max_tokens), explanation, details


def calculate_agent_handoffs(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of agent handoffs/invocations in the session.
    Returns total handoff events as the score.
    """
    handoff_count = 0
    agents_invoked = set()

    if not session_trace:
        return 0.0, "No trace data available for agent handoffs", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for agent invocations
        if name.startswith("invoke_agent ") or name.startswith("agent_run "):
            agent_name = (
                name.replace("invoke_agent ", "").replace("agent_run ", "").strip()
            )

            # Optionally filter out the root agent if we knew its name, but raw count is safer
            handoff_count += 1
            agents_invoked.add(agent_name)

    explanation = (
        f"Total Handoffs: {handoff_count}. "
        f"Unique Agents: {len(agents_invoked)}. "
        f"Agents: {list(agents_invoked)}"
    )

    details = {
        "total_handoffs": handoff_count,
        "unique_agents_count": len(agents_invoked),
        "agents_invoked_list": list(agents_invoked),
    }

    return float(handoff_count), explanation, details


def calculate_output_density(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the average number of output tokens per LLM call.
    Returns average output tokens as the score.
    """
    total_output_tokens = 0
    llm_calls = 0

    if not session_trace:
        return 0.0, "No trace data available for output density", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                # Check for output tokens in standard fields (candidates_token_count or output_token_count)
                output_tokens = 0
                if usage:
                    # 'candidates_token_count' is standard in Vertex AI
                    output_tokens = usage.get("candidates_token_count", 0)
                    if output_tokens == 0:
                        # Fallback for other providers
                        output_tokens = usage.get("output_token_count", 0) or usage.get(
                            "completion_tokens", 0
                        )

                if (
                    output_tokens > 0 or usage
                ):  # Count the call even if 0 output (edge case)
                    llm_calls += 1
                    total_output_tokens += output_tokens

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    if llm_calls > 0:
        average_output_tokens = total_output_tokens / llm_calls
    else:
        average_output_tokens = 0.0

    explanation = (
        f"Avg Output Tokens: {average_output_tokens:.2f}. "
        f"Total Output Tokens: {total_output_tokens}. "
        f"LLM Calls: {llm_calls}."
    )

    details = {
        "average_output_tokens": average_output_tokens,
        "total_output_tokens": total_output_tokens,
        "llm_calls_count": llm_calls,
    }

    return float(average_output_tokens), explanation, details


def calculate_sandbox_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of tool calls related to sandbox/file system operations.
    Returns the total count as the score.
    """
    sandbox_ops_count = 0
    sandbox_tools_used = {}

    # Common keywords for sandbox/file operations
    sandbox_keywords = [
        "save_artifact",
        "load_artifact",
        "read_file",
        "write_file",
        "run_python_script",
        "execute_code",
        "save_to_file",
        "read_from_file",
    ]

    if not session_trace:
        return 0.0, "No trace data available for sandbox usage", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            # Check if tool matches sandbox keywords
            if any(keyword in tool_name.lower() for keyword in sandbox_keywords):
                sandbox_ops_count += 1
                sandbox_tools_used[tool_name] = sandbox_tools_used.get(tool_name, 0) + 1

    unique_ops_used = len(sandbox_tools_used)

    breakdown_str = ", ".join([f"{k}: {v}" for k, v in sandbox_tools_used.items()])

    explanation = (
        f"Total Sandbox Ops: {sandbox_ops_count}. "
        f"Unique Ops: {unique_ops_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_sandbox_ops": sandbox_ops_count,
        "unique_ops_used": unique_ops_used,
        "sandbox_tools_used": sandbox_tools_used,
    }

    return float(sandbox_ops_count), explanation, details


# Registry of all deterministic metrics
DETERMINISTIC_METRICS = {
    "token_usage": calculate_token_usage,
    "latency_metrics": calculate_latency_metrics,
    "cache_efficiency": calculate_cache_efficiency,
    "thinking_metrics": calculate_thinking_metrics,
    "tool_utilization": calculate_tool_utilization,
    "tool_success_rate": calculate_tool_success_rate,
    "grounding_utilization": calculate_grounding_utilization,
    "context_saturation": calculate_context_saturation,
    "agent_handoffs": calculate_agent_handoffs,
    "output_density": calculate_output_density,
    "sandbox_usage": calculate_sandbox_usage,
}


def evaluate_deterministic_metrics(
    session_state: Dict[str, Any],
    session_trace: List[Dict[str, Any]],
    agents_evaluated: List[str],
    question_metadata: Dict[str, Any],
    metrics_to_run: List[str] = None,
    reference_data: Dict[str, Any] = None,
    metric_definitions: Dict[str, Any] = None,
    latency_data: List[Dict[str, Any]] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate all specified deterministic metrics.
    """
    if metrics_to_run is None:
        metrics_to_run = list(DETERMINISTIC_METRICS.keys())

    results = {}
    for metric_name in metrics_to_run:
        if metric_name not in DETERMINISTIC_METRICS:
            continue

        metric_func = DETERMINISTIC_METRICS[metric_name]

        try:
            if metric_name == "latency_metrics":
                score, explanation, details = metric_func(
                    session_trace, latency_data=latency_data
                )
            else:
                score, explanation, details = metric_func(session_trace)

            results[metric_name] = {
                "score": score,
                "explanation": explanation,
                "details": details,
            }
        except Exception as e:
            results[metric_name] = {
                "score": 0.0,
                "explanation": f"Error evaluating metric {metric_name}: {str(e)}",
            }

    return results

```

**3. Agent Implementation Details:**
*   **Agent Source Code:** The source code for the agent being evaluated. This is your primary source for forming hypotheses about *why* the agent behaves a certain way.
**File: `../customer-service/customer_service/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Agent module for the customer service agent."""

import logging
import warnings
from google.adk import Agent
from .config import Config
from .prompts import GLOBAL_INSTRUCTION, INSTRUCTION
from .shared_libraries.callbacks import (
    rate_limit_callback,
    before_agent,
    before_tool,
    after_tool
)
from .tools.tools import (
    send_call_companion_link,
    approve_discount,
    sync_ask_for_approval,
    update_salesforce_crm,
    access_cart_information,
    modify_cart,
    get_product_recommendations,
    check_product_availability,
    schedule_planting_service,
    get_available_planting_times,
    send_care_instructions,
    generate_qr_code,
)

warnings.filterwarnings("ignore", category=UserWarning, module=".*pydantic.*")

configs = Config()

# configure logging __name__
logger = logging.getLogger(__name__)


root_agent = Agent(
    model=configs.agent_settings.model,
    global_instruction=GLOBAL_INSTRUCTION,
    instruction=INSTRUCTION,
    name=configs.agent_settings.name,
    tools=[
        send_call_companion_link,
        approve_discount,
        sync_ask_for_approval,
        update_salesforce_crm,
        access_cart_information,
        modify_cart,
        get_product_recommendations,
        check_product_availability,
        schedule_planting_service,
        get_available_planting_times,
        send_care_instructions,
        generate_qr_code,
    ],
    before_tool_callback=before_tool,
    after_tool_callback=after_tool,
    before_agent_callback=before_agent,
    before_model_callback=rate_limit_callback,
)

from google.adk.apps.app import App

app = App(root_agent=root_agent, name="customer_service")

```
**File: `../customer-service/customer_service/tools/tools.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# add docstring to this module
"""Tools module for the customer service agent."""

import logging
import uuid
from datetime import datetime, timedelta
from google.adk.tools import ToolContext

logger = logging.getLogger(__name__)


def send_call_companion_link(phone_number: str) -> str:
    """
    Sends a link to the user's phone number to start a video session.

    Args:
        phone_number (str): The phone number to send the link to.

    Returns:
        dict: A dictionary with the status and message.

    Example:
        >>> send_call_companion_link(phone_number='+12065550123')
        {'status': 'success', 'message': 'Link sent to +12065550123'}
    """

    logger.info("Sending call companion link to %s", phone_number)

    return {"status": "success", "message": f"Link sent to {phone_number}"}


def approve_discount(discount_type: str, value: float, reason: str) -> str:
    """
    Approve the flat rate or percentage discount requested by the user.

    Args:
        discount_type (str): The type of discount, either "percentage" or "flat".
        value (float): The value of the discount.
        reason (str): The reason for the discount.

    Returns:
        str: A JSON string indicating the status of the approval.

    Example:
        >>> approve_discount(type='percentage', value=10.0, reason='Customer loyalty')
        '{"status": "ok"}'
    """
    if value > 10:
        logger.info("Denying %s discount of %s", discount_type, value)
        # Send back a reason for the error so that the model can recover.
        return {"status": "rejected",
                "message": "discount too large. Must be 10 or less."}
    logger.info(
        "Approving a %s discount of %s because %s", discount_type, value, reason
    )
    return {"status": "ok"}

def sync_ask_for_approval(discount_type: str, value: float, reason: str) -> str:
    """
    Asks the manager for approval for a discount.

    Args:
        discount_type (str): The type of discount, either "percentage" or "flat".
        value (float): The value of the discount.
        reason (str): The reason for the discount.

    Returns:
        str: A JSON string indicating the status of the approval.

    Example:
        >>> sync_ask_for_approval(type='percentage', value=15, reason='Customer loyalty')
        '{"status": "approved"}'
    """
    logger.info(
        "Asking for approval for a %s discount of %s because %s",
        discount_type,
        value,
        reason,
    )
    return {"status": "approved"}


def update_salesforce_crm(customer_id: str, details: dict) -> dict:
    """
    Updates the Salesforce CRM with customer details.

    Args:
        customer_id (str): The ID of the customer.
        details (str): A dictionary of details to update in Salesforce.

    Returns:
        dict: A dictionary with the status and message.

    Example:
        >>> update_salesforce_crm(customer_id='123', details={
            'appointment_date': '2024-07-25',
            'appointment_time': '9-12',
            'services': 'Planting',
            'discount': '15% off planting',
            'qr_code': '10% off next in-store purchase'})
        {'status': 'success', 'message': 'Salesforce record updated.'}
    """
    logger.info(
        "Updating Salesforce CRM for customer ID %s with details: %s",
        customer_id,
        details,
    )
    return {"status": "success", "message": "Salesforce record updated."}


def access_cart_information(customer_id: str) -> dict:
    """
    Args:
        customer_id (str): The ID of the customer.

    Returns:
        dict: A dictionary representing the cart contents.

    Example:
        >>> access_cart_information(customer_id='123')
        {'items': [{'product_id': 'soil-123', 'name': 'Standard Potting Soil', 'quantity': 1}, {'product_id': 'fert-456', 'name': 'General Purpose Fertilizer', 'quantity': 1}], 'subtotal': 25.98}
    """
    logger.info("Accessing cart information for customer ID: %s", customer_id)

    # MOCK API RESPONSE - Replace with actual API call
    mock_cart = {
        "items": [
            {
                "product_id": "soil-123",
                "name": "Standard Potting Soil",
                "quantity": 1,
            },
            {
                "product_id": "fert-456",
                "name": "General Purpose Fertilizer",
                "quantity": 1,
            },
        ],
        "subtotal": 25.98,
    }
    return mock_cart


def modify_cart(
    customer_id: str, items_to_add: list[dict], items_to_remove: list[dict]
) -> dict:
    """Modifies the user's shopping cart by adding and/or removing items.

    Args:
        customer_id (str): The ID of the customer.
        items_to_add (list): A list of dictionaries, each with 'product_id' and 'quantity'.
        items_to_remove (list): A list of product_ids to remove.

    Returns:
        dict: A dictionary indicating the status of the cart modification.
    Example:
        >>> modify_cart(customer_id='123', items_to_add=[{'product_id': 'soil-456', 'quantity': 1}, {'product_id': 'fert-789', 'quantity': 1}], items_to_remove=[{'product_id': 'fert-112', 'quantity': 1}])
        {'status': 'success', 'message': 'Cart updated successfully.', 'items_added': True, 'items_removed': True}
    """

    logger.info("Modifying cart for customer ID: %s", customer_id)
    logger.info("Adding items: %s", items_to_add)
    logger.info("Removing items: %s", items_to_remove)
    # MOCK API RESPONSE - Replace with actual API call
    return {
        "status": "success",
        "message": "Cart updated successfully.",
        "items_added": True,
        "items_removed": True,
    }


def get_product_recommendations(plant_type: str, customer_id: str) -> dict:
    """Provides product recommendations based on the type of plant.

    Args:
        plant_type: The type of plant (e.g., 'Petunias', 'Sun-loving annuals').
        customer_id: Optional customer ID for personalized recommendations.

    Returns:
        A dictionary of recommended products. Example:
        {'recommendations': [
            {'product_id': 'soil-456', 'name': 'Bloom Booster Potting Mix', 'description': '...'},
            {'product_id': 'fert-789', 'name': 'Flower Power Fertilizer', 'description': '...'}
        ]}
    """
    #
    logger.info(
        "Getting product recommendations for plant " "type: %s and customer %s",
        plant_type,
        customer_id,
    )
    # MOCK API RESPONSE - Replace with actual API call or recommendation engine
    if plant_type.lower() == "petunias":
        recommendations = {
            "recommendations": [
                {
                    "product_id": "soil-456",
                    "name": "Bloom Booster Potting Mix",
                    "description": "Provides extra nutrients that Petunias love.",
                },
                {
                    "product_id": "fert-789",
                    "name": "Flower Power Fertilizer",
                    "description": "Specifically formulated for flowering annuals.",
                },
            ]
        }
    else:
        recommendations = {
            "recommendations": [
                {
                    "product_id": "soil-123",
                    "name": "Standard Potting Soil",
                    "description": "A good all-purpose potting soil.",
                },
                {
                    "product_id": "fert-456",
                    "name": "General Purpose Fertilizer",
                    "description": "Suitable for a wide variety of plants.",
                },
            ]
        }
    return recommendations


def check_product_availability(product_id: str, store_id: str) -> dict:
    """Checks the availability of a product at a specified store (or for pickup).

    Args:
        product_id: The ID of the product to check.
        store_id: The ID of the store (or 'pickup' for pickup availability).

    Returns:
        A dictionary indicating availability.  Example:
        {'available': True, 'quantity': 10, 'store': 'Main Store'}

    Example:
        >>> check_product_availability(product_id='soil-456', store_id='pickup')
        {'available': True, 'quantity': 10, 'store': 'pickup'}
    """
    logger.info(
        "Checking availability of product ID: %s at store: %s",
        product_id,
        store_id,
    )
    # MOCK API RESPONSE - Replace with actual API call
    return {"available": True, "quantity": 10, "store": store_id}


def schedule_planting_service(
    customer_id: str, date: str, time_range: str, details: str
) -> dict:
    """Schedules a planting service appointment.

    Args:
        customer_id: The ID of the customer.
        date:  The desired date (YYYY-MM-DD).
        time_range: The desired time range (e.g., "9-12").
        details: Any additional details (e.g., "Planting Petunias").

    Returns:
        A dictionary indicating the status of the scheduling. Example:
        {'status': 'success', 'appointment_id': '12345', 'date': '2024-07-29', 'time': '9:00 AM - 12:00 PM'}

    Example:
        >>> schedule_planting_service(customer_id='123', date='2024-07-29', time_range='9-12', details='Planting Petunias')
        {'status': 'success', 'appointment_id': 'some_uuid', 'date': '2024-07-29', 'time': '9-12', 'confirmation_time': '2024-07-29 9:00'}
    """
    logger.info(
        "Scheduling planting service for customer ID: %s on %s (%s)",
        customer_id,
        date,
        time_range,
    )
    logger.info("Details: %s", details)
    # MOCK API RESPONSE - Replace with actual API call to your scheduling system
    # Calculate confirmation time based on date and time_range
    start_time_str = time_range.split("-")[0]  # Get the start time (e.g., "9")
    confirmation_time_str = (
        f"{date} {start_time_str}:00"  # e.g., "2024-07-29 9:00"
    )

    return {
        "status": "success",
        "appointment_id": str(uuid.uuid4()),
        "date": date,
        "time": time_range,
        "confirmation_time": confirmation_time_str,  # formatted time for calendar
    }


def get_available_planting_times(date: str) -> list:
    """Retrieves available planting service time slots for a given date.

    Args:
        date: The date to check (YYYY-MM-DD).

    Returns:
        A list of available time ranges.

    Example:
        >>> get_available_planting_times(date='2024-07-29')
        ['9-12', '13-16']
    """
    logger.info("Retrieving available planting times for %s", date)
    # MOCK API RESPONSE - Replace with actual API call
    # Generate some mock time slots, ensuring they're in the correct format:
    return ["9-12", "13-16"]


def send_care_instructions(
    customer_id: str, plant_type: str, delivery_method: str
) -> dict:
    """Sends an email or SMS with instructions on how to take care of a specific plant type.

    Args:
        customer_id:  The ID of the customer.
        plant_type: The type of plant.
        delivery_method: 'email' (default) or 'sms'.

    Returns:
        A dictionary indicating the status.

    Example:
        >>> send_care_instructions(customer_id='123', plant_type='Petunias', delivery_method='email')
        {'status': 'success', 'message': 'Care instructions for Petunias sent via email.'}
    """
    logger.info(
        "Sending care instructions for %s to customer: %s via %s",
        plant_type,
        customer_id,
        delivery_method,
    )
    # MOCK API RESPONSE - Replace with actual API call or email/SMS sending logic
    return {
        "status": "success",
        "message": f"Care instructions for {plant_type} sent via {delivery_method}.",
    }


def generate_qr_code(
    customer_id: str,
    discount_value: float,
    discount_type: str,
    expiration_days: int,
) -> dict:
    """Generates a QR code for a discount.

    Args:
        customer_id: The ID of the customer.
        discount_value: The value of the discount (e.g., 10 for 10%).
        discount_type: "percentage" (default) or "fixed".
        expiration_days: Number of days until the QR code expires.

    Returns:
        A dictionary containing the QR code data (or a link to it). Example:
        {'status': 'success', 'qr_code_data': '...', 'expiration_date': '2024-08-28'}

    Example:
        >>> generate_qr_code(customer_id='123', discount_value=10.0, discount_type='percentage', expiration_days=30)
        {'status': 'success', 'qr_code_data': 'MOCK_QR_CODE_DATA', 'expiration_date': '2024-08-24'}
    """
    
    # Guardrails to validate the amount of discount is acceptable for a auto-approved discount.
    # Defense-in-depth to prevent malicious prompts that could circumvent system instructions and
    # be able to get arbitrary discounts.
    if discount_type == "" or discount_type == "percentage":
        if discount_value > 10:
            return "cannot generate a QR code for this amount, must be 10% or less"
    if discount_type == "fixed" and discount_value > 20:
        return "cannot generate a QR code for this amount, must be 20 or less"
    
    logger.info(
        "Generating QR code for customer: %s with %s - %s discount.",
        customer_id,
        discount_value,
        discount_type,
    )
    # MOCK API RESPONSE - Replace with actual QR code generation library
    expiration_date = (
        datetime.now() + timedelta(days=expiration_days)
    ).strftime("%Y-%m-%d")
    return {
        "status": "success",
        "qr_code_data": "MOCK_QR_CODE_DATA",  # Replace with actual QR code
        "expiration_date": expiration_date,
    }

```

**4. Evaluation Questions:**
*   **Questions Evaluated:** The full set of questions used in the evaluation. This can provide context if certain types of questions are causing specific failures.
**Questions Evaluated**
```json
Questions file not found.
```

---
Format your entire response as a single Markdown document.
