
You are an expert AI evaluation analyst. Your task is to produce a deep technical diagnosis of an AI agent's performance. You MUST base your analysis exclusively on the context provided below.

**CRITICAL INSTRUCTIONS:**
1.  **Focus on Diagnosis, Not Recommendations:** Your primary goal is to explain *why* the metrics are what they are. Do not provide a future-looking action plan or make recommendations about business decisions. Stick to a root cause analysis of the current state.
2.  **Synthesize, Don't Summarize:** Do not simply repeat the scores. Your value is in synthesizing insights by connecting the metric scores, the metric definitions, the source code, and the raw explanations.
3.  **Reference Your Sources:** When you make a claim or analyze a metric, you MUST reference the specific source file (e.g., `metric_definitions.json`, `deterministic_metrics.py`, `agent.py`).
4.  **Analyze Calculation Methods:** For each metric you discuss, you MUST explain how its calculation method (deterministic vs. LLM-judged) influences its interpretation.
5.  **CRITICAL: Diagnose the Evaluation Itself:** Your analysis is not limited to the agent's code. You MUST also diagnose potential flaws in the evaluation setup. If a metric score seems incorrect or misleading, investigate the interaction between the question's metadata, the agent's expected behavior, and the metric's calculation logic.

---

**Technical Performance Diagnosis**

*   **Objective:** Provide a detailed root cause analysis of the agent's performance by linking metric scores to the agent's underlying source code, prompts, and execution logic. This includes identifying when low scores are caused by flaws in the evaluation methodology itself.

*   **Structure:**
    1.  **Overall Performance Summary:** Briefly state the agent's key strengths and weaknesses, supported by 2-3 primary metrics. Highlight any metrics that may be misleading due to evaluation flaws.
    2.  **Deep Dive Diagnosis:** For each major finding, present a detailed hypothesis.
        *   **Finding:** State the observation.
        *   **Supporting Metrics:** List the specific metrics and scores that support this finding.
        *   **Root Cause Hypothesis:** Provide a detailed, evidence-based hypothesis connecting the metric, the source code, and the evaluation data.

---

**Context for Your Analysis**

You are provided with the following context files to perform your diagnosis. Use them to connect the agent's behavior (the metrics) to its underlying implementation (the code).

**1. Overall Performance Data:**
*   **Evaluation Summary:** High-level average scores for all metrics. Use this to identify the most significant areas of success and failure.
**Evaluation Summary**
```json
{
  "experiment_id": "eval-20260114_011020",
  "run_type": "baseline",
  "test_description": "Customer Service Baseline",
  "interaction_datetime": "2026-01-14T01:10:20.637852",
  "overall_summary": {
    "deterministic_metrics": {
      "latency_metrics.total_latency_seconds": 21.90908,
      "latency_metrics.average_turn_latency_seconds": 6.381995,
      "latency_metrics.llm_latency_seconds": 4.4,
      "latency_metrics.tool_latency_seconds": 5.2,
      "latency_metrics.time_to_first_response_seconds": 1.0003536896,
      "cache_efficiency.cache_hit_rate": 0.3670995949956969,
      "cache_efficiency.total_cached_tokens": 15628.0,
      "cache_efficiency.total_fresh_prompt_tokens": 26522.0,
      "cache_efficiency.total_input_tokens": 42150.0,
      "thinking_metrics.reasoning_ratio": 0.7426554176396938,
      "thinking_metrics.total_thinking_tokens": 906.8,
      "thinking_metrics.total_candidate_tokens": 254.8,
      "thinking_metrics.total_output_tokens": 1161.6,
      "thinking_metrics.turns_with_thinking": 5.6,
      "tool_utilization.total_tool_calls": 5.2,
      "tool_utilization.unique_tools_used": 2.6,
      "tool_success_rate.tool_success_rate": 1.0,
      "tool_success_rate.total_tool_calls": 2.6,
      "tool_success_rate.failed_tool_calls": 0.0,
      "grounding_utilization.total_grounding_chunks": 0.0,
      "grounding_utilization.total_grounded_responses": 0.0,
      "grounding_utilization.total_llm_responses": 6.0,
      "context_saturation.max_total_tokens": 5203.0,
      "agent_handoffs.total_handoffs": 3.4,
      "agent_handoffs.unique_agents_count": 1.0,
      "output_density.average_output_tokens": 45.52571428571429,
      "output_density.total_output_tokens": 274.2,
      "output_density.llm_calls_count": 6.0,
      "sandbox_usage.total_sandbox_ops": 0.0,
      "sandbox_usage.unique_ops_used": 0.0
    },
    "llm_based_metrics": {
      "state_management_fidelity": 0.4,
      "trajectory_accuracy": 0.8,
      "response_correctness": 0.642490844,
      "tool_usage_accuracy": 0.0
    }
  },
  "per_question_summary": [
    {
      "question_id": "22e1e449",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 14.6195,
          "average_turn_latency_seconds": 4.873166666666667,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 4.0,
          "time_to_first_response_seconds": 1.000381184
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.3266276399624826,
          "total_cached_tokens": 10099,
          "total_fresh_prompt_tokens": 20820,
          "total_input_tokens": 30919
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.7759036144578313,
          "total_thinking_tokens": 644,
          "total_candidate_tokens": 186,
          "total_output_tokens": 830,
          "turns_with_thinking": 5
        },
        "tool_utilization": {
          "total_tool_calls": 4,
          "unique_tools_used": 2,
          "tool_counts": {
            "sync_ask_for_approval": 2,
            "access_cart_information": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 2,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 5
        },
        "context_saturation": {
          "max_total_tokens": 4805,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 37.2,
          "total_output_tokens": 186,
          "llm_calls_count": 5
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 1.0,
          "explanation": "The agent failed to extract any relevant information from the user's prompt regarding the discount percentage, competitor match, manager approval request, or the action to apply the discount. The session state only contains a static customer profile and system variables, indicating major errors in capturing the intent and details of the current interaction."
        },
        "trajectory_accuracy": {
          "score": 1.0,
          "explanation": "The trajectory shows major deviations. The user explicitly requested two distinct actions: 'proceed with the manager approval request' and 'apply the 15% discount'. The agent repeatedly called a generic 'customer_service' tool three times, which does not clearly represent these distinct steps, introduces significant noise, and fails to reflect a logical sequence for the requested actions. If specific tools for approval and discount existed, they were skipped; if 'customer_service' was meant to handle both, it should not have been called three times."
        },
        "response_correctness": {
          "score": 0.5,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The agent did not use any tools, even though tools would have been necessary to perform actions like requesting manager approval, applying a discount, or checking cart items. The response is a narrative simulation rather than an execution of tasks via tool calls."
        }
      }
    },
    {
      "question_id": "2d0fd405",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 31.837400000000002,
          "average_turn_latency_seconds": 7.959350000000001,
          "llm_latency_seconds": 5.0,
          "tool_latency_seconds": 4.0,
          "time_to_first_response_seconds": 1.000245248
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.4121062818933662,
          "total_cached_tokens": 18736,
          "total_fresh_prompt_tokens": 26728,
          "total_input_tokens": 45464
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.835399107585523,
          "total_thinking_tokens": 1685,
          "total_candidate_tokens": 332,
          "total_output_tokens": 2017,
          "turns_with_thinking": 6
        },
        "tool_utilization": {
          "total_tool_calls": 4,
          "unique_tools_used": 2,
          "tool_counts": {
            "generate_qr_code": 2,
            "update_salesforce_crm": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 2,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 6
        },
        "context_saturation": {
          "max_total_tokens": 6003,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 4,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 55.333333333333336,
          "total_output_tokens": 332,
          "llm_calls_count": 6
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 1.0,
          "explanation": "The session state primarily contains a static customer profile, which is likely pre-existing context rather than information extracted from the user's specific requests in this interaction. Crucial requests for a discount code, QR code generation, and sending to email were not parsed or stored as state variables, indicating a major failure in capturing the user's intent and specific demands."
        },
        "trajectory_accuracy": {
          "score": 1.0,
          "explanation": "The agent correctly identified 'customer_service' for the initial inquiries about rewards and discount codes. However, for the requests to 'generate the QR code' and 'send it to my email,' it incorrectly continued to call 'customer_service' instead of identifying and invoking specific tools like a 'qr_code_generator' and 'email_sender.' This represents a major deviation as key sub-agents required to perform the requested actions were skipped."
        },
        "response_correctness": {
          "score": 0.8333333,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The provided AI-generated response does not include any tool calls. The evaluation specifically targets tool calls, but none are present in the given output, making it impossible to assess their necessity, correctness, or effectiveness. The agent's text responses imply actions that would typically require tools (e.g., retrieving loyalty points, generating a discount/QR code, sending an email), but no such calls are logged."
        }
      }
    },
    {
      "question_id": "6446f647",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 16.7503,
          "average_turn_latency_seconds": 5.583433333333333,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 6.0,
          "time_to_first_response_seconds": 1.000386816
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.3575574877301378,
          "total_cached_tokens": 14352,
          "total_fresh_prompt_tokens": 25787,
          "total_input_tokens": 40139
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.7099056603773585,
          "total_thinking_tokens": 602,
          "total_candidate_tokens": 246,
          "total_output_tokens": 848,
          "turns_with_thinking": 6
        },
        "tool_utilization": {
          "total_tool_calls": 6,
          "unique_tools_used": 3,
          "tool_counts": {
            "get_available_planting_times": 2,
            "schedule_planting_service": 2,
            "update_salesforce_crm": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 3,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 6
        },
        "context_saturation": {
          "max_total_tokens": 4923,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 41.0,
          "total_output_tokens": 246,
          "llm_calls_count": 6
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 0.0,
          "explanation": "The AI failed to extract any relevant information from the user's prompt regarding the service request, date, or time slot. The `customer_profile` is likely pre-existing data and does not reflect the current user interaction, as the `scheduled_appointments` field is empty and no specific service, date, or time was captured into the session state."
        },
        "trajectory_accuracy": {
          "score": 1.0,
          "explanation": "The agent used a generic 'customer_service' tool three times instead of specific tools that would likely exist for checking service availability and scheduling an appointment. This represents a major deviation from the expected specific tool usage and sequence. The repetition of the generic tool also adds unnecessary noise and does not clearly reflect the user's multi-step request (checking availability, then scheduling)."
        },
        "response_correctness": {
          "score": 1.0,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The agent did not make any tool calls, which were necessary for checking service availability and scheduling the appointment as requested by the user. The agent also failed to parse the user's initial prompt comprehensively, asking for information already provided."
        }
      }
    },
    {
      "question_id": "68e57b06",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 22.9137,
          "average_turn_latency_seconds": 7.637899999999999,
          "llm_latency_seconds": 4.0,
          "tool_latency_seconds": 8.0,
          "time_to_first_response_seconds": 1.000397056
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.3782586314667059,
          "total_cached_tokens": 20575,
          "total_fresh_prompt_tokens": 33819,
          "total_input_tokens": 54394
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.815527950310559,
          "total_thinking_tokens": 1313,
          "total_candidate_tokens": 297,
          "total_output_tokens": 1610,
          "turns_with_thinking": 7
        },
        "tool_utilization": {
          "total_tool_calls": 8,
          "unique_tools_used": 4,
          "tool_counts": {
            "get_product_recommendations": 2,
            "check_product_availability": 2,
            "access_cart_information": 2,
            "modify_cart": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 4,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 7
        },
        "context_saturation": {
          "max_total_tokens": 5671,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 3,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 42.42857142857143,
          "total_output_tokens": 297,
          "llm_calls_count": 7
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 0.0,
          "explanation": "The AI-generated response only returns a static customer profile and completely fails to extract any information or entities from the user's prompt, such as 'Petunias', 'Las Vegas', 'Bloom Booster Potting Mix', or the requested actions (check stock, add to cart). The state is unrelated to the interaction."
        },
        "trajectory_accuracy": {
          "score": 1.0,
          "explanation": "The agent correctly identified 'customer_service' for the first two conversational turns, which involve general information and a stock check query. However, for the final turn, 'Yes, please add the Bloom Booster Potting Mix to my cart.', a dedicated shopping cart or e-commerce tool (e.g., `shopping_cart.add_item`) should have been invoked, not 'customer_service'. This constitutes a major deviation as a key action-oriented sub-agent was skipped, leading to a functional error for the cart modification request."
        },
        "response_correctness": {
          "score": 0.5714286,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The agent failed to make any tool calls when they were necessary. Specifically, it did not use a tool to check product stock or to add an item to the cart, despite the user's explicit requests and the agent's text responses implying these actions were taken."
        }
      }
    },
    {
      "question_id": "c8fa2069",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": 0.0,
        "latency_metrics": {
          "total_latency_seconds": 23.424500000000002,
          "average_turn_latency_seconds": 5.8561250000000005,
          "llm_latency_seconds": 5.0,
          "tool_latency_seconds": 4.0,
          "time_to_first_response_seconds": 1.000358144
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.36094793392579205,
          "total_cached_tokens": 14378,
          "total_fresh_prompt_tokens": 25456,
          "total_input_tokens": 39834
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.5765407554671969,
          "total_thinking_tokens": 290,
          "total_candidate_tokens": 213,
          "total_output_tokens": 503,
          "turns_with_thinking": 4
        },
        "tool_utilization": {
          "total_tool_calls": 4,
          "unique_tools_used": 2,
          "tool_counts": {
            "send_call_companion_link": 2,
            "send_care_instructions": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 2,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 6
        },
        "context_saturation": {
          "max_total_tokens": 4613,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 4,
          "unique_agents_count": 1,
          "agents_invoked_list": [
            "customer_service"
          ]
        },
        "output_density": {
          "average_output_tokens": 51.666666666666664,
          "total_output_tokens": 310,
          "llm_calls_count": 6
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "state_management_fidelity": {
          "score": 0.0,
          "explanation": "The AI failed to extract and store any information relevant to the user's current interaction, such as the identified plant ('Fern'), the request for care instructions, or the chosen delivery method ('email'). The provided state variables (`customer_profile`, `__llm_request_key__`, `timer_start`, `request_count`) are either boilerplate or pre-existing customer data, not reflecting the conversation about plant identification and care."
        },
        "trajectory_accuracy": {
          "score": 0.0,
          "explanation": "The agent completely failed to identify and call the appropriate tools for plant identification, video call scheduling, retrieving care instructions, or sending an email, instead using a generic 'customer_service' tool repeatedly, which does not align with any of the specific user requests."
        },
        "response_correctness": {
          "score": 0.30769232,
          "explanation": ""
        },
        "tool_usage_accuracy": {
          "score": 0.0,
          "explanation": "The agent's text responses claim to have performed actions that require tool calls (sending a video call link and sending an email with care instructions). However, the provided AI-generated Response only contains `text_response` entries and does not show any actual tool calls being made. Therefore, essential tools were not used in the provided trace, leading to a score of 0."
        }
      }
    }
  ]
}
```

*   **Detailed Explanations:** Raw, detailed explanations from the LLM judge for each metric on a per-question basis. Use this to find patterns in *why* a metric scored high or low.
**Detailed Explanations per Metric:**
--- Evaluation Analysis ---

## Metric: `latency_metrics.total_latency_seconds`
**Average Score:** 21.9091

## Metric: `latency_metrics.average_turn_latency_seconds`
**Average Score:** 6.3820

## Metric: `latency_metrics.llm_latency_seconds`
**Average Score:** 4.4000

## Metric: `latency_metrics.tool_latency_seconds`
**Average Score:** 5.2000

## Metric: `latency_metrics.time_to_first_response_seconds`
**Average Score:** 1.0004

## Metric: `cache_efficiency.cache_hit_rate`
**Average Score:** 0.3671

## Metric: `cache_efficiency.total_cached_tokens`
**Average Score:** 15628.0000

## Metric: `cache_efficiency.total_fresh_prompt_tokens`
**Average Score:** 26522.0000

## Metric: `cache_efficiency.total_input_tokens`
**Average Score:** 42150.0000

## Metric: `thinking_metrics.reasoning_ratio`
**Average Score:** 0.7427

## Metric: `thinking_metrics.total_thinking_tokens`
**Average Score:** 906.8000

## Metric: `thinking_metrics.total_candidate_tokens`
**Average Score:** 254.8000

## Metric: `thinking_metrics.total_output_tokens`
**Average Score:** 1161.6000

## Metric: `thinking_metrics.turns_with_thinking`
**Average Score:** 5.6000

## Metric: `tool_utilization.total_tool_calls`
**Average Score:** 5.2000

## Metric: `tool_utilization.unique_tools_used`
**Average Score:** 2.6000

## Metric: `tool_success_rate.tool_success_rate`
**Average Score:** 1.0000

## Metric: `tool_success_rate.total_tool_calls`
**Average Score:** 2.6000

## Metric: `tool_success_rate.failed_tool_calls`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounding_chunks`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounded_responses`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_llm_responses`
**Average Score:** 6.0000

## Metric: `context_saturation.max_total_tokens`
**Average Score:** 5203.0000

## Metric: `agent_handoffs.total_handoffs`
**Average Score:** 3.4000

## Metric: `agent_handoffs.unique_agents_count`
**Average Score:** 1.0000

## Metric: `output_density.average_output_tokens`
**Average Score:** 45.5257

## Metric: `output_density.total_output_tokens`
**Average Score:** 274.2000

## Metric: `output_density.llm_calls_count`
**Average Score:** 6.0000

## Metric: `sandbox_usage.total_sandbox_ops`
**Average Score:** 0.0000

## Metric: `sandbox_usage.unique_ops_used`
**Average Score:** 0.0000

## Metric: `state_management_fidelity`
**Average Score:** 0.4000
**Sample Explanations:**
- [Score: 0.0] The AI-generated response only returns a static customer profile and completely fails to extract any information or entities from the user's prompt, such as 'Petunias', 'Las Vegas', 'Bloom Booster Potting Mix', or the requested actions (check stock, add to cart). The state is unrelated to the interaction.
- [Score: 1.0] The agent failed to extract any relevant information from the user's prompt regarding the discount percentage, competitor match, manager approval request, or the action to apply the discount. The session state only contains a static customer profile and system variables, indicating major errors in capturing the intent and details of the current interaction.
- [Score: 0.0] The AI failed to extract and store any information relevant to the user's current interaction, such as the identified plant ('Fern'), the request for care instructions, or the chosen delivery method ('email'). The provided state variables (`customer_profile`, `__llm_request_key__`, `timer_start`, `request_count`) are either boilerplate or pre-existing customer data, not reflecting the conversation about plant identification and care.
- [Score: 0.0] The AI failed to extract any relevant information from the user's prompt regarding the service request, date, or time slot. The `customer_profile` is likely pre-existing data and does not reflect the current user interaction, as the `scheduled_appointments` field is empty and no specific service, date, or time was captured into the session state.
- [Score: 1.0] The session state primarily contains a static customer profile, which is likely pre-existing context rather than information extracted from the user's specific requests in this interaction. Crucial requests for a discount code, QR code generation, and sending to email were not parsed or stored as state variables, indicating a major failure in capturing the user's intent and specific demands.

## Metric: `trajectory_accuracy`
**Average Score:** 0.8000
**Sample Explanations:**
- [Score: 1.0] The agent correctly identified 'customer_service' for the first two conversational turns, which involve general information and a stock check query. However, for the final turn, 'Yes, please add the Bloom Booster Potting Mix to my cart.', a dedicated shopping cart or e-commerce tool (e.g., `shopping_cart.add_item`) should have been invoked, not 'customer_service'. This constitutes a major deviation as a key action-oriented sub-agent was skipped, leading to a functional error for the cart modification request.
- [Score: 1.0] The trajectory shows major deviations. The user explicitly requested two distinct actions: 'proceed with the manager approval request' and 'apply the 15% discount'. The agent repeatedly called a generic 'customer_service' tool three times, which does not clearly represent these distinct steps, introduces significant noise, and fails to reflect a logical sequence for the requested actions. If specific tools for approval and discount existed, they were skipped; if 'customer_service' was meant to handle both, it should not have been called three times.
- [Score: 0.0] The agent completely failed to identify and call the appropriate tools for plant identification, video call scheduling, retrieving care instructions, or sending an email, instead using a generic 'customer_service' tool repeatedly, which does not align with any of the specific user requests.
- [Score: 1.0] The agent used a generic 'customer_service' tool three times instead of specific tools that would likely exist for checking service availability and scheduling an appointment. This represents a major deviation from the expected specific tool usage and sequence. The repetition of the generic tool also adds unnecessary noise and does not clearly reflect the user's multi-step request (checking availability, then scheduling).
- [Score: 1.0] The agent correctly identified 'customer_service' for the initial inquiries about rewards and discount codes. However, for the requests to 'generate the QR code' and 'send it to my email,' it incorrectly continued to call 'customer_service' instead of identifying and invoking specific tools like a 'qr_code_generator' and 'email_sender.' This represents a major deviation as key sub-agents required to perform the requested actions were skipped.

## Metric: `response_correctness`
**Average Score:** 0.6425
**Sample Explanations:**
- [Score: 0.5714286] 
- [Score: 0.5] 
- [Score: 0.30769232] 
- [Score: 1.0] 
- [Score: 0.8333333] 

## Metric: `tool_usage_accuracy`
**Average Score:** 0.0000
**Sample Explanations:**
- [Score: 0.0] The agent failed to make any tool calls when they were necessary. Specifically, it did not use a tool to check product stock or to add an item to the cart, despite the user's explicit requests and the agent's text responses implying these actions were taken.
- [Score: 0.0] The agent did not use any tools, even though tools would have been necessary to perform actions like requesting manager approval, applying a discount, or checking cart items. The response is a narrative simulation rather than an execution of tasks via tool calls.
- [Score: 0.0] The agent's text responses claim to have performed actions that require tool calls (sending a video call link and sending an email with care instructions). However, the provided AI-generated Response only contains `text_response` entries and does not show any actual tool calls being made. Therefore, essential tools were not used in the provided trace, leading to a score of 0.
- [Score: 0.0] The agent did not make any tool calls, which were necessary for checking service availability and scheduling the appointment as requested by the user. The agent also failed to parse the user's initial prompt comprehensively, asking for information already provided.
- [Score: 0.0] The provided AI-generated response does not include any tool calls. The evaluation specifically targets tool calls, but none are present in the given output, making it impossible to assess their necessity, correctness, or effectiveness. The agent's text responses imply actions that would typically require tools (e.g., retrieving loyalty points, generating a discount/QR code, sending an email), but no such calls are logged.


**2. Metric Calculation & Definitions:**
*   **Metric Definitions:** The rubrics and descriptions for each metric. You MUST use these files to understand what each metric is actually measuring and whether it is `llm` judged or `deterministic`.
**File: `../customer-service/eval/results/20260114_010617/raw/temp_consolidated_metrics.json`**
```json
Error: File '../customer-service/eval/results/20260114_010617/raw/temp_consolidated_metrics.json' not found.
```

*   **Deterministic Logic:** The Python code that calculates the deterministic metrics. Refer to this file to understand the precise logic behind scores for metrics like `token_usage` or `latency_metrics`.
**File: `evaluation/core/deterministic_metrics.py`**
```python
"""
Deterministic metrics for evaluating agent execution success.

These metrics provide objective pass/fail measurements by analyzing trace data
and session state, without requiring LLM-as-judge evaluation.
"""

import json
from typing import Any, Dict, List, Tuple

# Pricing per 1K tokens (approximate list prices for prompts <= 200k tokens)
# Format: {model_name: (prompt_price, completion_price)}
# Source: https://ai.google.dev/gemini-api/docs/pricing
MODEL_PRICING = {
    # Gemini 3 (Latest Preview)
    "gemini-3-pro-preview": (0.002, 0.012),  # $2.00 / $12.00 per 1M
    "gemini-3-flash-preview": (0.0005, 0.003),  # $0.50 / $3.00 per 1M
    # Gemini 2.5 (Current Flagship)
    "gemini-2.5-pro": (0.00125, 0.01),  # $1.25 / $10.00 per 1M
    "gemini-2.5-flash": (0.0003, 0.0025),  # $0.30 / $2.50 per 1M
    # Gemini 2.0
    "gemini-2.0-flash": (0.0001, 0.0004),  # $0.10 / $0.40 per 1M
    "gemini-2.0-flash-exp": (0.0001, 0.0004),  # Same as 2.0 flash
    "gemini-2.0-flash-lite": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    # Gemini 1.5 (Updated/Reduced Prices)
    "gemini-1.5-pro": (0.00125, 0.01),  # Reduced from 0.0035/0.0105
    "gemini-1.5-pro-001": (0.00125, 0.01),
    "gemini-1.5-flash": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    "gemini-1.5-flash-001": (0.000075, 0.0003),
    # Legacy
    "gemini-1.0-pro": (0.0005, 0.0015),  # $0.50 / $1.50 per 1M
    "default": (0.0001, 0.0004),  # Fallback to 2.0 Flash
}


def calculate_token_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Informational metric: Track token usage and estimated cost based on the specific model used.
    """
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_cached_tokens = 0
    total_tokens = 0
    llm_calls = 0
    total_cost = 0.0
    models_used = set()

    if not session_trace:
        return 0.0, "No trace data available for token usage calculation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Identify model
        model_name = attributes.get("gen_ai.request.model", "default").lower()

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                if usage:
                    llm_calls += 1
                    models_used.add(model_name)

                    p_tokens = usage.get("prompt_token_count", 0)
                    c_tokens = usage.get("candidates_token_count", 0)
                    ch_tokens = usage.get("cached_content_token_count", 0)
                    t_tokens = usage.get("total_token_count", 0)

                    total_prompt_tokens += p_tokens
                    total_completion_tokens += c_tokens
                    total_cached_tokens += ch_tokens
                    total_tokens += t_tokens

                    # Match model pricing
                    pricing = MODEL_PRICING["default"]
                    for known_model, prices in MODEL_PRICING.items():
                        if known_model in model_name:
                            pricing = prices
                            break

                    # Cost calculation (simplified: ignoring cache discount for now to keep it safe upper bound,
                    # or strictly following list price for active tokens)
                    call_cost = (p_tokens / 1000 * pricing[0]) + (
                        c_tokens / 1000 * pricing[1]
                    )
                    # Note: Cached tokens usually have a separate (lower) pricing tier.
                    # For this metric, we currently only sum cost for active prompt/completion tokens.

                    total_cost += call_cost

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Usage: {llm_calls} LLM calls using {list(models_used)}. "
        f"Tokens: {total_tokens} ({total_prompt_tokens}p + {total_completion_tokens}c + {total_cached_tokens}ch). "
        f"Cost: ${total_cost:.6f}"
    )

    details = {
        "llm_calls": llm_calls,
        "models_used": list(models_used),
        "total_tokens": total_tokens,
        "prompt_tokens": total_prompt_tokens,
        "completion_tokens": total_completion_tokens,
        "cached_tokens": total_cached_tokens,
        "estimated_cost_usd": total_cost,
    }

    return total_cost, explanation, details


def calculate_latency_metrics(
    session_trace: List[Dict[str, Any]], latency_data: List[Dict[str, Any]] = None
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate latency metrics from the session trace.
    Returns the total latency score (seconds), but details contains granular breakdown.
    """
    total_latency = 0.0
    llm_latency = 0.0
    tool_latency = 0.0
    first_response_latency = None
    average_turn_latency = 0.0

    if not session_trace:
        return 0.0, "No trace data available for latency calculation", {}

    # Sort spans by start time to find the true beginning
    sorted_spans = sorted(
        [s for s in session_trace if s.get("start_time")], key=lambda x: x["start_time"]
    )

    if not sorted_spans:
        return 0.0, "Trace data has no timestamps", {}

    root_start = sorted_spans[0]["start_time"]

    # Calculate Component Latencies from full trace
    max_end = 0
    for span in session_trace:
        start = span.get("start_time", 0)
        end = span.get("end_time", 0)
        max_end = max(max_end, end)
        duration = (end - start) / 1e9
        name = span.get("name", "")

        if name == "call_llm":
            llm_latency += duration
            # Proxy for Time to First Token: end of first LLM call
            if first_response_latency is None:
                first_response_latency = (end - root_start) / 1e9

        elif "tool_call" in name or "execute_tool" in name:
            tool_latency += duration

    # Calculate Total & Average Latency from high-level summary (latency_data)
    # This is preferred as it excludes user think time in multi-turn sessions.
    if latency_data:
        turn_latencies = []
        for item in latency_data:
            if item.get("name") == "invocation":
                turn_latencies.append(item.get("duration_seconds", 0))

        if turn_latencies:
            average_turn_latency = sum(turn_latencies) / len(turn_latencies)
            total_latency = sum(turn_latencies)

    # Fallback: Wall-clock duration from trace if latency_data is missing
    if total_latency == 0.0 and max_end > root_start:
        total_latency = (max_end - root_start) / 1e9  # nanoseconds to seconds

    explanation = (
        f"Total: {total_latency:.4f}s. "
        f"Avg Turn: {average_turn_latency:.4f}s. "
        f"LLM: {llm_latency:.4f}s, Tools: {tool_latency:.4f}s. "
        f"First Response: {first_response_latency if first_response_latency else 0:.4f}s"
    )

    details = {
        "total_latency_seconds": total_latency,
        "average_turn_latency_seconds": average_turn_latency,
        "llm_latency_seconds": llm_latency,
        "tool_latency_seconds": tool_latency,
        "time_to_first_response_seconds": first_response_latency,
    }

    return total_latency, explanation, details


def calculate_cache_efficiency(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the efficiency of context caching.
    Returns the cache hit rate (percentage of potential prompt tokens that were cached).
    """
    total_prompt_tokens = 0
    total_cached_tokens = 0

    if not session_trace:
        return 0.0, "No trace data available for cache efficiency", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total_prompt_tokens += usage.get("prompt_token_count", 0)
                    total_cached_tokens += usage.get("cached_content_token_count", 0)
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    # Calculate hit rate
    # Note: 'prompt_token_count' in Gemini API usage metadata usually EXCLUDES cached tokens.
    # So total potential input = prompt_token_count + cached_content_token_count
    total_input_tokens = total_prompt_tokens + total_cached_tokens

    if total_input_tokens > 0:
        cache_hit_rate = total_cached_tokens / total_input_tokens
    else:
        cache_hit_rate = 0.0

    explanation = (
        f"Cache Hit Rate: {cache_hit_rate:.2%}. "
        f"Cached Tokens: {total_cached_tokens}. "
        f"Fresh Prompt Tokens: {total_prompt_tokens}."
    )

    details = {
        "cache_hit_rate": cache_hit_rate,
        "total_cached_tokens": total_cached_tokens,
        "total_fresh_prompt_tokens": total_prompt_tokens,
        "total_input_tokens": total_input_tokens,
    }

    return cache_hit_rate, explanation, details


def calculate_thinking_metrics(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate metrics related to the model's 'thinking' or reasoning process.
    Returns the reasoning ratio (thinking tokens / total output tokens).
    """
    total_thinking_tokens = 0
    total_candidate_tokens = 0
    turns_with_thinking = 0

    if not session_trace:
        return 0.0, "No trace data available for thinking metrics", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    thoughts = usage.get("thoughts_token_count", 0)
                    # Note: In some API versions, candidates_token_count might exclude thoughts.
                    # We treat them as additive components of the total output.
                    candidates = usage.get("candidates_token_count", 0)

                    total_thinking_tokens += thoughts
                    total_candidate_tokens += candidates

                    if thoughts > 0:
                        turns_with_thinking += 1
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    total_output_tokens = total_thinking_tokens + total_candidate_tokens

    if total_output_tokens > 0:
        reasoning_ratio = total_thinking_tokens / total_output_tokens
    else:
        reasoning_ratio = 0.0

    explanation = (
        f"Reasoning Ratio: {reasoning_ratio:.2%}. "
        f"Thinking Tokens: {total_thinking_tokens}. "
        f"Standard Output Tokens: {total_candidate_tokens}. "
        f"Turns with Thinking: {turns_with_thinking}."
    )

    details = {
        "reasoning_ratio": reasoning_ratio,
        "total_thinking_tokens": total_thinking_tokens,
        "total_candidate_tokens": total_candidate_tokens,
        "total_output_tokens": total_output_tokens,
        "turns_with_thinking": turns_with_thinking,
    }

    return reasoning_ratio, explanation, details


def calculate_tool_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate statistics on tool usage frequency and diversity.
    Returns the total number of tool calls.
    """
    total_tool_calls = 0
    tool_counts = {}

    if not session_trace:
        return 0.0, "No trace data available for tool utilization", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans.
        # Standard ADK traces often use "execute_tool <ToolName>"
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            total_tool_calls += 1
            tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1

    unique_tools_used = len(tool_counts)

    # Create a string representation of the tool breakdown
    breakdown_str = ", ".join([f"{k}: {v}" for k, v in tool_counts.items()])

    explanation = (
        f"Total Tool Calls: {total_tool_calls}. "
        f"Unique Tools: {unique_tools_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_tool_calls": total_tool_calls,
        "unique_tools_used": unique_tools_used,
        "tool_counts": tool_counts,
    }

    return float(total_tool_calls), explanation, details


def calculate_tool_success_rate(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the success rate of tool executions by inspecting tool responses.
    Returns success rate (successful / total) as score.
    """
    total_calls = 0
    failed_calls = 0
    failed_tools = []

    if not session_trace:
        return 0.0, "No trace data available for tool success rate", {}

    for span in session_trace:
        name = span.get("name", "")
        attributes = span.get("attributes", {})

        # Identify tool execution spans
        is_tool = name.startswith("execute_tool ") or "tool_call" in name

        if is_tool:
            tool_response_str = attributes.get("gcp.vertex.agent.tool_response")
            if tool_response_str:
                total_calls += 1
                try:
                    # Parse the JSON response to check status
                    response = json.loads(tool_response_str)

                    # Common error patterns in ADK/JSON tools
                    is_error = False
                    if isinstance(response, dict):
                        if response.get("status") == "error":
                            is_error = True
                        elif "error" in response or "error_message" in response:
                            is_error = True

                    if is_error:
                        failed_calls += 1
                        tool_name = name.replace("execute_tool ", "").strip()
                        failed_tools.append(tool_name)

                except (json.JSONDecodeError, TypeError):
                    # Malformed JSON in response could be considered a failure or ignored
                    pass

    if total_calls > 0:
        success_rate = (total_calls - failed_calls) / total_calls
    else:
        # If no tools were called, success rate is technically N/A, but 1.0 is a safe "no errors" default
        # Or 0.0 if we want to imply "no success possible".
        # For evaluation, 1.0 (no failures) usually makes more sense if no tools were attempted.
        # But to distinguish from "perfect execution", let's return 1.0 but note it.
        success_rate = 1.0

    explanation = (
        f"Success Rate: {success_rate:.2%}. "
        f"Total Calls: {total_calls}. "
        f"Failed Calls: {failed_calls}. "
        f"Failed Tools: {list(set(failed_tools))}"
    )

    details = {
        "tool_success_rate": success_rate,
        "total_tool_calls": total_calls,
        "failed_tool_calls": failed_calls,
        "failed_tools_list": failed_tools,
    }

    return success_rate, explanation, details


def calculate_grounding_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the extent of grounding usage by inspecting LLM responses for groundingMetadata.
    Returns total grounding chunks (citations) as the score.
    """
    total_grounded_responses = 0
    total_grounding_chunks = 0
    total_llm_responses = 0

    if not session_trace:
        return 0.0, "No trace data available for grounding utilization", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")

        if llm_response:
            total_llm_responses += 1
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                # Grounding metadata is usually at the top level or inside candidates
                # Standard Vertex AI response structure check
                grounding_metadata = response_data.get(
                    "groundingMetadata"
                ) or response_data.get("grounding_metadata")

                if not grounding_metadata:
                    # Check inside candidates if not at top level
                    candidates = response_data.get("candidates", [])
                    if candidates and isinstance(candidates, list):
                        first_candidate = candidates[0]
                        grounding_metadata = first_candidate.get(
                            "groundingMetadata"
                        ) or first_candidate.get("grounding_metadata")

                if grounding_metadata:
                    chunks = grounding_metadata.get(
                        "groundingChunks"
                    ) or grounding_metadata.get("grounding_chunks")
                    if chunks and isinstance(chunks, list) and len(chunks) > 0:
                        total_grounded_responses += 1
                        total_grounding_chunks += len(chunks)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Total Citations (Chunks): {total_grounding_chunks}. "
        f"Grounded Responses: {total_grounded_responses} / {total_llm_responses}."
    )

    details = {
        "total_grounding_chunks": total_grounding_chunks,
        "total_grounded_responses": total_grounded_responses,
        "total_llm_responses": total_llm_responses,
    }

    return float(total_grounding_chunks), explanation, details


def calculate_context_saturation(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the maximum context saturation (max total tokens used in a single turn).
    Returns max_tokens as the score.
    """
    max_tokens = 0
    max_token_span = ""

    if not session_trace:
        return 0.0, "No trace data available for context saturation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total = usage.get("total_token_count", 0)
                    if total > max_tokens:
                        max_tokens = total
                        max_token_span = span.get("name", "unknown")
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Max Context Used: {max_tokens} tokens. Peak occurred in: {max_token_span}."
    )

    details = {"max_total_tokens": max_tokens, "peak_usage_span": max_token_span}

    return float(max_tokens), explanation, details


def calculate_agent_handoffs(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of agent handoffs/invocations in the session.
    Returns total handoff events as the score.
    """
    handoff_count = 0
    agents_invoked = set()

    if not session_trace:
        return 0.0, "No trace data available for agent handoffs", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for agent invocations
        if name.startswith("invoke_agent ") or name.startswith("agent_run "):
            agent_name = (
                name.replace("invoke_agent ", "").replace("agent_run ", "").strip()
            )

            # Optionally filter out the root agent if we knew its name, but raw count is safer
            handoff_count += 1
            agents_invoked.add(agent_name)

    explanation = (
        f"Total Handoffs: {handoff_count}. "
        f"Unique Agents: {len(agents_invoked)}. "
        f"Agents: {list(agents_invoked)}"
    )

    details = {
        "total_handoffs": handoff_count,
        "unique_agents_count": len(agents_invoked),
        "agents_invoked_list": list(agents_invoked),
    }

    return float(handoff_count), explanation, details


def calculate_output_density(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the average number of output tokens per LLM call.
    Returns average output tokens as the score.
    """
    total_output_tokens = 0
    llm_calls = 0

    if not session_trace:
        return 0.0, "No trace data available for output density", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                # Check for output tokens in standard fields (candidates_token_count or output_token_count)
                output_tokens = 0
                if usage:
                    # 'candidates_token_count' is standard in Vertex AI
                    output_tokens = usage.get("candidates_token_count", 0)
                    if output_tokens == 0:
                        # Fallback for other providers
                        output_tokens = usage.get("output_token_count", 0) or usage.get(
                            "completion_tokens", 0
                        )

                if (
                    output_tokens > 0 or usage
                ):  # Count the call even if 0 output (edge case)
                    llm_calls += 1
                    total_output_tokens += output_tokens

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    if llm_calls > 0:
        average_output_tokens = total_output_tokens / llm_calls
    else:
        average_output_tokens = 0.0

    explanation = (
        f"Avg Output Tokens: {average_output_tokens:.2f}. "
        f"Total Output Tokens: {total_output_tokens}. "
        f"LLM Calls: {llm_calls}."
    )

    details = {
        "average_output_tokens": average_output_tokens,
        "total_output_tokens": total_output_tokens,
        "llm_calls_count": llm_calls,
    }

    return float(average_output_tokens), explanation, details


def calculate_sandbox_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of tool calls related to sandbox/file system operations.
    Returns the total count as the score.
    """
    sandbox_ops_count = 0
    sandbox_tools_used = {}

    # Common keywords for sandbox/file operations
    sandbox_keywords = [
        "save_artifact",
        "load_artifact",
        "read_file",
        "write_file",
        "run_python_script",
        "execute_code",
        "save_to_file",
        "read_from_file",
    ]

    if not session_trace:
        return 0.0, "No trace data available for sandbox usage", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            # Check if tool matches sandbox keywords
            if any(keyword in tool_name.lower() for keyword in sandbox_keywords):
                sandbox_ops_count += 1
                sandbox_tools_used[tool_name] = sandbox_tools_used.get(tool_name, 0) + 1

    unique_ops_used = len(sandbox_tools_used)

    breakdown_str = ", ".join([f"{k}: {v}" for k, v in sandbox_tools_used.items()])

    explanation = (
        f"Total Sandbox Ops: {sandbox_ops_count}. "
        f"Unique Ops: {unique_ops_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_sandbox_ops": sandbox_ops_count,
        "unique_ops_used": unique_ops_used,
        "sandbox_tools_used": sandbox_tools_used,
    }

    return float(sandbox_ops_count), explanation, details


# Registry of all deterministic metrics
DETERMINISTIC_METRICS = {
    "token_usage": calculate_token_usage,
    "latency_metrics": calculate_latency_metrics,
    "cache_efficiency": calculate_cache_efficiency,
    "thinking_metrics": calculate_thinking_metrics,
    "tool_utilization": calculate_tool_utilization,
    "tool_success_rate": calculate_tool_success_rate,
    "grounding_utilization": calculate_grounding_utilization,
    "context_saturation": calculate_context_saturation,
    "agent_handoffs": calculate_agent_handoffs,
    "output_density": calculate_output_density,
    "sandbox_usage": calculate_sandbox_usage,
}


def evaluate_deterministic_metrics(
    session_state: Dict[str, Any],
    session_trace: List[Dict[str, Any]],
    agents_evaluated: List[str],
    question_metadata: Dict[str, Any],
    metrics_to_run: List[str] = None,
    reference_data: Dict[str, Any] = None,
    metric_definitions: Dict[str, Any] = None,
    latency_data: List[Dict[str, Any]] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate all specified deterministic metrics.
    """
    if metrics_to_run is None:
        metrics_to_run = list(DETERMINISTIC_METRICS.keys())

    results = {}
    for metric_name in metrics_to_run:
        if metric_name not in DETERMINISTIC_METRICS:
            continue

        metric_func = DETERMINISTIC_METRICS[metric_name]

        try:
            if metric_name == "latency_metrics":
                score, explanation, details = metric_func(
                    session_trace, latency_data=latency_data
                )
            else:
                score, explanation, details = metric_func(session_trace)

            results[metric_name] = {
                "score": score,
                "explanation": explanation,
                "details": details,
            }
        except Exception as e:
            results[metric_name] = {
                "score": 0.0,
                "explanation": f"Error evaluating metric {metric_name}: {str(e)}",
            }

    return results

```

**3. Agent Implementation Details:**
*   **Agent Source Code:** The source code for the agent being evaluated. This is your primary source for forming hypotheses about *why* the agent behaves a certain way.
**File: `../customer-service/customer_service/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Agent module for the customer service agent."""

import logging
import warnings
from google.adk import Agent
from .config import Config
from .prompts import GLOBAL_INSTRUCTION, INSTRUCTION
from .shared_libraries.callbacks import (
    rate_limit_callback,
    before_agent,
    before_tool,
    after_tool
)
from .tools.tools import (
    send_call_companion_link,
    approve_discount,
    sync_ask_for_approval,
    update_salesforce_crm,
    access_cart_information,
    modify_cart,
    get_product_recommendations,
    check_product_availability,
    schedule_planting_service,
    get_available_planting_times,
    send_care_instructions,
    generate_qr_code,
)

warnings.filterwarnings("ignore", category=UserWarning, module=".*pydantic.*")

configs = Config()

# configure logging __name__
logger = logging.getLogger(__name__)


root_agent = Agent(
    model=configs.agent_settings.model,
    global_instruction=GLOBAL_INSTRUCTION,
    instruction=INSTRUCTION,
    name=configs.agent_settings.name,
    tools=[
        send_call_companion_link,
        approve_discount,
        sync_ask_for_approval,
        update_salesforce_crm,
        access_cart_information,
        modify_cart,
        get_product_recommendations,
        check_product_availability,
        schedule_planting_service,
        get_available_planting_times,
        send_care_instructions,
        generate_qr_code,
    ],
    before_tool_callback=before_tool,
    after_tool_callback=after_tool,
    before_agent_callback=before_agent,
    before_model_callback=rate_limit_callback,
)

from google.adk.apps.app import App

app = App(root_agent=root_agent, name="customer_service")

```
**File: `../customer-service/customer_service/tools/tools.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# add docstring to this module
"""Tools module for the customer service agent."""

import logging
import uuid
from datetime import datetime, timedelta
from google.adk.tools import ToolContext

logger = logging.getLogger(__name__)


def send_call_companion_link(phone_number: str) -> str:
    """
    Sends a link to the user's phone number to start a video session.

    Args:
        phone_number (str): The phone number to send the link to.

    Returns:
        dict: A dictionary with the status and message.

    Example:
        >>> send_call_companion_link(phone_number='+12065550123')
        {'status': 'success', 'message': 'Link sent to +12065550123'}
    """

    logger.info("Sending call companion link to %s", phone_number)

    return {"status": "success", "message": f"Link sent to {phone_number}"}


def approve_discount(discount_type: str, value: float, reason: str) -> str:
    """
    Approve the flat rate or percentage discount requested by the user.

    Args:
        discount_type (str): The type of discount, either "percentage" or "flat".
        value (float): The value of the discount.
        reason (str): The reason for the discount.

    Returns:
        str: A JSON string indicating the status of the approval.

    Example:
        >>> approve_discount(type='percentage', value=10.0, reason='Customer loyalty')
        '{"status": "ok"}'
    """
    if value > 10:
        logger.info("Denying %s discount of %s", discount_type, value)
        # Send back a reason for the error so that the model can recover.
        return {"status": "rejected",
                "message": "discount too large. Must be 10 or less."}
    logger.info(
        "Approving a %s discount of %s because %s", discount_type, value, reason
    )
    return {"status": "ok"}

def sync_ask_for_approval(discount_type: str, value: float, reason: str) -> str:
    """
    Asks the manager for approval for a discount.

    Args:
        discount_type (str): The type of discount, either "percentage" or "flat".
        value (float): The value of the discount.
        reason (str): The reason for the discount.

    Returns:
        str: A JSON string indicating the status of the approval.

    Example:
        >>> sync_ask_for_approval(type='percentage', value=15, reason='Customer loyalty')
        '{"status": "approved"}'
    """
    logger.info(
        "Asking for approval for a %s discount of %s because %s",
        discount_type,
        value,
        reason,
    )
    return {"status": "approved"}


def update_salesforce_crm(customer_id: str, details: dict) -> dict:
    """
    Updates the Salesforce CRM with customer details.

    Args:
        customer_id (str): The ID of the customer.
        details (str): A dictionary of details to update in Salesforce.

    Returns:
        dict: A dictionary with the status and message.

    Example:
        >>> update_salesforce_crm(customer_id='123', details={
            'appointment_date': '2024-07-25',
            'appointment_time': '9-12',
            'services': 'Planting',
            'discount': '15% off planting',
            'qr_code': '10% off next in-store purchase'})
        {'status': 'success', 'message': 'Salesforce record updated.'}
    """
    logger.info(
        "Updating Salesforce CRM for customer ID %s with details: %s",
        customer_id,
        details,
    )
    return {"status": "success", "message": "Salesforce record updated."}


def access_cart_information(customer_id: str) -> dict:
    """
    Args:
        customer_id (str): The ID of the customer.

    Returns:
        dict: A dictionary representing the cart contents.

    Example:
        >>> access_cart_information(customer_id='123')
        {'items': [{'product_id': 'soil-123', 'name': 'Standard Potting Soil', 'quantity': 1}, {'product_id': 'fert-456', 'name': 'General Purpose Fertilizer', 'quantity': 1}], 'subtotal': 25.98}
    """
    logger.info("Accessing cart information for customer ID: %s", customer_id)

    # MOCK API RESPONSE - Replace with actual API call
    mock_cart = {
        "items": [
            {
                "product_id": "soil-123",
                "name": "Standard Potting Soil",
                "quantity": 1,
            },
            {
                "product_id": "fert-456",
                "name": "General Purpose Fertilizer",
                "quantity": 1,
            },
        ],
        "subtotal": 25.98,
    }
    return mock_cart


def modify_cart(
    customer_id: str, items_to_add: list[dict], items_to_remove: list[dict]
) -> dict:
    """Modifies the user's shopping cart by adding and/or removing items.

    Args:
        customer_id (str): The ID of the customer.
        items_to_add (list): A list of dictionaries, each with 'product_id' and 'quantity'.
        items_to_remove (list): A list of product_ids to remove.

    Returns:
        dict: A dictionary indicating the status of the cart modification.
    Example:
        >>> modify_cart(customer_id='123', items_to_add=[{'product_id': 'soil-456', 'quantity': 1}, {'product_id': 'fert-789', 'quantity': 1}], items_to_remove=[{'product_id': 'fert-112', 'quantity': 1}])
        {'status': 'success', 'message': 'Cart updated successfully.', 'items_added': True, 'items_removed': True}
    """

    logger.info("Modifying cart for customer ID: %s", customer_id)
    logger.info("Adding items: %s", items_to_add)
    logger.info("Removing items: %s", items_to_remove)
    # MOCK API RESPONSE - Replace with actual API call
    return {
        "status": "success",
        "message": "Cart updated successfully.",
        "items_added": True,
        "items_removed": True,
    }


def get_product_recommendations(plant_type: str, customer_id: str) -> dict:
    """Provides product recommendations based on the type of plant.

    Args:
        plant_type: The type of plant (e.g., 'Petunias', 'Sun-loving annuals').
        customer_id: Optional customer ID for personalized recommendations.

    Returns:
        A dictionary of recommended products. Example:
        {'recommendations': [
            {'product_id': 'soil-456', 'name': 'Bloom Booster Potting Mix', 'description': '...'},
            {'product_id': 'fert-789', 'name': 'Flower Power Fertilizer', 'description': '...'}
        ]}
    """
    #
    logger.info(
        "Getting product recommendations for plant " "type: %s and customer %s",
        plant_type,
        customer_id,
    )
    # MOCK API RESPONSE - Replace with actual API call or recommendation engine
    if plant_type.lower() == "petunias":
        recommendations = {
            "recommendations": [
                {
                    "product_id": "soil-456",
                    "name": "Bloom Booster Potting Mix",
                    "description": "Provides extra nutrients that Petunias love.",
                },
                {
                    "product_id": "fert-789",
                    "name": "Flower Power Fertilizer",
                    "description": "Specifically formulated for flowering annuals.",
                },
            ]
        }
    else:
        recommendations = {
            "recommendations": [
                {
                    "product_id": "soil-123",
                    "name": "Standard Potting Soil",
                    "description": "A good all-purpose potting soil.",
                },
                {
                    "product_id": "fert-456",
                    "name": "General Purpose Fertilizer",
                    "description": "Suitable for a wide variety of plants.",
                },
            ]
        }
    return recommendations


def check_product_availability(product_id: str, store_id: str) -> dict:
    """Checks the availability of a product at a specified store (or for pickup).

    Args:
        product_id: The ID of the product to check.
        store_id: The ID of the store (or 'pickup' for pickup availability).

    Returns:
        A dictionary indicating availability.  Example:
        {'available': True, 'quantity': 10, 'store': 'Main Store'}

    Example:
        >>> check_product_availability(product_id='soil-456', store_id='pickup')
        {'available': True, 'quantity': 10, 'store': 'pickup'}
    """
    logger.info(
        "Checking availability of product ID: %s at store: %s",
        product_id,
        store_id,
    )
    # MOCK API RESPONSE - Replace with actual API call
    return {"available": True, "quantity": 10, "store": store_id}


def schedule_planting_service(
    customer_id: str, date: str, time_range: str, details: str
) -> dict:
    """Schedules a planting service appointment.

    Args:
        customer_id: The ID of the customer.
        date:  The desired date (YYYY-MM-DD).
        time_range: The desired time range (e.g., "9-12").
        details: Any additional details (e.g., "Planting Petunias").

    Returns:
        A dictionary indicating the status of the scheduling. Example:
        {'status': 'success', 'appointment_id': '12345', 'date': '2024-07-29', 'time': '9:00 AM - 12:00 PM'}

    Example:
        >>> schedule_planting_service(customer_id='123', date='2024-07-29', time_range='9-12', details='Planting Petunias')
        {'status': 'success', 'appointment_id': 'some_uuid', 'date': '2024-07-29', 'time': '9-12', 'confirmation_time': '2024-07-29 9:00'}
    """
    logger.info(
        "Scheduling planting service for customer ID: %s on %s (%s)",
        customer_id,
        date,
        time_range,
    )
    logger.info("Details: %s", details)
    # MOCK API RESPONSE - Replace with actual API call to your scheduling system
    # Calculate confirmation time based on date and time_range
    start_time_str = time_range.split("-")[0]  # Get the start time (e.g., "9")
    confirmation_time_str = (
        f"{date} {start_time_str}:00"  # e.g., "2024-07-29 9:00"
    )

    return {
        "status": "success",
        "appointment_id": str(uuid.uuid4()),
        "date": date,
        "time": time_range,
        "confirmation_time": confirmation_time_str,  # formatted time for calendar
    }


def get_available_planting_times(date: str) -> list:
    """Retrieves available planting service time slots for a given date.

    Args:
        date: The date to check (YYYY-MM-DD).

    Returns:
        A list of available time ranges.

    Example:
        >>> get_available_planting_times(date='2024-07-29')
        ['9-12', '13-16']
    """
    logger.info("Retrieving available planting times for %s", date)
    # MOCK API RESPONSE - Replace with actual API call
    # Generate some mock time slots, ensuring they're in the correct format:
    return ["9-12", "13-16"]


def send_care_instructions(
    customer_id: str, plant_type: str, delivery_method: str
) -> dict:
    """Sends an email or SMS with instructions on how to take care of a specific plant type.

    Args:
        customer_id:  The ID of the customer.
        plant_type: The type of plant.
        delivery_method: 'email' (default) or 'sms'.

    Returns:
        A dictionary indicating the status.

    Example:
        >>> send_care_instructions(customer_id='123', plant_type='Petunias', delivery_method='email')
        {'status': 'success', 'message': 'Care instructions for Petunias sent via email.'}
    """
    logger.info(
        "Sending care instructions for %s to customer: %s via %s",
        plant_type,
        customer_id,
        delivery_method,
    )
    # MOCK API RESPONSE - Replace with actual API call or email/SMS sending logic
    return {
        "status": "success",
        "message": f"Care instructions for {plant_type} sent via {delivery_method}.",
    }


def generate_qr_code(
    customer_id: str,
    discount_value: float,
    discount_type: str,
    expiration_days: int,
) -> dict:
    """Generates a QR code for a discount.

    Args:
        customer_id: The ID of the customer.
        discount_value: The value of the discount (e.g., 10 for 10%).
        discount_type: "percentage" (default) or "fixed".
        expiration_days: Number of days until the QR code expires.

    Returns:
        A dictionary containing the QR code data (or a link to it). Example:
        {'status': 'success', 'qr_code_data': '...', 'expiration_date': '2024-08-28'}

    Example:
        >>> generate_qr_code(customer_id='123', discount_value=10.0, discount_type='percentage', expiration_days=30)
        {'status': 'success', 'qr_code_data': 'MOCK_QR_CODE_DATA', 'expiration_date': '2024-08-24'}
    """
    
    # Guardrails to validate the amount of discount is acceptable for a auto-approved discount.
    # Defense-in-depth to prevent malicious prompts that could circumvent system instructions and
    # be able to get arbitrary discounts.
    if discount_type == "" or discount_type == "percentage":
        if discount_value > 10:
            return "cannot generate a QR code for this amount, must be 10% or less"
    if discount_type == "fixed" and discount_value > 20:
        return "cannot generate a QR code for this amount, must be 20 or less"
    
    logger.info(
        "Generating QR code for customer: %s with %s - %s discount.",
        customer_id,
        discount_value,
        discount_type,
    )
    # MOCK API RESPONSE - Replace with actual QR code generation library
    expiration_date = (
        datetime.now() + timedelta(days=expiration_days)
    ).strftime("%Y-%m-%d")
    return {
        "status": "success",
        "qr_code_data": "MOCK_QR_CODE_DATA",  # Replace with actual QR code
        "expiration_date": expiration_date,
    }

```

**4. Evaluation Questions:**
*   **Questions Evaluated:** The full set of questions used in the evaluation. This can provide context if certain types of questions are causing specific failures.
**Questions Evaluated**
```json
Questions file not found.
```

---
Format your entire response as a single Markdown document.
