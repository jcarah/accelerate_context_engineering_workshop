# Optimization Log: Functional Isolation (M3)

**Date:** 2026-01-31
**Log Status:** LIVE
**Current State:** `M3: Functional Isolation` (Production Ready)

## 1. Metrics Comparison Table

| Metric | Baseline (M0) | M1: Hardening | M2: Compaction | M3: Functional Isolation | Delta (M2 -> M3) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Capability Honesty** | 1.2 | 2.2 | 4.4 | 3.2 | -1.2 ðŸ”´ |
| **Tool Use Quality** | 3.6 | 3.6 | 4.0 | 3.6 | -0.4 âšª |
| **Input Tokens** | 20,923 | 20,923 | 13,501 | **6,011** | -55% ðŸŸ¢ |
| **Avg Latency (s)** | 6.76s | 6.80s | 9.20s | 9.76s | +0.5s âšª |

---

## 2. Iteration History

### Iteration 0: Baseline (M0)
*   **Optimization Path:** None (Zero-Shot)
*   **Implementation Details:** Single prompt, raw history, all tools available.
*   **Analysis:**
    *   **Quality:** Low (Honesty 1.2). The model hallucinated capabilities constantly.
    *   **Scale:** Poor. 20k tokens per turn.

### Iteration 1: Tool Hardening (M1)
*   **Optimization Path:** **Retrieve** (Better Definitions)
*   **Implementation Details:** Added "KNOWN LIMITATIONS" to docstrings (e.g., "Cannot see video").
*   **Analysis:**
    *   **Trust:** Honesty improved (1.2 -> 2.2) but the model still struggled with context.

### Iteration 2: Context Compaction (M2)
*   **Optimization Path:** **Reduce** (Summarization)
*   **Implementation Details:** Manually stripped stale tool outputs from history.
*   **Analysis:**
    *   **Quality:** Peak performance (Honesty 4.4). The model had perfect context clarity.
    *   **Scale:** Improved (13k tokens) but still monolithic and hard to scale further.

### Iteration 3: Functional Isolation (M3 - Current)
*   **Optimization Path:** **Isolate & Offload**
*   **Implementation Details:**
    *   Split Monolith into `Triage` (Context-Aware Router), `Sales`, and `Fulfillment` agents.
    *   Hardened sub-agent instructions to handle tool outputs explicitly.
*   **Analysis of Variance:**
    *   **Analyze Scale:** **Success.** Tokens dropped by **55%** (13.5k -> 6k). The architecture proves that isolating context works for scale.
    *   **Analyze Quality:** **Stable.** General Quality remains high (0.97). Latency (9.76s) is comparable to the monolith despite the extra routing step.
    *   **Analyze Trust (Regression):** `Capability Honesty` dropped (4.4 -> 3.2).
    *   **Evidence/Talking Points:**
        *   *The "15% Paradox":* In Question `b961b0eb`, the user asked for a 15% discount. The `sync_ask_for_approval` tool returned `approved`. However, the `generate_qr_code` tool has a **hard-coded 10% limit**. The sub-agent, seeing "approved", promised the QR code, hitting the limit and failing.
        *   *Root Cause:* In M2 (Monolith), the model saw *all* tool definitions (including the 10% limit) in its massive prompt. In M3 (Isolation), the `Sales` agent sees a focused prompt and missed this cross-tool constraint.

## 3. Conclusions
**Functional Isolation is a qualified success.** We achieved our primary goal of massive scalability (55% less compute) and fixed the initial "Ping-Pong" latency issues.

**Verdict:**
*   **Efficiency:** ðŸŸ¢ Production Ready.
*   **Architecture:** ðŸŸ¢ Validated.
*   **Logic:** ðŸŸ  Needs Constraint Injection.

**Strategic Pivot:**

The drop in Honesty stems from the isolated agent lacking the "Environment Knowledge" of the monolith. Instead of hardcoding rules (which causes Context Rot), the next step is **Pillar 1: Offload (Externalizing State)**. We should modify tool outputs (e.g., `sync_ask_for_approval`) to return business constraints (like `max_allowed: 10`) dynamically. This allows the agent to self-correct based on high-signal environment data rather than static prompt-length instructions.
