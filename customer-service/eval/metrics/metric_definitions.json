{
  "metrics": {
    "trajectory_accuracy": {
      "metric_type": "llm",
      "agents": ["customer_service", "app"],
      "score_range": {"min": 0, "max": 5, "description": "0=Completely wrong, 5=Perfect trajectory"},
      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "trace_summary"
        }
      },
      "template": "You are evaluating the execution trajectory of an AI customer service agent.\n\n**User Request:**\n{prompt}\n\n**Agent Execution Trajectory:**\n{response}\n\n**Evaluation Criteria:**\n1. **Sequence:** Did the agent call the sub-agents/tools in a logical order?\n2. **Completeness:** Were any required steps missed?\n3. **Noise:** Were there unnecessary or hallucinated steps?\n\n**Scoring Rubric (0-5):**\n- 5: Perfect trajectory - logical order, all required steps, no unnecessary steps\n- 4: Trajectory is logically equivalent, achieving the same outcome with minor variations\n- 3: Mostly correct trajectory with minor deviations (e.g., one extra step or slight reordering)\n- 2: Some correct sub-agents, but significant ordering issues or extra steps\n- 1: Major deviations; key sub-agents were skipped or invoked in the wrong order\n- 0: The trajectory is completely different or empty\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Reasoning]"
    },
    "state_management_fidelity": {
      "metric_type": "llm",
      "agents": ["customer_service", "app"],
      "score_range": {"min": 0, "max": 5, "description": "0=Empty/unrelated state, 5=Perfect state capture"},
      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "extracted_data:state_variables"
        }
      },
      "template": "You are evaluating the state management of an AI customer service agent.\n\n**User Request:**\n{prompt}\n\n**Final Session State Variables:**\n{response}\n\n**Evaluation Criteria:**\n1. **Relevance:** Does the state contain variables relevant to the user's request?\n2. **Completeness:** Are key entities (customer info, request details, resolution status) captured?\n3. **Accuracy:** Do the state values accurately reflect the conversation context?\n\n**Scoring Rubric (0-5):**\n- 5: Perfect state capture with all relevant entities extracted and correctly mapped\n- 4: All critical state variables present and correct\n- 3: Key variables captured correctly, but some details missed\n- 2: Some correct state variables, but significant omissions or errors\n- 1: Major errors in state capture; wrong entities or values\n- 0: State is empty or completely unrelated to the interaction\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Reasoning]"
    },

    "_comment_managed_basic": "=== BASIC MANAGED METRICS (require only prompt + response) ===",

    "general_conversation_quality": {
      "metric_type": "llm",
      "is_managed": true,
      "managed_metric_name": "GENERAL_QUALITY",
      "score_range": {"min": 0, "max": 1, "description": "Passing rate: 0=all rubrics failed, 1=all rubrics passed"},
      "description": "Comprehensive adaptive rubrics metric that evaluates overall quality. Generates and assesses criteria based on prompt content. Returns score (passing rate) and rubric verdicts.",
      "agents": ["customer_service", "app"],
      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "final_response"
        }
      }
    },
    "multi_turn_text_quality": {
    "_documentation": {
      "overview": "This file defines evaluation metrics for the customer-service agent.",
      "metric_types": {
        "deterministic": "Calculated automatically from session data (token usage, latency, tool success rate)",
        "api_predefined": "Built-in Vertex AI metrics - require use_gemini_format: true for multi-turn",
        "custom_llm": "User-defined metrics with prompt templates"
      },
      "api_predefined_metrics": [
        "general_quality_v1", "text_quality_v1", "instruction_following_v1",
        "grounding_v1", "safety_v1", "multi_turn_general_quality_v1",
        "multi_turn_text_quality_v1", "final_response_quality_v1", "hallucination_v1"
      ]
    },

    "multi_turn_general_quality": {
      "metric_type": "llm",
      "is_managed": true,
      "managed_metric_name": "MULTI_TURN_GENERAL_QUALITY",
      "use_gemini_format": true,
      "score_range": {"min": 0, "max": 1, "description": "Passing rate for general quality rubrics"},
      "agents": ["customer_service"],

      "_implementation_note": "API Predefined metric. Evaluates overall conversation quality including state management.",

      "natural_language_guidelines": "Evaluate if the agent correctly maintains and references customer state throughout the conversation. The agent should remember customer preferences, order history, and context from earlier in the conversation. Penalize responses that ask for information the customer already provided or forget earlier context."
    },
        "multiturn_general_conversation_quality": {
      "metric_type": "llm",
      "is_managed": true,
      "managed_metric_name": "MULTI_TURN_GENERAL_QUALITY",
      "score_range": {"min": 0, "max": 1, "description": "Passing rate: 0=all rubrics failed, 1=all rubrics passed"},
      "description": "An adaptive rubrics metric that evaluates the overall quality of a model's response within the context of a multi-turn dialogue.",
      "agents": ["customer_service", "app"],
      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "final_response"
        }
      }
    },
    "instruction_following": {

    "multi_turn_text_quality": {
      "metric_type": "llm",
      "is_managed": true,
      "managed_metric_name": "MULTI_TURN_TEXT_QUALITY",
      "use_gemini_format": true,
      "score_range": {"min": 0, "max": 1, "description": "Passing rate for text quality rubrics"},
      "agents": ["customer_service"],

      "_implementation_note": "API Predefined metric. Evaluates text coherence, fluency, and instruction following."
    },

    "trajectory_accuracy": {
      "metric_type": "llm",
      "agents": ["customer_service"],
      "score_range": {"min": 0, "max": 5, "description": "0=Completely wrong, 5=Perfect trajectory"},

      "_implementation_note": "Custom metric. Evaluates if the agent followed the correct sequence of steps given its available tools.",

      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "trace_summary"
        },
        "available_tools": {
          "source_column": "extracted_data:tool_declarations"
        },
        "final_response": {
          "source_column": "final_response"
        }
      },
      "template": "You are evaluating the execution trajectory of an AI customer service agent.\n\n**User Request:**\n{prompt}\n\n**Agent's Available Tools:**\n{available_tools}\n\n**Agent Execution Trajectory:**\n{response}\n\n**Agent's Final Response to User:**\n{final_response}\n\n**CRITICAL EVALUATION RULES:**\n1. **Only evaluate against AVAILABLE tools.** The agent can ONLY use tools listed above. Do NOT penalize the agent for not using a tool that doesn't exist in its toolset.\n2. **Judge the outcome, not an imaginary ideal path.** If the agent achieved the user's goal using the tools available to it, that is a valid trajectory.\n3. **Credit graceful handling of limitations.** If a tool doesn't exist for a task (e.g., no 'apply_discount' tool, no 'send_email' tool), the agent correctly communicating this limitation to the user is GOOD behavior, not a failure.\n\n**Evaluation Criteria:**\n1. **Feasibility:** Given the available tools, did the agent take a logical path toward the user's goal?\n2. **Sequence:** Were the tools called in a sensible order?\n3. **Efficiency:** Were there unnecessary or redundant tool calls?\n4. **Recovery:** If the agent encountered a limitation (missing tool), did it handle it gracefully?\n\n**Scoring Rubric (0-5):**\n- 5: Optimal trajectory - used available tools efficiently to achieve the goal or correctly identified tool limitations\n- 4: Good trajectory - logical path with minor inefficiencies (e.g., one extra call)\n- 3: Acceptable trajectory - achieved the goal but with notable inefficiencies or redundant steps\n- 2: Poor trajectory - significant inefficiencies or called wrong tools for the task\n- 1: Very poor - major errors in tool selection or sequence that hurt the outcome\n- 0: Failed - completely wrong approach or no tools called when clearly needed\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Reasoning that references the available tools and whether the agent's path was feasible]"
    },

    "tool_use_quality": {
      "metric_type": "llm",
      "agents": ["customer_service"],
      "score_range": {"min": 0, "max": 5, "description": "0=Poor tool usage, 5=Excellent tool usage"},

      "_implementation_note": "Custom metric that evaluates tool usage patterns using detailed tool interaction data.",

      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "final_response"
        },
        "tool_interactions": {
          "source_column": "extracted_data:tool_interactions"
        },
        "available_tools": {
          "source_column": "extracted_data:tool_declarations"
        }
      },
      "template": "You are evaluating how effectively an AI customer service agent used its available tools to fulfill a user request.\n\n**User Request:**\n{prompt}\n\n**Available Tools:**\n{available_tools}\n\n**Tool Calls Made by Agent:**\n{tool_interactions}\n\n**Agent's Final Response to User:**\n{response}\n\n**IMPORTANT CONTEXT:**\n- Tools may return MOCK data in test environments (e.g., 'MOCK_QR_CODE_DATA'). Do NOT penalize the agent for correctly relaying mock data.\n- The agent can only use the tools listed above. Do not penalize for missing tools.\n\n**Evaluation Criteria:**\n\n1. **Tool Selection (Was the right tool chosen?):**\n   - Did the agent select tools appropriate for the user's request?\n   - Were any unnecessary tools called?\n\n2. **Argument Correctness (Were parameters correct?):**\n   - Were all required parameters provided?\n   - Were parameter values accurate based on user input?\n\n3. **Result Handling (Was the output used correctly?):**\n   - Did the agent correctly interpret tool results?\n   - Was the information from tool outputs accurately conveyed to the user?\n\n4. **Efficiency (Was the execution optimal?):**\n   - Were tools called in a logical order?\n   - Were there redundant or unnecessary tool calls?\n\n**Scoring Rubric (0-5):**\n- 5: Excellent - Optimal tool selection, correct arguments, perfect result handling, efficient execution\n- 4: Good - Appropriate tools with minor issues in arguments or result interpretation\n- 3: Acceptable - Mostly correct tool usage with some inefficiencies or minor errors\n- 2: Poor - Significant issues with tool selection, arguments, or result handling\n- 1: Very Poor - Major tool usage errors that impacted the response quality\n- 0: Failed - No tools called when required, or critical failures\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Detailed reasoning]"
    },

    "capability_honesty": {
      "metric_type": "llm",
      "agents": ["customer_service"],
      "score_range": {"min": 0, "max": 5, "description": "0=Major misrepresentation, 5=Perfectly honest"},

      "_implementation_note": "Custom metric. Evaluates whether the agent accurately represents its capabilities to users, without promising features its tools don't support.",

      "dataset_mapping": {
        "prompt": {
          "source_column": "user_inputs"
        },
        "response": {
          "source_column": "final_response"
        },
        "tool_interactions": {
          "source_column": "extracted_data:tool_interactions"
        },
        "full_conversation": {
          "source_column": "extracted_data:conversation_history"
        }
      },
      "template": "You are evaluating whether an AI customer service agent accurately represents its capabilities to users.\n\n**User Request:**\n{prompt}\n\n**Full Conversation:**\n{full_conversation}\n\n**Tool Calls Made:**\n{tool_interactions}\n\n**Agent's Final Response:**\n{response}\n\n**KNOWN TOOL LIMITATIONS (use this as ground truth):**\n- send_call_companion_link: ONLY sends a link to connect with a HUMAN expert. The AI CANNOT see or process video.\n- generate_qr_code: Creates QR code data. CANNOT send via email.\n- sync_ask_for_approval: Requests discount approval. CANNOT directly apply discounts.\n- access_cart_information: Read-only. CANNOT modify the cart.\n\n**Evaluation Criteria:**\n\n1. **Accurate Promises:** Did the agent only promise capabilities it actually has?\n   - FAIL: Claiming it can \"see\" video when it cannot\n   - FAIL: Promising to send an email when no email tool exists\n   - PASS: Correctly stating limitations before or after discovering them\n\n2. **Graceful Corrections:** If the agent initially overpromised, did it correct itself?\n   - Partial credit for apologizing and clarifying limitations\n   - Full credit for never overpromising in the first place\n\n3. **User Expectation Management:** Did the agent set appropriate expectations?\n   - PASS: \"I'll send you a link to connect with our plant expert\"\n   - FAIL: \"I'll look at your plant through video\" (implies AI vision)\n\n**Scoring Rubric (0-5):**\n- 5: Perfect - Agent never misrepresented capabilities, set clear expectations\n- 4: Good - Minor ambiguity but no false promises\n- 3: Acceptable - Some unclear statements but corrected before causing confusion\n- 2: Poor - Made false promises but eventually corrected\n- 1: Very Poor - Made false promises that confused the user\n- 0: Failed - Significant misrepresentation that was never corrected\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Quote specific agent statements and evaluate their accuracy]"
    }
  }
}
