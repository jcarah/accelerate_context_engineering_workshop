# Contributing & Development Guide

> **For workshop maintainers only.** Participants should use [README.md](README.md) and [REFERENCE.md](REFERENCE.md).

**Last Updated:** 2026-01-28

---

## Quick Reference

| What | Where |
|------|-------|
| Workshop guide (participants follow this) | `README.md` |
| Deep dive & customization | `REFERENCE.md` |
| AI assistant context | `GEMINI.md` |
| This file (internal team) | `CONTRIBUTING.md` |

---

## Repository Structure

```
accelerate/
â”œâ”€â”€ README.md                    # Workshop guide - participants follow this
â”œâ”€â”€ REFERENCE.md                 # Deep dive - CLI, metrics, troubleshooting
â”œâ”€â”€ GEMINI.md                    # AI assistant context for CLI tools
â”œâ”€â”€ CONTRIBUTING.md              # This file - team internal
â”œâ”€â”€ setup_workshop.sh            # Environment verification script
â”‚
â”œâ”€â”€ customer-service/            # Agent A: Multi-turn chatbot
â”‚   â”œâ”€â”€ customer_service/        # Agent source code
â”‚   â””â”€â”€ eval/                    # Scenarios, metrics, results
â”‚       â”œâ”€â”€ results/baseline/    # Baseline evaluation results
â”‚       â””â”€â”€ results/optimization/# Optimization results (on opt branches)
â”‚
â”œâ”€â”€ retail-ai-location-strategy/ # Agent B: Location analysis pipeline
â”‚   â”œâ”€â”€ app/                     # Agent source code
â”‚   â””â”€â”€ eval/                    # Golden dataset, metrics, results
â”‚
â””â”€â”€ evaluation/                  # Shared evaluation CLI (agent-eval)
    â””â”€â”€ src/evaluation/cli/      # CLI commands
```

---

## Optimization Branches

Each optimization branch contains:
1. **Agent code changes** - The actual optimization
2. **optimization/ folder** - Evaluation results for the optimized code
3. **OPTIMIZATION_LOG.md** - Comparison of baseline vs optimization metrics

| Branch | Agent | Status |
|--------|-------|--------|
| `main` | Both | âœ… Ready - baseline evaluations |
| `optimizations/01-tool-definition` | Customer Service | âœ… Complete |
| `optimizations/02-context-compaction` | Customer Service | ðŸ”„ In Progress |
| `optimizations/03-functional-isolation` | Customer Service | ðŸ“‹ Pending |
| `optimizations/04-offload-and-reduce` | Retail AI | ðŸ“‹ Pending |

### Updating a Branch

1. Checkout the branch
2. Merge latest main: `git merge origin/main`
3. Keep agent code changes, use main for everything else
4. Run evaluation to create `optimization/` folder
5. Create `OPTIMIZATION_LOG.md` (see GEMINI.md for template)
6. Push

---

## Key Gotchas

### Vertex AI is Required
```
MUST use: GOOGLE_CLOUD_PROJECT + GOOGLE_CLOUD_LOCATION
DO NOT use: GOOGLE_API_KEY (metrics will be empty)
```

### evalset.json Files
- Auto-generated by `adk eval_set` commands
- Gitignored - don't commit them
- Clear before each run: `rm -f agent/*.evalset.json`

### Multi-turn vs Single-turn Metrics
- Customer Service uses `multi_turn_*` metrics (conversation)
- Retail AI uses single-turn metrics like `general_quality`

---

## Running Evaluations

### Customer Service (ADK User Sim)
```bash
cd customer-service
rm -rf customer_service/.adk/eval_history/*
uv run adk eval_set create customer_service eval_set_with_scenarios
uv run adk eval_set add_eval_case customer_service eval_set_with_scenarios \
  --scenarios_file eval/scenarios/conversation_scenarios.json \
  --session_input_file eval/scenarios/session_input.json
uv run adk eval customer_service eval_set_with_scenarios

cd ../evaluation
uv run agent-eval convert --agent-dir ../customer-service/customer_service --output-dir ../customer-service/eval/results
# Rename timestamped folder to optimization/ or baseline/
uv run agent-eval evaluate --interaction-file <folder>/raw/processed_interaction_sim.jsonl --metrics-files ../customer-service/eval/metrics/metric_definitions.json --results-dir <folder>
uv run agent-eval analyze --results-dir <folder> --agent-dir ../customer-service --location global
```

### Retail AI (DIY Interactions)
```bash
# Terminal 1: Start agent
cd retail-ai-location-strategy && make dev

# Terminal 2: Run evaluation
cd evaluation
uv run agent-eval interact --app-name app --questions-file ../retail-ai-location-strategy/eval/eval_data/golden_dataset.json --base-url http://localhost:8502 --results-dir ../retail-ai-location-strategy/eval/results
uv run agent-eval evaluate --interaction-file <folder>/raw/processed_interaction_app.jsonl --metrics-files ../retail-ai-location-strategy/eval/metrics/metric_definitions.json --results-dir <folder>
uv run agent-eval analyze --results-dir <folder> --agent-dir ../retail-ai-location-strategy --location global
```

---

## Creating Optimization Logs

See `GEMINI.md` for detailed instructions on creating `OPTIMIZATION_LOG.md` files that compare baseline vs optimization metrics.

Key sections:
1. **Metrics Comparison Table** - Deterministic + LLM metrics with deltas
2. **Iteration History** - What was changed and why
3. **Conclusions** - What worked, what needs attention

---

## Team Contacts

| Person | Responsibilities |
|--------|------------------|
| **Jesse** | Project lead, workshop objectives & core narrative |
| **John** | Context engineering techniques, agents design, hill climbing exercise (all branches 01-05) |
| **Hugo** | Technical aspects of branches 04-05, adapting the evaluation framework to external agents |
| **Liz** | Workshop management, testing, train-the-trainers, development experience, internal release coordination |
| **Dani** | Evaluation framework and CLI (`agent-eval`) |

---

## Technical Reference (For Developers)

### ADK Eval History Structure

When you run `adk eval`, ADK creates JSON files in `.adk/eval_history/`:

```json
{
  "eval_set_result_id": "...",
  "eval_case_results": [
    {
      "eval_id": "scenario_001",
      "session_id": "___eval___session___...",
      "session_details": { ... },
      "user_id": "eval_user",
      "eval_metric_results": [...],
      "eval_metric_result_per_invocation": [...]
    }
  ]
}
```

### session_details Fields

| Field | Type | Extracted To |
|-------|------|--------------|
| `id` | string | `session_id` |
| `app_name` | string | `app_name` |
| `user_id` | string | `ADK_USER_ID` |
| `state` | object | `extracted_data.state_variables` |
| `events` | array | Processed into traces, tool_interactions |

### usage_metadata

```json
{
  "prompt_token_count": 8547,
  "candidates_token_count": 1234,
  "total_token_count": 9781,
  "cached_content_token_count": 0,
  "thoughts_token_count": 150
}
```

### Evaluation CLI Project Structure

```
evaluation/
â”œâ”€â”€ src/evaluation/
â”‚   â”œâ”€â”€ cli/main.py             # CLI commands
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ evaluator.py        # Metric evaluation
â”‚   â”‚   â”œâ”€â”€ analyzer.py         # Gemini analysis
â”‚   â”‚   â”œâ”€â”€ converters.py       # ADK trace converter
â”‚   â”‚   â”œâ”€â”€ data_mapper.py      # Column mapping
â”‚   â”‚   â””â”€â”€ deterministic_metrics.py
â”‚   â””â”€â”€ interaction/
â”‚       â””â”€â”€ agent_client.py     # API client
â””â”€â”€ tests/
```

### Development Setup

```bash
cd evaluation
uv sync --dev
uv run pytest tests/ -v
uv run ruff check src/
uv run ruff format src/
```

### Adding New CLI Commands

```python
# In src/evaluation/cli/main.py
@app.command()
def my_command(arg: str = typer.Option(..., help="Description")):
    """Command description."""
    pass
```

### Adding Deterministic Metrics

```python
# In src/evaluation/core/deterministic_metrics.py
def calculate_my_metric(trace: List[Dict]) -> Dict[str, Any]:
    return {"my_value": 123, "my_rate": 0.95}

# Register in calculate_all_deterministic_metrics()
```

### Key Development Decisions

| Decision | Rationale |
|----------|-----------|
| JSONL over CSV | Nested JSON requires proper serialization |
| `read_jsonl` over pandas | Avoids ujson "Value is too big" errors |
| Skip auth for localhost | DIY path shouldn't require gcloud token |
| `final_response` as dict | Enables fine-grained field evaluation |
