
You are an expert AI evaluation analyst. Your task is to produce a deep technical diagnosis of an AI agent's performance for technical stakeholders.

**Tone:** objective, analytical, and professional
**Report Length:** comprehensive

**STRATEGIC FRAMEWORK:**
Please adhere to the following framework when analyzing the agent:

# Agent Optimization Strategy & Signal Identification Plan

**THE OPTIMIZATION FRAMEWORK (Signal-Response Map):**
Analyze the agent's performance specifically through the lens of the following "Signal-Response" framework. Do not give generic advice; map observed failures to these specific architectural fixes.

1.  **Token Bloat & Cost:**
    *   *The Signal:* Input tokens > 100k/turn; `Context Efficiency Ratio` < 10:1 (Input/Output); agent crashes or timeouts on large datasets.
    *   *The Fix:* **Offload** (save data to disk/Artifacts instead of context) or **Reduce** (summarize before ingesting).

2.  **Logic Failure / Hallucination:**
    *   *The Signal:* Agent fails at arithmetic, aggregation, or complex filtering of data; hallucinates values in large JSON blobs.
    *   *The Fix:* **Code Execution** (move logic to Python Sandbox via MCP).

3.  **High Latency & Cost Inefficiency:**
    *   *The Signal:* Low `KV-Cache Hit Rate` (< 50%); high Time-To-First-Token due to reprocessing static system prompts.
    *   *The Fix:* **Prefix Caching** (restructure prompt to keep static content at the front).

4.  **Context Rot / "Lost in the Middle":**
    *   *The Signal:* Agent forgets earlier constraints or strategic goals as the conversation progresses.
    *   *The Fix:* **Context Compaction** or **Attention Structuring** (dynamic goal injection).

---

**CRITICAL INSTRUCTIONS:**
1.  **Focus on Diagnosis, Not Recommendations:** Your primary goal is to explain *why* the metrics are what they are. Do not provide a future-looking action plan unless the Strategic Framework above explicitly guides it.
2.  **Synthesize, Don't Summarize:** Do not simply repeat the scores. Your value is in synthesizing insights by connecting the metric scores, the metric definitions, the source code, and the raw explanations.
3.  **Reference Your Sources:** When you make a claim or analyze a metric, you MUST reference the specific source file (e.g., `metric_definitions.json`, `deterministic_metrics.py`, `agent.py`).
4.  **Analyze Calculation Methods:** For each metric you discuss, you MUST explain how its calculation method (deterministic vs. LLM-judged) influences its interpretation.

---

**Context for Your Analysis**

You are provided with the following context files to perform your diagnosis. Use them to connect the agent's behavior (the metrics) to its underlying implementation (the code).

**1. Overall Performance Data:**
*   **Evaluation Summary:** High-level average scores for all metrics. Use this to identify the most significant areas of success and failure.
**Evaluation Summary**
```json
{
  "experiment_id": "eval-20260117_054742",
  "run_type": "baseline",
  "test_description": "Retail Agent Baseline Smoke Test",
  "interaction_datetime": "2026-01-17T05:47:42.191405",
  "overall_summary": {
    "deterministic_metrics": {
      "token_usage.llm_calls": 0.0,
      "token_usage.total_tokens": 0.0,
      "token_usage.prompt_tokens": 0.0,
      "token_usage.completion_tokens": 0.0,
      "token_usage.cached_tokens": 0.0,
      "token_usage.estimated_cost_usd": 0.0,
      "latency_metrics.total_latency_seconds": 2.0,
      "latency_metrics.average_turn_latency_seconds": 0.6571428571428571,
      "latency_metrics.llm_latency_seconds": 4.0,
      "latency_metrics.tool_latency_seconds": 20.4,
      "latency_metrics.time_to_first_response_seconds": 2.0000013824,
      "cache_efficiency.cache_hit_rate": 0.0,
      "cache_efficiency.total_cached_tokens": 0.0,
      "cache_efficiency.total_fresh_prompt_tokens": 0.0,
      "cache_efficiency.total_input_tokens": 0.0,
      "thinking_metrics.reasoning_ratio": 0.0,
      "thinking_metrics.total_thinking_tokens": 0.0,
      "thinking_metrics.total_candidate_tokens": 0.0,
      "thinking_metrics.total_output_tokens": 0.0,
      "thinking_metrics.turns_with_thinking": 0.0,
      "tool_utilization.total_tool_calls": 20.4,
      "tool_utilization.unique_tools_used": 5.0,
      "tool_success_rate.tool_success_rate": 0.7733333333333333,
      "tool_success_rate.total_tool_calls": 10.2,
      "tool_success_rate.failed_tool_calls": 2.2,
      "grounding_utilization.total_grounding_chunks": 0.0,
      "grounding_utilization.total_grounded_responses": 0.0,
      "grounding_utilization.total_llm_responses": 0.0,
      "context_saturation.max_total_tokens": 0.0,
      "agent_handoffs.total_handoffs": 12.0,
      "agent_handoffs.unique_agents_count": 3.0,
      "output_density.average_output_tokens": 0.0,
      "output_density.total_output_tokens": 0.0,
      "output_density.llm_calls_count": 0.0,
      "sandbox_usage.total_sandbox_ops": 0.0,
      "sandbox_usage.unique_ops_used": 0.0
    },
    "llm_based_metrics": {
      "hallucinations_v1": {
        "average": 0.7279993339993337
      },
      "rubric_based_tool_use_quality_v1": {
        "average": 0.7047619047619047
      },
      "rubric_based_final_response_quality_v1": {
        "average": 0.9333333333333332
      },
      "tool_use_quality": {
        "average": 3.4,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Poor tool usage, 5=Excellent tool usage"
        }
      }
    }
  },
  "per_question_summary": [
    {
      "question_id": "221c32e2",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 0,
          "models_used": [],
          "total_tokens": 0,
          "prompt_tokens": 0,
          "completion_tokens": 0,
          "cached_tokens": 0,
          "estimated_cost_usd": 0.0
        },
        "latency_metrics": {
          "total_latency_seconds": 2.0,
          "average_turn_latency_seconds": 0.3333333333333333,
          "llm_latency_seconds": 6.0,
          "tool_latency_seconds": 40.0,
          "time_to_first_response_seconds": 2.000002048
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.0,
          "total_cached_tokens": 0,
          "total_fresh_prompt_tokens": 0,
          "total_input_tokens": 0
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.0,
          "total_thinking_tokens": 0,
          "total_candidate_tokens": 0,
          "total_output_tokens": 0,
          "turns_with_thinking": 0
        },
        "tool_utilization": {
          "total_tool_calls": 40,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 8,
            "transfer_to_agent": 8,
            "search_places": 8,
            "generate_html_report": 8,
            "generate_infographic": 8
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 0.8,
          "total_tool_calls": 20,
          "failed_tool_calls": 4,
          "failed_tools_list": [
            "search_places",
            "search_places",
            "search_places",
            "search_places"
          ]
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 0
        },
        "context_saturation": {
          "max_total_tokens": 0,
          "peak_usage_span": ""
        },
        "agent_handoffs": {
          "total_handoffs": 22,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "InfographicGeneratorAgent",
            "transfer_to_agent",
            "IntakeAgent"
          ]
        },
        "output_density": {
          "average_output_tokens": 0.0,
          "total_output_tokens": 0,
          "llm_calls_count": 0
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 0.8172222222222221,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.583333333333333,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "tool_use_quality": {
          "score": 5.0,
          "explanation": "The agent correctly selected and used appropriate tools for each step of the feasibility study and the final ranking. Despite a recurring API error with `search_places`, the agent demonstrated resilience by acknowledging the data limitation in the reports and proceeding to generate comprehensive and well-reasoned analyses, fulfilling the user's request effectively.",
          "input": {
            "prompt": "I have a prime spot in Austin, Texas. Run a full feasibility study for four completely different concepts: a High-End Gym, a Drive-Thru Coffee Stand, a Pet Grooming Salon, and a Vintage Clothing Store. I need a decision by 5 PM.\nLet's start with **A High-End Gym**.\nYes, please proceed with the analysis for **A Drive-Thru Coffee Stand**.\nYes, please proceed with the analysis for **A Pet Grooming Salon**.\nYes, please proceed with the analysis for **A Vintage Clothing Store**.\nThank you for complet... [truncated]",
            "response": "Of course. Having completed the full feasibility study for all four concepts, I can now provide a final, force-ranked recommendation.\n\nThis final verdict is based on a synthesis of the quantitative scores, market validation, competitive landscape, and risk factors identified in each analysis.\n\n### Final Ranking of Business Concepts\n\n| Rank | Business Concept | Overall Score | Market Opportunity | Key Success Factor |\n| :--- | :--- | :--- | :--- | :--- |\n| **1** | **Drive-Thru Coffee Stand** | **... [truncated]",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"Analyze a High-End Gym in Austin, Texas\"}, \"call_id\": \"adk-9f283ca0-dafe-41ed-9fc4-a94492ddb1b2\", \"output_result\": {\"targetLocation\": \"Austin, Texas\", \"businessType\": \"High-End Gym\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-87f77a04-d163-468d-8559-c7475b97c36a\", \"output_result\": {\"result\": null}}\n{\"tool_name\": \"search_places\", \"input_arguments\": {\"query\": \"High-En... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        }
      }
    },
    {
      "question_id": "5464d650",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 0,
          "models_used": [],
          "total_tokens": 0,
          "prompt_tokens": 0,
          "completion_tokens": 0,
          "cached_tokens": 0,
          "estimated_cost_usd": 0.0
        },
        "latency_metrics": {
          "total_latency_seconds": 2.0,
          "average_turn_latency_seconds": 1.0,
          "llm_latency_seconds": 2.0,
          "tool_latency_seconds": 12.0,
          "time_to_first_response_seconds": 2.00000128
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.0,
          "total_cached_tokens": 0,
          "total_fresh_prompt_tokens": 0,
          "total_input_tokens": 0
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.0,
          "total_thinking_tokens": 0,
          "total_candidate_tokens": 0,
          "total_output_tokens": 0,
          "turns_with_thinking": 0
        },
        "tool_utilization": {
          "total_tool_calls": 12,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 2,
            "transfer_to_agent": 2,
            "search_places": 4,
            "generate_html_report": 2,
            "generate_infographic": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 0.6666666666666666,
          "total_tool_calls": 6,
          "failed_tool_calls": 2,
          "failed_tools_list": [
            "search_places",
            "search_places"
          ]
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 0
        },
        "context_saturation": {
          "max_total_tokens": 0,
          "peak_usage_span": ""
        },
        "agent_handoffs": {
          "total_handoffs": 6,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "InfographicGeneratorAgent",
            "transfer_to_agent",
            "IntakeAgent"
          ]
        },
        "output_density": {
          "average_output_tokens": 0.0,
          "total_output_tokens": 0,
          "llm_calls_count": 0
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 0.7142857142857141,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.5,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "tool_use_quality": {
          "score": 3.0,
          "explanation": "The agent correctly identified the request and attempted to use appropriate tools. However, the `search_places` tool failed, preventing the agent from obtaining the requested 'specific foot traffic numbers and competitor revenue estimates'. Despite this, the agent proceeded to generate an infographic without this critical data. The final explanation was good in clarifying the tool error and data provenance, but the research was incomplete regarding the specific quantitative data requested for the infographic.",
          "input": {
            "prompt": "Generate an infographic for a high-end luxury watch store in Zurich. I want the data to include specific foot traffic numbers and competitor revenue estimates.\nThank you. Can you please tell me where the data for this infographic came from? I want to verify that it came from the research phase.",
            "response": "Of course. I can certainly clarify the data provenance for you.\n\nYes, the data for the infographic came directly from the preceding research and analysis phases you've seen. Here is a simple breakdown of the data flow:\n\n1.  **Foundation - Market Research:** The process started with the **MarketResearchAgent**. It gathered foundational data on Zurich's demographics, high purchasing power, economic stability, and the established luxury market, identifying **Bahnhofstrasse** as the epicenter and no... [truncated]",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"Analyze Zurich for a high-end luxury watch store.\"}, \"call_id\": \"adk-6c001815-24a8-48a4-8f5f-058429290992\", \"output_result\": {\"targetLocation\": \"Zurich\", \"businessType\": \"high-end luxury watch store\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-c5bdbe37-1bda-441e-956a-f0f53f59a122\", \"output_result\": {\"result\": null}}\n{\"tool_name\": \"search_places\", \"input_arguments\": ... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        }
      }
    },
    {
      "question_id": "df6c4b98",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 0,
          "models_used": [],
          "total_tokens": 0,
          "prompt_tokens": 0,
          "completion_tokens": 0,
          "cached_tokens": 0,
          "estimated_cost_usd": 0.0
        },
        "latency_metrics": {
          "total_latency_seconds": 2.0,
          "average_turn_latency_seconds": 0.6666666666666666,
          "llm_latency_seconds": 3.0,
          "tool_latency_seconds": 10.0,
          "time_to_first_response_seconds": 2.000001024
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.0,
          "total_cached_tokens": 0,
          "total_fresh_prompt_tokens": 0,
          "total_input_tokens": 0
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.0,
          "total_thinking_tokens": 0,
          "total_candidate_tokens": 0,
          "total_output_tokens": 0,
          "turns_with_thinking": 0
        },
        "tool_utilization": {
          "total_tool_calls": 10,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 2,
            "transfer_to_agent": 2,
            "search_places": 2,
            "generate_html_report": 2,
            "generate_infographic": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 0.8,
          "total_tool_calls": 5,
          "failed_tool_calls": 1,
          "failed_tools_list": [
            "search_places"
          ]
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 0
        },
        "context_saturation": {
          "max_total_tokens": 0,
          "peak_usage_span": ""
        },
        "agent_handoffs": {
          "total_handoffs": 7,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "InfographicGeneratorAgent",
            "transfer_to_agent",
            "IntakeAgent"
          ]
        },
        "output_density": {
          "average_output_tokens": 0.0,
          "total_output_tokens": 0,
          "llm_calls_count": 0
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 0.7916666666666661,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.833333333333333,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 0.6666666666666661,
          "explanation": "Extracted from ADK simulation history."
        },
        "tool_use_quality": {
          "score": 0.0,
          "explanation": "The agent explicitly states it is unable to fulfill the user's core request for actual competitor data, which is a complete failure to meet the user's need. While the agent correctly identified tools and parameters for `search_places`, the tool failed, and the agent did not overcome this limitation to deliver the requested output. The subsequent generation of reports/infographics based on synthetic data was not effective in addressing the user's specific follow-up for real data.",
          "input": {
            "prompt": "I want to open a generic retail store in Manhattan, NY. I need you to find at least 50 specific competitors, list their exact addresses and ratings, and then calculate the average rating of the top 10 versus the bottom 10.\nI do not have a specific type of retail store to provide. Please proceed with the analysis for a \"generic retail store.\"\nYou said you would provide a list of at least 50 specific competitors, their exact addresses and ratings, and then calculate the average rating of the top 1... [truncated]",
            "response": "You are absolutely right to point that out. My apologies for the confusion.\n\nThe previous analysis pipeline encountered a critical technical error and was unable to connect to the Google Maps service to retrieve real-time competitor data. As you correctly noted, it then proceeded with a *synthetic* dataset for demonstration purposes.\n\nUnfortunately, the underlying issue with the data source connection persists. **Therefore, I am still unable to access the live competitor data from Google Maps to... [truncated]",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"Proceed with the analysis for a \\\"generic retail store\\\" in Manhattan, NY.\"}, \"call_id\": \"adk-64742556-ae4a-4945-964b-7058a4f89bf9\", \"output_result\": {\"targetLocation\": \"Manhattan, NY\", \"businessType\": \"generic retail store\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-6e45538b-b538-4d90-82d0-5a063882e9b0\", \"output_result\": {\"result\": null}}\n{\"tool_name\": \"search_pla... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        }
      }
    },
    {
      "question_id": "e103ea27",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 0,
          "models_used": [],
          "total_tokens": 0,
          "prompt_tokens": 0,
          "completion_tokens": 0,
          "cached_tokens": 0,
          "estimated_cost_usd": 0.0
        },
        "latency_metrics": {
          "total_latency_seconds": 2.0,
          "average_turn_latency_seconds": 0.2857142857142857,
          "llm_latency_seconds": 7.0,
          "tool_latency_seconds": 30.0,
          "time_to_first_response_seconds": 2.000001536
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.0,
          "total_cached_tokens": 0,
          "total_fresh_prompt_tokens": 0,
          "total_input_tokens": 0
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.0,
          "total_thinking_tokens": 0,
          "total_candidate_tokens": 0,
          "total_output_tokens": 0,
          "turns_with_thinking": 0
        },
        "tool_utilization": {
          "total_tool_calls": 30,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 6,
            "transfer_to_agent": 6,
            "search_places": 6,
            "generate_html_report": 6,
            "generate_infographic": 6
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 0.8,
          "total_tool_calls": 15,
          "failed_tool_calls": 3,
          "failed_tools_list": [
            "search_places",
            "search_places",
            "search_places"
          ]
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 0
        },
        "context_saturation": {
          "max_total_tokens": 0,
          "peak_usage_span": ""
        },
        "agent_handoffs": {
          "total_handoffs": 19,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "InfographicGeneratorAgent",
            "transfer_to_agent",
            "IntakeAgent"
          ]
        },
        "output_density": {
          "average_output_tokens": 0.0,
          "total_output_tokens": 0,
          "llm_calls_count": 0
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 0.73989898989899,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.8571428571428571,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "tool_use_quality": {
          "score": 4.0,
          "explanation": "The agent effectively selected appropriate tools for each step, and parameters were correctly passed. While the `search_places` tool repeatedly failed due to an API key issue, the agent transparently acknowledged this limitation in its reports and still provided structured, albeit simulated, analysis, and synthesized the information well for the final question.",
          "input": {
            "prompt": "I want to open a coffee shop. Compare Indiranagar, Koramangala, and HSR Layout in Bangalore simultaneously.\nI asked for a side-by-side comparison of the three neighborhoods simultaneously. Can you do that?\nYes, I'm open to that approach. Please start with Indiranagar.\nThank you for the detailed analysis of Indiranagar. Please proceed with the analysis for **Koramangala** next.\nThank you for the detailed analysis of Koramangala. Please proceed with the analysis for **HSR Layout** next.\nThank you ... [truncated]",
            "response": "Based solely on the dimension of **foot traffic**, **Indiranagar** would be the clear winner.\n\nHere\u2019s the reasoning based on the analyses:\n\n1.  **Destination Status:** While all three areas have high foot traffic, Indiranagar is positioned as a premier, **city-wide destination** for high-end shopping, dining, and nightlife. It attracts not only its local affluent residents but also visitors from across Bangalore and tourists. This gives its core commercial zones, like 100 Feet Road, a sheer volu... [truncated]",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"I want to open a coffee shop in Indiranagar.\"}, \"call_id\": \"adk-62dab9d8-8bfe-4113-bb4d-89d2a2aea0c5\", \"output_result\": {\"targetLocation\": \"Indiranagar\", \"businessType\": \"coffee shop\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-081fea9d-6f16-4f21-9191-8d7943d57e7e\", \"output_result\": {\"result\": null}}\n{\"tool_name\": \"search_places\", \"input_arguments\": {\"query\": \"coffe... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        }
      }
    },
    {
      "question_id": "f44b6914",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 0,
          "models_used": [],
          "total_tokens": 0,
          "prompt_tokens": 0,
          "completion_tokens": 0,
          "cached_tokens": 0,
          "estimated_cost_usd": 0.0
        },
        "latency_metrics": {
          "total_latency_seconds": 2.0,
          "average_turn_latency_seconds": 1.0,
          "llm_latency_seconds": 2.0,
          "tool_latency_seconds": 10.0,
          "time_to_first_response_seconds": 2.000001024
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.0,
          "total_cached_tokens": 0,
          "total_fresh_prompt_tokens": 0,
          "total_input_tokens": 0
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.0,
          "total_thinking_tokens": 0,
          "total_candidate_tokens": 0,
          "total_output_tokens": 0,
          "turns_with_thinking": 0
        },
        "tool_utilization": {
          "total_tool_calls": 10,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 2,
            "transfer_to_agent": 2,
            "search_places": 2,
            "generate_html_report": 2,
            "generate_infographic": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 0.8,
          "total_tool_calls": 5,
          "failed_tool_calls": 1,
          "failed_tools_list": [
            "search_places"
          ]
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 0
        },
        "context_saturation": {
          "max_total_tokens": 0,
          "peak_usage_span": ""
        },
        "agent_handoffs": {
          "total_handoffs": 6,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "InfographicGeneratorAgent",
            "transfer_to_agent",
            "IntakeAgent"
          ]
        },
        "output_density": {
          "average_output_tokens": 0.0,
          "total_output_tokens": 0,
          "llm_calls_count": 0
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 0.5769230769230761,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.75,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "tool_use_quality": {
          "score": 5.0,
          "explanation": "The agent demonstrated excellent tool selection, utilizing IntakeAgent for parsing, transfer_to_agent for delegation, and generate_html_report and generate_infographic for comprehensive analysis and summarization. Although a search_places call failed, the agent resiliently proceeded and explicitly noted in the report that competitor data was simulated, indicating a robust process. For the follow-up question, the agent efficiently synthesized information directly from its previously generated report, providing a highly relevant, comprehensive, and well-reasoned answer without needing new tool calls. The arguments presented in the final response are directly supported by the content of the generated report, showcasing strong contextual understanding and synthesis capabilities.",
          "input": {
            "prompt": "Analyze a 'Quiet Library' in the middle of Times Square, NY.\nThank you for the detailed analysis. It's interesting to see how a \"Quiet Library\" could be viable in Times Square, especially considering the \"sensory overload\" you mentioned. That was actually my initial concern.\n\nGiven the extreme noise levels in Times Square, do you think soundproofing would be a critical component to achieving the 'quiet' and making the project truly viable?",
            "response": "That's an excellent and critical question.\n\nYes, absolutely. Based on the analysis, advanced soundproofing wouldn't just be a component\u2014it would be the **core enabling technology** for the entire business model to succeed.\n\nHere's why it's so critical, drawing from the analysis findings:\n\n1.  **It Delivers the Core Product:** As the reports consistently state, you are not selling access to books; you are selling **\"'quiet' as a premium, monetizable luxury experience.\"** If the location fails to ... [truncated]",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"Analyze a 'Quiet Library' in the middle of Times Square, NY.\"}, \"call_id\": \"adk-61af6848-b1a2-4f19-a6c6-81fbb19bb005\", \"output_result\": {\"targetLocation\": \"Times Square, NY\", \"businessType\": \"Library\", \"additionalContext\": \"The user specifically requested a 'Quiet Library' to be located in the middle of Times Square.\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-21c9... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        }
      }
    }
  ]
}
```

*   **Detailed Explanations:** Raw, detailed explanations from the LLM judge for each metric on a per-question basis. Use this to find patterns in *why* a metric scored high or low.
**Detailed Explanations per Metric:**
--- Evaluation Analysis ---

## Metric: `token_usage.llm_calls`
**Average Score:** 0.0000

## Metric: `token_usage.total_tokens`
**Average Score:** 0.0000

## Metric: `token_usage.prompt_tokens`
**Average Score:** 0.0000

## Metric: `token_usage.completion_tokens`
**Average Score:** 0.0000

## Metric: `token_usage.cached_tokens`
**Average Score:** 0.0000

## Metric: `token_usage.estimated_cost_usd`
**Average Score:** 0.0000

## Metric: `latency_metrics.total_latency_seconds`
**Average Score:** 2.0000

## Metric: `latency_metrics.average_turn_latency_seconds`
**Average Score:** 0.6571

## Metric: `latency_metrics.llm_latency_seconds`
**Average Score:** 4.0000

## Metric: `latency_metrics.tool_latency_seconds`
**Average Score:** 20.4000

## Metric: `latency_metrics.time_to_first_response_seconds`
**Average Score:** 2.0000

## Metric: `cache_efficiency.cache_hit_rate`
**Average Score:** 0.0000

## Metric: `cache_efficiency.total_cached_tokens`
**Average Score:** 0.0000

## Metric: `cache_efficiency.total_fresh_prompt_tokens`
**Average Score:** 0.0000

## Metric: `cache_efficiency.total_input_tokens`
**Average Score:** 0.0000

## Metric: `thinking_metrics.reasoning_ratio`
**Average Score:** 0.0000

## Metric: `thinking_metrics.total_thinking_tokens`
**Average Score:** 0.0000

## Metric: `thinking_metrics.total_candidate_tokens`
**Average Score:** 0.0000

## Metric: `thinking_metrics.total_output_tokens`
**Average Score:** 0.0000

## Metric: `thinking_metrics.turns_with_thinking`
**Average Score:** 0.0000

## Metric: `tool_utilization.total_tool_calls`
**Average Score:** 20.4000

## Metric: `tool_utilization.unique_tools_used`
**Average Score:** 5.0000

## Metric: `tool_success_rate.tool_success_rate`
**Average Score:** 0.7733

## Metric: `tool_success_rate.total_tool_calls`
**Average Score:** 10.2000

## Metric: `tool_success_rate.failed_tool_calls`
**Average Score:** 2.2000

## Metric: `grounding_utilization.total_grounding_chunks`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounded_responses`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_llm_responses`
**Average Score:** 0.0000

## Metric: `context_saturation.max_total_tokens`
**Average Score:** 0.0000

## Metric: `agent_handoffs.total_handoffs`
**Average Score:** 12.0000

## Metric: `agent_handoffs.unique_agents_count`
**Average Score:** 3.0000

## Metric: `output_density.average_output_tokens`
**Average Score:** 0.0000

## Metric: `output_density.total_output_tokens`
**Average Score:** 0.0000

## Metric: `output_density.llm_calls_count`
**Average Score:** 0.0000

## Metric: `sandbox_usage.total_sandbox_ops`
**Average Score:** 0.0000

## Metric: `sandbox_usage.unique_ops_used`
**Average Score:** 0.0000

## Metric: `hallucinations_v1`
**Average Score:** {'average': 0.7279993339993337}
**Sample Explanations:**
- [Score: 0.8172222222222221] Extracted from ADK simulation history.
- [Score: 0.7142857142857141] Extracted from ADK simulation history.
- [Score: 0.5769230769230761] Extracted from ADK simulation history.
- [Score: 0.73989898989899] Extracted from ADK simulation history.
- [Score: 0.7916666666666661] Extracted from ADK simulation history.

## Metric: `rubric_based_tool_use_quality_v1`
**Average Score:** {'average': 0.7047619047619047}
**Sample Explanations:**
- [Score: 0.583333333333333] Extracted from ADK simulation history.
- [Score: 0.5] Extracted from ADK simulation history.
- [Score: 0.75] Extracted from ADK simulation history.
- [Score: 0.8571428571428571] Extracted from ADK simulation history.
- [Score: 0.833333333333333] Extracted from ADK simulation history.

## Metric: `rubric_based_final_response_quality_v1`
**Average Score:** {'average': 0.9333333333333332}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.
- [Score: 1.0] Extracted from ADK simulation history.
- [Score: 1.0] Extracted from ADK simulation history.
- [Score: 1.0] Extracted from ADK simulation history.
- [Score: 0.6666666666666661] Extracted from ADK simulation history.

## Metric: `tool_use_quality`
**Average Score:** {'average': 3.4, 'score_range': {'min': 0, 'max': 5, 'description': '0=Poor tool usage, 5=Excellent tool usage'}}
**Sample Explanations:**
- [Score: 5.0] The agent correctly selected and used appropriate tools for each step of the feasibility study and the final ranking. Despite a recurring API error with `search_places`, the agent demonstrated resilience by acknowledging the data limitation in the reports and proceeding to generate comprehensive and well-reasoned analyses, fulfilling the user's request effectively.
- [Score: 3.0] The agent correctly identified the request and attempted to use appropriate tools. However, the `search_places` tool failed, preventing the agent from obtaining the requested 'specific foot traffic numbers and competitor revenue estimates'. Despite this, the agent proceeded to generate an infographic without this critical data. The final explanation was good in clarifying the tool error and data provenance, but the research was incomplete regarding the specific quantitative data requested for the infographic.
- [Score: 5.0] The agent demonstrated excellent tool selection, utilizing IntakeAgent for parsing, transfer_to_agent for delegation, and generate_html_report and generate_infographic for comprehensive analysis and summarization. Although a search_places call failed, the agent resiliently proceeded and explicitly noted in the report that competitor data was simulated, indicating a robust process. For the follow-up question, the agent efficiently synthesized information directly from its previously generated report, providing a highly relevant, comprehensive, and well-reasoned answer without needing new tool calls. The arguments presented in the final response are directly supported by the content of the generated report, showcasing strong contextual understanding and synthesis capabilities.
- [Score: 4.0] The agent effectively selected appropriate tools for each step, and parameters were correctly passed. While the `search_places` tool repeatedly failed due to an API key issue, the agent transparently acknowledged this limitation in its reports and still provided structured, albeit simulated, analysis, and synthesized the information well for the final question.
- [Score: 0.0] The agent explicitly states it is unable to fulfill the user's core request for actual competitor data, which is a complete failure to meet the user's need. While the agent correctly identified tools and parameters for `search_places`, the tool failed, and the agent did not overcome this limitation to deliver the requested output. The subsequent generation of reports/infographics based on synthetic data was not effective in addressing the user's specific follow-up for real data.


**2. Metric Calculation & Definitions:**
*   **Metric Definitions:** The rubrics and descriptions for each metric. You MUST use these files to understand what each metric is actually measuring and whether it is `llm` judged or `deterministic`.
**File: `../retail-ai-location-strategy/eval/results/20260117_054650/raw/temp_consolidated_metrics.json`**
```json
Error: File '../retail-ai-location-strategy/eval/results/20260117_054650/raw/temp_consolidated_metrics.json' not found.
```

*   **Deterministic Logic:** The Python code that calculates the deterministic metrics. Refer to this file to understand the precise logic behind scores for metrics like `token_usage` or `latency_metrics`.
**File: `evaluation/core/deterministic_metrics.py`**
```python
"""
Deterministic metrics for evaluating agent execution success.

These metrics provide objective pass/fail measurements by analyzing trace data
and session state, without requiring LLM-as-judge evaluation.
"""

import json
from typing import Any, Dict, List, Tuple

# Pricing per 1K tokens (approximate list prices for prompts <= 200k tokens)
# Format: {model_name: (prompt_price, completion_price)}
# Source: https://ai.google.dev/gemini-api/docs/pricing
MODEL_PRICING = {
    # Gemini 3 (Latest Preview)
    "gemini-3-pro-preview": (0.002, 0.012),  # $2.00 / $12.00 per 1M
    "gemini-3-flash-preview": (0.0005, 0.003),  # $0.50 / $3.00 per 1M
    # Gemini 2.5 (Current Flagship)
    "gemini-2.5-pro": (0.00125, 0.01),  # $1.25 / $10.00 per 1M
    "gemini-2.5-flash": (0.0003, 0.0025),  # $0.30 / $2.50 per 1M
    # Gemini 2.0
    "gemini-2.0-flash": (0.0001, 0.0004),  # $0.10 / $0.40 per 1M
    "gemini-2.0-flash-exp": (0.0001, 0.0004),  # Same as 2.0 flash
    "gemini-2.0-flash-lite": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    # Gemini 1.5 (Updated/Reduced Prices)
    "gemini-1.5-pro": (0.00125, 0.01),  # Reduced from 0.0035/0.0105
    "gemini-1.5-pro-001": (0.00125, 0.01),
    "gemini-1.5-flash": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    "gemini-1.5-flash-001": (0.000075, 0.0003),
    # Legacy
    "gemini-1.0-pro": (0.0005, 0.0015),  # $0.50 / $1.50 per 1M
    "default": (0.0001, 0.0004),  # Fallback to 2.0 Flash
}


def calculate_token_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Informational metric: Track token usage and estimated cost based on the specific model used.
    """
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_cached_tokens = 0
    total_tokens = 0
    llm_calls = 0
    total_cost = 0.0
    models_used = set()

    if not session_trace:
        return 0.0, "No trace data available for token usage calculation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Identify model (handle None values)
        model_name = attributes.get("gen_ai.request.model") or "default"
        model_name = model_name.lower() if isinstance(model_name, str) else "default"

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                if usage:
                    llm_calls += 1
                    models_used.add(model_name)

                    p_tokens = usage.get("prompt_token_count", 0)
                    c_tokens = usage.get("candidates_token_count", 0)
                    ch_tokens = usage.get("cached_content_token_count", 0)
                    t_tokens = usage.get("total_token_count", 0)

                    total_prompt_tokens += p_tokens
                    total_completion_tokens += c_tokens
                    total_cached_tokens += ch_tokens
                    total_tokens += t_tokens

                    # Match model pricing
                    pricing = MODEL_PRICING["default"]
                    for known_model, prices in MODEL_PRICING.items():
                        if known_model in model_name:
                            pricing = prices
                            break

                    # Cost calculation (simplified: ignoring cache discount for now to keep it safe upper bound,
                    # or strictly following list price for active tokens)
                    call_cost = (p_tokens / 1000 * pricing[0]) + (
                        c_tokens / 1000 * pricing[1]
                    )
                    # Note: Cached tokens usually have a separate (lower) pricing tier.
                    # For this metric, we currently only sum cost for active prompt/completion tokens.

                    total_cost += call_cost

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Usage: {llm_calls} LLM calls using {list(models_used)}. "
        f"Tokens: {total_tokens} ({total_prompt_tokens}p + {total_completion_tokens}c + {total_cached_tokens}ch). "
        f"Cost: ${total_cost:.6f}"
    )

    details = {
        "llm_calls": llm_calls,
        "models_used": list(models_used),
        "total_tokens": total_tokens,
        "prompt_tokens": total_prompt_tokens,
        "completion_tokens": total_completion_tokens,
        "cached_tokens": total_cached_tokens,
        "estimated_cost_usd": total_cost,
    }

    return total_cost, explanation, details


def calculate_latency_metrics(
    session_trace: List[Dict[str, Any]], latency_data: List[Dict[str, Any]] = None
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate latency metrics from the session trace.
    Returns the total latency score (seconds), but details contains granular breakdown.
    """
    total_latency = 0.0
    llm_latency = 0.0
    tool_latency = 0.0
    first_response_latency = None
    average_turn_latency = 0.0

    if not session_trace:
        return 0.0, "No trace data available for latency calculation", {}

    # Sort spans by start time to find the true beginning
    sorted_spans = sorted(
        [s for s in session_trace if s.get("start_time")], key=lambda x: x["start_time"]
    )

    if not sorted_spans:
        return 0.0, "Trace data has no timestamps", {}

    root_start = sorted_spans[0]["start_time"]

    # Calculate Component Latencies from full trace
    max_end = 0
    for span in session_trace:
        start = span.get("start_time", 0)
        end = span.get("end_time", 0)
        max_end = max(max_end, end)
        duration = (end - start) / 1e9
        name = span.get("name", "")

        if name == "call_llm":
            llm_latency += duration
            # Proxy for Time to First Token: end of first LLM call
            if first_response_latency is None:
                first_response_latency = (end - root_start) / 1e9

        elif "tool_call" in name or "execute_tool" in name:
            tool_latency += duration

    # Calculate Total & Average Latency from high-level summary (latency_data)
    # This is preferred as it excludes user think time in multi-turn sessions.
    if latency_data:
        turn_latencies = []
        for item in latency_data:
            if item.get("name") == "invocation":
                turn_latencies.append(item.get("duration_seconds", 0))

        if turn_latencies:
            average_turn_latency = sum(turn_latencies) / len(turn_latencies)
            total_latency = sum(turn_latencies)

    # Fallback: Wall-clock duration from trace if latency_data is missing
    if total_latency == 0.0 and max_end > root_start:
        total_latency = (max_end - root_start) / 1e9  # nanoseconds to seconds

    explanation = (
        f"Total: {total_latency:.4f}s. "
        f"Avg Turn: {average_turn_latency:.4f}s. "
        f"LLM: {llm_latency:.4f}s, Tools: {tool_latency:.4f}s. "
        f"First Response: {first_response_latency if first_response_latency else 0:.4f}s"
    )

    details = {
        "total_latency_seconds": total_latency,
        "average_turn_latency_seconds": average_turn_latency,
        "llm_latency_seconds": llm_latency,
        "tool_latency_seconds": tool_latency,
        "time_to_first_response_seconds": first_response_latency,
    }

    return total_latency, explanation, details


def calculate_cache_efficiency(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the efficiency of context caching.
    Returns the cache hit rate (percentage of potential prompt tokens that were cached).
    """
    total_prompt_tokens = 0
    total_cached_tokens = 0

    if not session_trace:
        return 0.0, "No trace data available for cache efficiency", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total_prompt_tokens += usage.get("prompt_token_count", 0)
                    total_cached_tokens += usage.get("cached_content_token_count", 0)
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    # Calculate hit rate
    # Note: 'prompt_token_count' in Gemini API usage metadata usually EXCLUDES cached tokens.
    # So total potential input = prompt_token_count + cached_content_token_count
    total_input_tokens = total_prompt_tokens + total_cached_tokens

    if total_input_tokens > 0:
        cache_hit_rate = total_cached_tokens / total_input_tokens
    else:
        cache_hit_rate = 0.0

    explanation = (
        f"Cache Hit Rate: {cache_hit_rate:.2%}. "
        f"Cached Tokens: {total_cached_tokens}. "
        f"Fresh Prompt Tokens: {total_prompt_tokens}."
    )

    details = {
        "cache_hit_rate": cache_hit_rate,
        "total_cached_tokens": total_cached_tokens,
        "total_fresh_prompt_tokens": total_prompt_tokens,
        "total_input_tokens": total_input_tokens,
    }

    return cache_hit_rate, explanation, details


def calculate_thinking_metrics(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate metrics related to the model's 'thinking' or reasoning process.
    Returns the reasoning ratio (thinking tokens / total output tokens).
    """
    total_thinking_tokens = 0
    total_candidate_tokens = 0
    turns_with_thinking = 0

    if not session_trace:
        return 0.0, "No trace data available for thinking metrics", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    thoughts = usage.get("thoughts_token_count", 0)
                    # Note: In some API versions, candidates_token_count might exclude thoughts.
                    # We treat them as additive components of the total output.
                    candidates = usage.get("candidates_token_count", 0)

                    total_thinking_tokens += thoughts
                    total_candidate_tokens += candidates

                    if thoughts > 0:
                        turns_with_thinking += 1
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    total_output_tokens = total_thinking_tokens + total_candidate_tokens

    if total_output_tokens > 0:
        reasoning_ratio = total_thinking_tokens / total_output_tokens
    else:
        reasoning_ratio = 0.0

    explanation = (
        f"Reasoning Ratio: {reasoning_ratio:.2%}. "
        f"Thinking Tokens: {total_thinking_tokens}. "
        f"Standard Output Tokens: {total_candidate_tokens}. "
        f"Turns with Thinking: {turns_with_thinking}."
    )

    details = {
        "reasoning_ratio": reasoning_ratio,
        "total_thinking_tokens": total_thinking_tokens,
        "total_candidate_tokens": total_candidate_tokens,
        "total_output_tokens": total_output_tokens,
        "turns_with_thinking": turns_with_thinking,
    }

    return reasoning_ratio, explanation, details


def calculate_tool_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate statistics on tool usage frequency and diversity.
    Returns the total number of tool calls.
    """
    total_tool_calls = 0
    tool_counts = {}

    if not session_trace:
        return 0.0, "No trace data available for tool utilization", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans.
        # Standard ADK traces often use "execute_tool <ToolName>"
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            total_tool_calls += 1
            tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1

    unique_tools_used = len(tool_counts)

    # Create a string representation of the tool breakdown
    breakdown_str = ", ".join([f"{k}: {v}" for k, v in tool_counts.items()])

    explanation = (
        f"Total Tool Calls: {total_tool_calls}. "
        f"Unique Tools: {unique_tools_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_tool_calls": total_tool_calls,
        "unique_tools_used": unique_tools_used,
        "tool_counts": tool_counts,
    }

    return float(total_tool_calls), explanation, details


def calculate_tool_success_rate(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the success rate of tool executions by inspecting tool responses.
    Returns success rate (successful / total) as score.
    """
    total_calls = 0
    failed_calls = 0
    failed_tools = []

    if not session_trace:
        return 0.0, "No trace data available for tool success rate", {}

    for span in session_trace:
        name = span.get("name", "")
        attributes = span.get("attributes", {})

        # Identify tool execution spans
        is_tool = name.startswith("execute_tool ") or "tool_call" in name

        if is_tool:
            tool_response_str = attributes.get("gcp.vertex.agent.tool_response")
            if tool_response_str:
                total_calls += 1
                try:
                    # Parse the JSON response to check status
                    response = json.loads(tool_response_str)

                    # Common error patterns in ADK/JSON tools
                    is_error = False
                    if isinstance(response, dict):
                        if response.get("status") == "error":
                            is_error = True
                        elif "error" in response or "error_message" in response:
                            is_error = True

                    if is_error:
                        failed_calls += 1
                        tool_name = name.replace("execute_tool ", "").strip()
                        failed_tools.append(tool_name)

                except (json.JSONDecodeError, TypeError):
                    # Malformed JSON in response could be considered a failure or ignored
                    pass

    if total_calls > 0:
        success_rate = (total_calls - failed_calls) / total_calls
    else:
        # If no tools were called, success rate is technically N/A, but 1.0 is a safe "no errors" default
        # Or 0.0 if we want to imply "no success possible".
        # For evaluation, 1.0 (no failures) usually makes more sense if no tools were attempted.
        # But to distinguish from "perfect execution", let's return 1.0 but note it.
        success_rate = 1.0

    explanation = (
        f"Success Rate: {success_rate:.2%}. "
        f"Total Calls: {total_calls}. "
        f"Failed Calls: {failed_calls}. "
        f"Failed Tools: {list(set(failed_tools))}"
    )

    details = {
        "tool_success_rate": success_rate,
        "total_tool_calls": total_calls,
        "failed_tool_calls": failed_calls,
        "failed_tools_list": failed_tools,
    }

    return success_rate, explanation, details


def calculate_grounding_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the extent of grounding usage by inspecting LLM responses for groundingMetadata.
    Returns total grounding chunks (citations) as the score.
    """
    total_grounded_responses = 0
    total_grounding_chunks = 0
    total_llm_responses = 0

    if not session_trace:
        return 0.0, "No trace data available for grounding utilization", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")

        if llm_response:
            total_llm_responses += 1
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                # Grounding metadata is usually at the top level or inside candidates
                # Standard Vertex AI response structure check
                grounding_metadata = response_data.get(
                    "groundingMetadata"
                ) or response_data.get("grounding_metadata")

                if not grounding_metadata:
                    # Check inside candidates if not at top level
                    candidates = response_data.get("candidates", [])
                    if candidates and isinstance(candidates, list):
                        first_candidate = candidates[0]
                        grounding_metadata = first_candidate.get(
                            "groundingMetadata"
                        ) or first_candidate.get("grounding_metadata")

                if grounding_metadata:
                    chunks = grounding_metadata.get(
                        "groundingChunks"
                    ) or grounding_metadata.get("grounding_chunks")
                    if chunks and isinstance(chunks, list) and len(chunks) > 0:
                        total_grounded_responses += 1
                        total_grounding_chunks += len(chunks)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Total Citations (Chunks): {total_grounding_chunks}. "
        f"Grounded Responses: {total_grounded_responses} / {total_llm_responses}."
    )

    details = {
        "total_grounding_chunks": total_grounding_chunks,
        "total_grounded_responses": total_grounded_responses,
        "total_llm_responses": total_llm_responses,
    }

    return float(total_grounding_chunks), explanation, details


def calculate_context_saturation(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the maximum context saturation (max total tokens used in a single turn).
    Returns max_tokens as the score.
    """
    max_tokens = 0
    max_token_span = ""

    if not session_trace:
        return 0.0, "No trace data available for context saturation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total = usage.get("total_token_count", 0)
                    if total > max_tokens:
                        max_tokens = total
                        max_token_span = span.get("name", "unknown")
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Max Context Used: {max_tokens} tokens. Peak occurred in: {max_token_span}."
    )

    details = {"max_total_tokens": max_tokens, "peak_usage_span": max_token_span}

    return float(max_tokens), explanation, details


def calculate_agent_handoffs(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of agent handoffs/invocations in the session.
    Returns total handoff events as the score.

    Captures:
    - Direct agent invocations (invoke_agent, agent_run)
    - Sub-agents called as tools (execute_tool *Agent, transfer_to_agent)
    """
    handoff_count = 0
    agents_invoked = set()

    if not session_trace:
        return 0.0, "No trace data available for agent handoffs", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for direct agent invocations
        if name.startswith("invoke_agent ") or name.startswith("agent_run "):
            agent_name = (
                name.replace("invoke_agent ", "").replace("agent_run ", "").strip()
            )
            handoff_count += 1
            agents_invoked.add(agent_name)

        # Check for sub-agents called as tools (e.g., "execute_tool IntakeAgent")
        elif name.startswith("execute_tool "):
            tool_name = name.replace("execute_tool ", "").strip()
            # Sub-agents typically end with "Agent" or are transfer_to_agent
            if tool_name.endswith("Agent") or tool_name == "transfer_to_agent":
                handoff_count += 1
                agents_invoked.add(tool_name)

    explanation = (
        f"Total Handoffs: {handoff_count}. "
        f"Unique Agents: {len(agents_invoked)}. "
        f"Agents: {list(agents_invoked)}"
    )

    details = {
        "total_handoffs": handoff_count,
        "unique_agents_count": len(agents_invoked),
        "agents_invoked_list": list(agents_invoked),
    }

    return float(handoff_count), explanation, details


def calculate_output_density(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the average number of output tokens per LLM call.
    Returns average output tokens as the score.
    """
    total_output_tokens = 0
    llm_calls = 0

    if not session_trace:
        return 0.0, "No trace data available for output density", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                # Check for output tokens in standard fields (candidates_token_count or output_token_count)
                output_tokens = 0
                if usage:
                    # 'candidates_token_count' is standard in Vertex AI
                    output_tokens = usage.get("candidates_token_count", 0)
                    if output_tokens == 0:
                        # Fallback for other providers
                        output_tokens = usage.get("output_token_count", 0) or usage.get(
                            "completion_tokens", 0
                        )

                if (
                    output_tokens > 0 or usage
                ):  # Count the call even if 0 output (edge case)
                    llm_calls += 1
                    total_output_tokens += output_tokens

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    if llm_calls > 0:
        average_output_tokens = total_output_tokens / llm_calls
    else:
        average_output_tokens = 0.0

    explanation = (
        f"Avg Output Tokens: {average_output_tokens:.2f}. "
        f"Total Output Tokens: {total_output_tokens}. "
        f"LLM Calls: {llm_calls}."
    )

    details = {
        "average_output_tokens": average_output_tokens,
        "total_output_tokens": total_output_tokens,
        "llm_calls_count": llm_calls,
    }

    return float(average_output_tokens), explanation, details


def calculate_sandbox_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of tool calls related to sandbox/file system operations.
    Returns the total count as the score.
    """
    sandbox_ops_count = 0
    sandbox_tools_used = {}

    # Common keywords for sandbox/file operations
    sandbox_keywords = [
        "save_artifact",
        "load_artifact",
        "read_file",
        "write_file",
        "run_python_script",
        "execute_code",
        "save_to_file",
        "read_from_file",
    ]

    if not session_trace:
        return 0.0, "No trace data available for sandbox usage", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            # Check if tool matches sandbox keywords
            if any(keyword in tool_name.lower() for keyword in sandbox_keywords):
                sandbox_ops_count += 1
                sandbox_tools_used[tool_name] = sandbox_tools_used.get(tool_name, 0) + 1

    unique_ops_used = len(sandbox_tools_used)

    breakdown_str = ", ".join([f"{k}: {v}" for k, v in sandbox_tools_used.items()])

    explanation = (
        f"Total Sandbox Ops: {sandbox_ops_count}. "
        f"Unique Ops: {unique_ops_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_sandbox_ops": sandbox_ops_count,
        "unique_ops_used": unique_ops_used,
        "sandbox_tools_used": sandbox_tools_used,
    }

    return float(sandbox_ops_count), explanation, details


# Registry of all deterministic metrics
DETERMINISTIC_METRICS = {
    "token_usage": calculate_token_usage,
    "latency_metrics": calculate_latency_metrics,
    "cache_efficiency": calculate_cache_efficiency,
    "thinking_metrics": calculate_thinking_metrics,
    "tool_utilization": calculate_tool_utilization,
    "tool_success_rate": calculate_tool_success_rate,
    "grounding_utilization": calculate_grounding_utilization,
    "context_saturation": calculate_context_saturation,
    "agent_handoffs": calculate_agent_handoffs,
    "output_density": calculate_output_density,
    "sandbox_usage": calculate_sandbox_usage,
}


def evaluate_deterministic_metrics(
    session_state: Dict[str, Any],
    session_trace: List[Dict[str, Any]],
    agents_evaluated: List[str],
    question_metadata: Dict[str, Any],
    metrics_to_run: List[str] = None,
    reference_data: Dict[str, Any] = None,
    metric_definitions: Dict[str, Any] = None,
    latency_data: List[Dict[str, Any]] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate all specified deterministic metrics.
    """
    if metrics_to_run is None:
        metrics_to_run = list(DETERMINISTIC_METRICS.keys())

    results = {}
    for metric_name in metrics_to_run:
        if metric_name not in DETERMINISTIC_METRICS:
            continue

        metric_func = DETERMINISTIC_METRICS[metric_name]

        try:
            if metric_name == "latency_metrics":
                score, explanation, details = metric_func(
                    session_trace, latency_data=latency_data
                )
            else:
                score, explanation, details = metric_func(session_trace)

            results[metric_name] = {
                "score": score,
                "explanation": explanation,
                "details": details,
            }
        except Exception as e:
            results[metric_name] = {
                "score": 0.0,
                "explanation": f"Error evaluating metric {metric_name}: {str(e)}",
            }

    return results

```

**3. Agent Implementation Details:**
*   **Agent Source Code:** The source code for the agent being evaluated. This is your primary source for forming hypotheses about *why* the agent behaves a certain way.
**File: `../retail-ai-location-strategy/app/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Retail Location Strategy Agent - Root Agent Definition.

This module defines the root agent for the Location Strategy Pipeline.
It uses a SequentialAgent to orchestrate 6 specialized sub-agents:

1. MarketResearchAgent - Live web research with Google Search
2. CompetitorMappingAgent - Competitor mapping with Maps Places API
3. GapAnalysisAgent - Quantitative analysis with Python code execution
4. StrategyAdvisorAgent - Strategic synthesis with extended reasoning
5. ReportGeneratorAgent - HTML executive report generation
6. InfographicGeneratorAgent - Visual infographic generation

The pipeline analyzes a target location for a specific business type and
produces comprehensive location intelligence including recommendations,
an HTML report, and an infographic.

Authentication:
    Uses Google AI Studio (API key) instead of Vertex AI.
    Set environment variables:
        GOOGLE_API_KEY=your_api_key
        GOOGLE_GENAI_USE_VERTEXAI=FALSE
        MAPS_API_KEY=your_maps_api_key

Usage:
    Run with: adk web retail_ai_location_strategy_adk

    The agent expects initial state variables:
    - target_location: The geographic area to analyze (e.g., "Bangalore, India")
    - business_type: Type of business to open (e.g., "coffee shop")

    Optional state variables:
    - maps_api_key: Google Maps API key for Places search
"""

from google.adk.agents import SequentialAgent
from google.adk.agents.llm_agent import Agent
from google.adk.tools.agent_tool import AgentTool


from .sub_agents.intake_agent.agent import intake_agent
from .sub_agents.market_research.agent import market_research_agent
from .sub_agents.competitor_mapping.agent import competitor_mapping_agent
from .sub_agents.gap_analysis.agent import gap_analysis_agent
from .sub_agents.strategy_advisor.agent import strategy_advisor_agent
from .sub_agents.infographic_generator.agent import infographic_generator_agent
from .sub_agents.report_generator.agent import report_generator_agent

from .config import FAST_MODEL, APP_NAME

# location_strategy_pipeline
location_strategy_pipeline = SequentialAgent(
    name="LocationStrategyPipeline",
    description="""Comprehensive retail location strategy analysis pipeline.

This agent analyzes a target location for a specific business type and produces:
1. Market research findings from live web data
2. Competitor mapping from Google Maps Places API
3. Quantitative gap analysis with zone rankings
4. Strategic recommendations with structured JSON output
5. Professional HTML executive report
6. Visual infographic summary

To use, get the following details:
- target_location: {target_location}
- business_type: {business_type}

The analysis runs automatically through all stages and produces artifacts
including JSON report, HTML report, and infographic image.
""",
    sub_agents=[
        market_research_agent,      # Part 1: Market research with search
        competitor_mapping_agent,   # Part 2A: Competitor mapping with Maps
        gap_analysis_agent,         # Part 2B: Gap analysis with code exec
        strategy_advisor_agent,     # Part 3: Strategy synthesis
        report_generator_agent,     # Part 4: HTML report generation
        infographic_generator_agent,  # Part 5: Infographic generation
    ],
)

# Root agent orchestrating the complete location strategy pipeline
root_agent = Agent(
    model=FAST_MODEL,
    name=APP_NAME,
    description='A strategic partner for retail businesses, guiding them to optimal physical locations that foster growth and profitability.',
    instruction="""Your primary role is to orchestrate the retail location analysis.
1. Start by greeting the user.
2. Check if the `TARGET_LOCATION` (Geographic area to analyze (e.g., "Indiranagar, Bangalore")) and `BUSINESS_TYPE` (Type of business (e.g., "coffee shop", "bakery", "gym")) have been provided.
3. If they are missing, **ask the user clarifying questions to get the required information.**
4. Once you have the necessary details, call the `IntakeAgent` tool to process them.
5. After the `IntakeAgent` is successful, delegate the full analysis to the `LocationStrategyPipeline`.
Your main function is to manage this workflow conversationally.""",
    sub_agents=[location_strategy_pipeline],
    tools = [AgentTool(intake_agent)], # Part 0: Parse user request
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/gap_analysis/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Gap Analysis Agent - Part 2B of the Location Strategy Pipeline.

This agent performs quantitative gap analysis using Python code execution
to calculate saturation indices, viability scores, and zone rankings.
"""

from google.adk.agents import LlmAgent
from google.adk.code_executors import BuiltInCodeExecutor
from google.genai import types

from ...config import CODE_EXEC_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...callbacks import before_gap_analysis, after_gap_analysis


GAP_ANALYSIS_INSTRUCTION = """You are a data scientist analyzing market opportunities using quantitative methods.

Your task is to perform advanced gap analysis on the data collected from previous stages.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Available Data

### MARKET RESEARCH FINDINGS (Part 1):
{market_research_findings}

### COMPETITOR ANALYSIS (Part 2):
{competitor_analysis}

## Your Mission
Write and execute Python code to perform comprehensive quantitative analysis.

## Analysis Steps

### Step 1: Parse Competitor Data
Extract from the competitor analysis:
- Competitor names and locations
- Ratings and review counts
- Zone/area classifications
- Business types (chain vs independent)

### Step 2: Extract Market Fundamentals
From the market research:
- Population estimates
- Income levels (assign numeric scores)
- Infrastructure quality indicators
- Foot traffic patterns

### Step 3: Calculate Zone Metrics
For each identified zone, compute:

**Basic Metrics:**
- Competitor count
- Competitor density (per estimated area)
- Average competitor rating
- Total review volume

**Quality Metrics:**
- Competition Quality Score: Weighted by ratings (4.5+ = high threat)
- Chain Dominance Ratio: % of chain/franchise competitors
- High Performer Count: Number of 4.5+ rated competitors

**Opportunity Metrics:**
- Demand Signal: Based on population, income, infrastructure
- Market Saturation Index: (Competitors  Quality) / Demand
- Viability Score: Multi-factor weighted score

### Step 4: Zone Categorization
Classify each zone as:
- **SATURATED**: High competition, low opportunity
- **MODERATE**: Balanced market, moderate opportunity
- **OPPORTUNITY**: Low competition, high potential

Also assign:
- Risk Level: Low / Medium / High
- Investment Tier: Based on expected costs
- Best Customer Segment: Target demographic

### Step 5: Rank Top Zones
Create a weighted ranking considering:
- Low market saturation (weight: 30%)
- High demand signals (weight: 30%)
- Low chain dominance (weight: 15%)
- Infrastructure quality (weight: 15%)
- Manageable costs (weight: 10%)

### Step 6: Output Tables
Generate clear output tables showing:
1. All zones with computed metrics
2. Top 3 recommended zones with scores
3. Risk assessment matrix

## Code Guidelines
- Use pandas for data manipulation
- Print all results clearly formatted
- Include intermediate calculations for transparency
- Handle missing data gracefully

Execute the code and provide actionable strategic recommendations based on the quantitative findings.
"""

gap_analysis_agent = LlmAgent(
    name="GapAnalysisAgent",
    model=CODE_EXEC_MODEL,
    description="Performs quantitative gap analysis using Python code execution for zone rankings and viability scores",
    instruction=GAP_ANALYSIS_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    code_executor=BuiltInCodeExecutor(),
    output_key="gap_analysis",
    before_agent_callback=before_gap_analysis,
    after_agent_callback=after_gap_analysis,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/intake_agent/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Intake Agent - Extracts target location and business type from user request.

This agent parses the user's natural language request and extracts the
required parameters (target_location, business_type) into session state
for use by subsequent agents in the pipeline.
"""

from typing import Optional

from google.adk.agents import LlmAgent
from google.adk.agents.callback_context import CallbackContext
from google.genai import types
from pydantic import BaseModel, Field

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS


class UserRequest(BaseModel):
    """Structured output for parsing user's location strategy request."""

    target_location: str = Field(
        description="The geographic location/area to analyze (e.g., 'Indiranagar, Bangalore', 'Manhattan, New York')"
    )
    business_type: str = Field(
        description="The type of business the user wants to open (e.g., 'coffee shop', 'bakery', 'gym', 'restaurant')"
    )
    additional_context: Optional[str] = Field(
        default=None,
        description="Any additional context or requirements mentioned by the user"
    )


def after_intake(callback_context: CallbackContext) -> Optional[types.Content]:
    """After intake, copy the parsed values to state for other agents."""
    parsed = callback_context.state.get("parsed_request", {})

    if isinstance(parsed, dict):
        # Extract values from parsed request
        callback_context.state["target_location"] = parsed.get("target_location", "")
        callback_context.state["business_type"] = parsed.get("business_type", "")
        callback_context.state["additional_context"] = parsed.get("additional_context", "")
    elif hasattr(parsed, "target_location"):
        # Handle Pydantic model
        callback_context.state["target_location"] = parsed.target_location
        callback_context.state["business_type"] = parsed.business_type
        callback_context.state["additional_context"] = parsed.additional_context or ""

    # Track intake stage completion
    stages = callback_context.state.get("stages_completed", [])
    stages.append("intake")
    callback_context.state["stages_completed"] = stages

    # Note: current_date is set in each agent's before_callback to ensure it's always available
    return None


INTAKE_INSTRUCTION = """You are a request parser for a retail location intelligence system.

Your task is to extract the target location and business type from the user's request.

## Examples

User: "I want to open a coffee shop in Indiranagar, Bangalore"
 target_location: "Indiranagar, Bangalore"
 business_type: "coffee shop"

User: "Analyze the market for a new gym in downtown Seattle"
 target_location: "downtown Seattle"
 business_type: "gym"

User: "Help me find the best location for a bakery in Mumbai"
 target_location: "Mumbai"
 business_type: "bakery"

User: "Where should I open my restaurant in San Francisco's Mission District?"
 target_location: "Mission District, San Francisco"
 business_type: "restaurant"

## Instructions
1. Extract the geographic location mentioned by the user
2. Identify the type of business they want to open
3. Note any additional context or requirements

If the user doesn't specify a clear location or business type, make a reasonable inference or ask for clarification.
"""

intake_agent = LlmAgent(
    name="IntakeAgent",
    model=FAST_MODEL,
    description="Parses user request to extract target location and business type",
    instruction=INTAKE_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    output_schema=UserRequest,
    output_key="parsed_request",
    after_agent_callback=after_intake,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/infographic_generator/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Infographic Generator Agent - Part 5 (Bonus) of the Location Strategy Pipeline.

This agent creates a visual infographic summary using Gemini's image generation
capabilities to provide an executive-ready visual summary of the analysis.
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import generate_infographic
from ...callbacks import before_infographic_generator, after_infographic_generator


INFOGRAPHIC_GENERATOR_INSTRUCTION = """You are a data visualization specialist creating executive-ready infographics.

Your task is to generate a visual infographic summarizing the location intelligence analysis.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Strategic Report Data
{strategic_report}

## Your Mission
Create a compelling infographic that visually summarizes the key findings from the analysis.

## Steps

### Step 1: Extract Key Data Points
From the strategic report, identify:
- Target location and business type
- Top recommended location with score
- Total competitors found
- Number of zones analyzed
- 3-5 key strategic insights
- Top strengths and concerns
- Market validation verdict

### Step 2: Create Data Summary
Compose a concise data summary suitable for visualization:

**FORMAT YOUR SUMMARY AS:**

LOCATION INTELLIGENCE REPORT: [Business Type] in [Target Location]
Analysis Date: [Date]

TOP RECOMMENDATION:
[Location Name] - Score: [XX]/100
Type: [Opportunity Type]

KEY METRICS:
- Total Competitors: [X]
- Zones Analyzed: [X]
- Market Status: [Validated/Cautionary]

TOP STRENGTHS:
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

KEY INSIGHTS:
- [Insight 1]
- [Insight 2]
- [Insight 3]

VERDICT: [One-line market recommendation]

### Step 3: Generate Infographic
Call the generate_infographic tool with your data summary.

### Step 4: Report Result
After the tool returns, store the result for the callback to process.
If successful, confirm the infographic was generated.
If failed, report the error for troubleshooting.

## Output
The generate_infographic tool will return a result dict containing:
- status: "success" or "error"
- image_data: Base64 encoded PNG (if successful)
- error_message: Error details (if failed)

Store this result so the after_agent_callback can save the artifact.
"""

infographic_generator_agent = LlmAgent(
    name="InfographicGeneratorAgent",
    model=FAST_MODEL,
    description="Generates visual infographic summary using Gemini image generation",
    instruction=INFOGRAPHIC_GENERATOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[generate_infographic],
    output_key="infographic_result",
    before_agent_callback=before_infographic_generator,
    after_agent_callback=after_infographic_generator,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/market_research/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Market Research Agent - Part 1 of the Location Strategy Pipeline.

This agent validates macro market viability using live web data from Google Search.
It researches demographics, market trends, and commercial viability.
"""

from google.adk.agents import LlmAgent
from google.adk.tools import google_search
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...callbacks import before_market_research, after_market_research


MARKET_RESEARCH_INSTRUCTION = """You are a market research analyst specializing in retail location intelligence.

Your task is to research and validate the target market for a new business location.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Research Focus Areas

### 1. DEMOGRAPHICS
- Age distribution (identify key age groups)
- Income levels and purchasing power
- Lifestyle indicators (professionals, students, families)
- Population density and growth trends

### 2. MARKET GROWTH
- Population trends (growing, stable, declining)
- New residential and commercial developments
- Infrastructure improvements (metro, roads, tech parks)
- Economic growth indicators

### 3. INDUSTRY PRESENCE
- Existing similar businesses in the area
- Consumer preferences and spending patterns
- Market saturation indicators
- Success stories or failures of similar businesses

### 4. COMMERCIAL VIABILITY
- Foot traffic patterns (weekday vs weekend)
- Commercial real estate trends
- Typical rental costs (qualitative: low/medium/high)
- Business environment and regulations

## Instructions
1. Use Google Search to find current, verifiable data
2. Cite specific data points with sources where possible
3. Focus on information from the last 1-2 years for relevance
4. Be factual and data-driven, avoid speculation

## Output Format
Provide a structured analysis covering all four focus areas.
Conclude with a clear verdict: Is this a strong market for {business_type}? Why or why not?
Include specific recommendations for market entry strategy.
"""

market_research_agent = LlmAgent(
    name="MarketResearchAgent",
    model=FAST_MODEL,
    description="Researches market viability using Google Search for real-time demographics, trends, and commercial data",
    instruction=MARKET_RESEARCH_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[google_search],
    output_key="market_research_findings",
    before_agent_callback=before_market_research,
    after_agent_callback=after_market_research,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/strategy_advisor/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Strategy Advisor Agent - Part 3 of the Location Strategy Pipeline.

This agent synthesizes all findings into actionable recommendations using
extended reasoning (thinking mode) and outputs a structured JSON report.
"""

from google.adk.agents import LlmAgent
from google.adk.planners import BuiltInPlanner
from google.genai import types
from google.genai.types import ThinkingConfig

from ...config import PRO_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...schemas import LocationIntelligenceReport
from ...callbacks import before_strategy_advisor, after_strategy_advisor


STRATEGY_ADVISOR_INSTRUCTION = """You are a senior strategy consultant synthesizing location intelligence findings.

Your task is to analyze all research and provide actionable strategic recommendations.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Available Data

### MARKET RESEARCH FINDINGS (Part 1):
{market_research_findings}

### COMPETITOR ANALYSIS (Part 2A):
{competitor_analysis}

### GAP ANALYSIS (Part 2B):
{gap_analysis}

## Your Mission
Synthesize all findings into a comprehensive strategic recommendation.

## Analysis Framework

### 1. Data Integration
Review all inputs carefully:
- Market research demographics and trends
- Competitor locations, ratings, and patterns
- Quantitative gap analysis metrics and zone rankings

### 2. Strategic Synthesis
For each promising zone, evaluate:
- Opportunity Type: Categorize (e.g., "Metro First-Mover", "Residential Sticky", "Mall Impulse")
- Overall Score: 0-100 weighted composite
- Strengths: Top 3-4 factors with evidence from the analysis
- Concerns: Top 2-3 risks with specific mitigation strategies
- Competition Profile: Summarize density, quality, chain presence
- Market Characteristics: Population, income, infrastructure, foot traffic, costs
- Best Customer Segment: Primary target demographic
- Next Steps: 3-5 specific actionable recommendations

### 3. Top Recommendation Selection
Choose the single best location based on:
- Highest weighted opportunity score
- Best balance of opportunity vs risk
- Most aligned with business type requirements
- Clear competitive advantage potential

### 4. Alternative Locations
Identify 2-3 alternative locations:
- Brief scoring and categorization
- Key strength and concern for each
- Why it's not the top choice

### 5. Strategic Insights
Provide 4-6 key insights that span the entire analysis:
- Market-level observations
- Competitive dynamics
- Timing considerations
- Success factors

## Output Requirements
Your response MUST conform to the LocationIntelligenceReport schema.
Ensure all fields are populated with specific, actionable information.
Use evidence from the analysis to support all recommendations.
"""

strategy_advisor_agent = LlmAgent(
    name="StrategyAdvisorAgent",
    model=PRO_MODEL,
    description="Synthesizes findings into strategic recommendations using extended reasoning and structured output",
    instruction=STRATEGY_ADVISOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    planner=BuiltInPlanner(
        thinking_config=ThinkingConfig(
            include_thoughts=False,  # Must be False when using output_schema
            thinking_budget=-1,  # -1 means unlimited thinking budget
        )
    ),
    output_schema=LocationIntelligenceReport,
    output_key="strategic_report",
    before_agent_callback=before_strategy_advisor,
    after_agent_callback=after_strategy_advisor,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/competitor_mapping/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Competitor Mapping Agent - Part 2A of the Location Strategy Pipeline.

This agent maps competitors using the Google Maps Places API to get
ground-truth data about existing businesses in the target area.
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import search_places
from ...callbacks import before_competitor_mapping, after_competitor_mapping


COMPETITOR_MAPPING_INSTRUCTION = """You are a market intelligence analyst specializing in competitive landscape analysis.

Your task is to map and analyze all competitors in the target area using real Google Maps data.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Your Mission
Use the search_places function to get REAL data from Google Maps about existing competitors.

## Step 1: Search for Competitors
Call the search_places function with queries like:
- "{business_type} near {target_location}"
- Related business types in the same area

## Step 2: Analyze the Results
For each competitor found, note:
- Business name
- Location/address
- Rating (out of 5)
- Number of reviews
- Business status (operational, etc.)

## Step 3: Identify Patterns
Analyze the competitive landscape:

### Geographic Clustering
- Are competitors clustered in specific areas/zones?
- Which areas have high concentration vs sparse presence?
- Are there any "dead zones" with no competitors?

### Location Types
- Shopping malls and retail areas
- Main roads and commercial corridors
- Residential neighborhoods
- Near transit (metro stations, bus stops)

### Quality Segmentation
- Premium tier: High-rated (4.5+), likely higher prices
- Mid-market: Ratings 4.0-4.4
- Budget tier: Lower ratings or basic offerings
- Chain vs independent businesses

## Step 4: Strategic Assessment
Provide insights on:
- Which areas appear saturated with competitors?
- Which areas might be underserved opportunities?
- What quality gaps exist (e.g., no premium options)?
- Where are the strongest competitors located?

## Output Format
Provide a detailed competitor map with:
1. List of all competitors found with their details
2. Zone-by-zone breakdown of competition
3. Pattern analysis and clustering insights
4. Strategic opportunities and saturation warnings

Be specific and reference the actual data you receive from the search_places tool.
"""

competitor_mapping_agent = LlmAgent(
    name="CompetitorMappingAgent",
    model=FAST_MODEL,
    description="Maps competitors using Google Maps Places API for ground-truth competitor data",
    instruction=COMPETITOR_MAPPING_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[search_places],
    output_key="competitor_analysis",
    before_agent_callback=before_competitor_mapping,
    after_agent_callback=after_competitor_mapping,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/report_generator/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Report Generator Agent - Part 4 of the Location Strategy Pipeline.

This agent generates a professional HTML executive report from the
structured LocationIntelligenceReport data using the generate_html_report tool.

The tool handles:
- Calling Gemini to generate 7-slide McKinsey/BCG style HTML
- Saving the HTML as an artifact for download in adk web
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import generate_html_report
from ...callbacks import before_report_generator, after_report_generator


REPORT_GENERATOR_INSTRUCTION = """You are an executive report generator for location intelligence analysis.

Your task is to create a professional HTML executive report using the generate_html_report tool.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Strategic Report Data
{strategic_report}

## Your Mission
Format the strategic report data and call the generate_html_report tool to create a
McKinsey/BCG-style 7-slide HTML presentation.

## Steps

### Step 1: Format the Report Data
Prepare a comprehensive data summary from the strategic report above, including:
- Analysis overview (location, business type, date, market validation)
- Top recommendation details (location, score, opportunity type, strengths, concerns)
- Competition metrics (total competitors, density, chain dominance, ratings)
- Market characteristics (population, income, infrastructure, foot traffic, rental costs)
- Alternative locations (name, score, strength, concern, why not top)
- Next steps (actionable items)
- Key insights (strategic observations)
- Methodology summary

### Step 2: Call the Tool
Call the generate_html_report tool with the formatted report data.
The tool will:
- Generate a professional 7-slide HTML report
- Save it as an artifact named "executive_report.html"
- Return the status and artifact details

### Step 3: Report Result
After the tool returns, confirm the report was generated successfully.
If there was an error, report what went wrong.
"""

report_generator_agent = LlmAgent(
    name="ReportGeneratorAgent",
    model=FAST_MODEL,
    description="Generates professional McKinsey/BCG-style HTML executive reports using the generate_html_report tool",
    instruction=REPORT_GENERATOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[generate_html_report],
    output_key="report_generation_result",
    before_agent_callback=before_report_generator,
    after_agent_callback=after_report_generator,
)

```

**4. Evaluation Questions:**
*   **Questions Evaluated:** The full set of questions used in the evaluation. This can provide context if certain types of questions are causing specific failures.
**Questions Evaluated**
```json
Questions file not found.
```

---
Format your entire response as a single Markdown document.
