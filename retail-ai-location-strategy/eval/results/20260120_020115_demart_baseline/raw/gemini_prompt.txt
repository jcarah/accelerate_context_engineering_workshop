
You are an expert AI evaluation analyst. Your task is to produce a deep technical diagnosis of an AI agent's performance for technical stakeholders.

**Tone:** objective, analytical, and professional
**Report Length:** comprehensive

**STRATEGIC FRAMEWORK:**
Please adhere to the following framework when analyzing the agent:

# Agent Optimization Strategy & Signal Identification Plan

## 1. Overview & Workshop Objective
This framework guides the technical diagnosis of AI agents during the "hill climbing" optimization exercise. The objective is to transition from simple "Prompt Engineering" to "Architectural Context Engineering" by identifying specific performance signals and mapping them to optimized architectural patterns.

## 2. Core Optimization Principles (The Hill Climbing Framework)

### A. Offload (Move Logic to Tools)
*   **The Signal:** High reasoning errors, hallucinations in deterministic tasks (arithmetic, data aggregation), or long "thinking" traces that result in incorrect logic.
*   **Logic vs. Knowledge:** Use **Offload** when the agent has the correct facts but fails to process them effectively (e.g., failing to sort a list, calculate a sum, or filter a JSON blob). These are **Reasoning** errors that should be moved to a `FunctionTool` or Python Sandbox via Code Execution.

### B. Reduce (Trim the Context Window)
*   **The Signal:** High latency, high prompt costs, and "Lost in the Middle" syndrome where the agent ignores instructions. High `prompt_token_count` relative to task complexity.
*   **The Fix:** Implement `EventsCompactionConfig` to summarize history, or use `include_contents='none'` for stateless utility agents to minimize the active context window.

### C. Retrieve (Dynamic Grounding)
*   **The Signal:** Hallucinations regarding facts, or poor `HALLUCINATION` scores despite high token usage. The prompt is "stuffed" with static data that isn't always relevant.
*   **Logic vs. Knowledge:** Use **Retrieve** when the agent fails due to missing or stale information (e.g., not knowing a specific customer's order history or a specific retail competitor's location). These are **Knowledge** errors that should be fixed by replacing static data blocks with a Retrieval-Augmented Generation (RAG) pattern.

### D. Isolate (Modular Decomposition)
*   **The Signal:** "Monolithic Confusion"â€”the agent has too many instructions or tools and frequently chooses the wrong one or gets stuck in loops.
*   **The Fix:** Break the monolith into a `SequentialAgent` pipeline or a `Coordinator` pattern with specialized sub-agents, isolating specific tasks into distinct execution contexts.

### E. Cache (Efficiency & Latency)
*   **The Signal:** Low `cache_hit_rate` (KV-Cache efficiency < 50%) despite repetitive multi-turn interactions or large static system instructions. High Time-To-First-Token (TTFT).
*   **The Fix:** Restructure the prompt to keep static content at the beginning and implement `ContextCacheConfig` in the ADK `App` wrapper.

## 3. Performance Dimensions
*   **Quality:** Measured via LLM-as-a-Judge (Rubric-based metrics like `TOOL_USE_QUALITY` and `GENERAL_QUALITY`).
*   **Cost:** Measured via total `token_usage` and estimated USD cost.
*   **Latency:** Measured via `latency_metrics` (TTFT, Total Turn Latency, and Average Turn Latency).

## 4. Analysis Goal
Your diagnosis should identify which specific principle (Offload, Reduce, Retrieve, Isolate, or Cache) will provide the highest ROI for the next step in the agent's optimization journey.

## 5. Justification with Evidence
Every diagnosis (positive or negative) MUST be supported by a combination of quantitative scores and qualitative interaction examples.
*   **Reference Metric Scores:** Cite specific scores from the `Evaluation Summary` or `Detailed Explanations` (e.g., "The agent scored 1.2 on `tool_usage_accuracy`...").
*   **Identify the Question ID:** Reference specific test cases (e.g., `q_billing_01`).
*   **Quote the Interaction:** Provide snippets of user input, tool calls (arguments/responses), or agent text that directly justify the score and the diagnosis.
*   **Connect Signal to Fix:** Explain how the combination of the score and the quoted evidence points directly to one of the optimization principles (e.g., "The low `tool_usage_accuracy` score combined with the agent's failure to handle the math in question `q_retail_05` after seeing 50 rows of JSON indicates it is a prime candidate for **Offloading** to Code Execution").


---

**CRITICAL INSTRUCTIONS:**
1.  **Focus on Diagnosis, Not Recommendations:** Your primary goal is to explain *why* the metrics are what they are. Do not provide a future-looking action plan unless the Strategic Framework above explicitly guides it.
2.  **Synthesize, Don't Summarize:** Do not simply repeat the scores. Your value is in synthesizing insights by connecting the metric scores, the metric definitions, the source code, and the raw explanations.
3.  **Reference Your Sources:** When you make a claim or analyze a metric, you MUST reference the specific source file (e.g., `metric_definitions.json`, `deterministic_metrics.py`, `agent.py`).
4.  **Analyze Calculation Methods:** For each metric you discuss, you MUST explain how its calculation method (deterministic vs. LLM-judged) influences its interpretation.
5.  **Cite Quantitative and Qualitative Evidence:** You MUST quote specific examples from the conversation logs (user inputs, tool calls, or agent responses) AND cite the corresponding metric scores to justify your findings. Don't just say "the agent failed"; say "The agent's score of 1.2 on `tool_usage_accuracy` is justified by question `q_billing_01`, where it failed to call `lookup_invoice` despite the user explicitly asking for it."

---

**Context for Your Analysis**

You are provided with the following context files to perform your diagnosis. Use them to connect the agent's behavior (the metrics) to its underlying implementation (the code).

**1. Overall Performance Data:**
*   **Evaluation Summary:** High-level average scores for all metrics. Use this to identify the most significant areas of success and failure.
**Evaluation Summary**
```json
{
  "experiment_id": "eval-20260120_020245",
  "run_type": "manual",
  "test_description": "Automated evaluation",
  "interaction_datetime": "2026-01-20T02:02:45.256502",
  "overall_summary": {
    "deterministic_metrics": {
      "token_usage.llm_calls": 11.0,
      "token_usage.total_tokens": 0.0,
      "token_usage.prompt_tokens": 135448.0,
      "token_usage.completion_tokens": 16156.0,
      "token_usage.cached_tokens": 0.0,
      "token_usage.estimated_cost_usd": 0.0,
      "latency_metrics.total_latency_seconds": 325.4837,
      "latency_metrics.average_turn_latency_seconds": 325.4837,
      "latency_metrics.llm_latency_seconds": 19.0,
      "latency_metrics.tool_latency_seconds": 10.0,
      "latency_metrics.time_to_first_response_seconds": 13.811457024,
      "cache_efficiency.cache_hit_rate": 0.0,
      "cache_efficiency.total_cached_tokens": 0.0,
      "cache_efficiency.total_fresh_prompt_tokens": 135448.0,
      "cache_efficiency.total_input_tokens": 135448.0,
      "thinking_metrics.reasoning_ratio": 0.37177789891752977,
      "thinking_metrics.total_thinking_tokens": 9548.0,
      "thinking_metrics.total_candidate_tokens": 16134.0,
      "thinking_metrics.total_output_tokens": 25682.0,
      "thinking_metrics.turns_with_thinking": 10.0,
      "tool_utilization.total_tool_calls": 10.0,
      "tool_utilization.unique_tools_used": 5.0,
      "tool_success_rate.tool_success_rate": 1.0,
      "tool_success_rate.total_tool_calls": 5.0,
      "tool_success_rate.failed_tool_calls": 0.0,
      "grounding_utilization.total_grounding_chunks": 0.0,
      "grounding_utilization.total_grounded_responses": 0.0,
      "grounding_utilization.total_llm_responses": 11.0,
      "context_saturation.max_total_tokens": 63350.0,
      "agent_handoffs.total_handoffs": 5.0,
      "agent_handoffs.unique_agents_count": 3.0,
      "output_density.average_output_tokens": 1468.7272727272727,
      "output_density.total_output_tokens": 16156.0,
      "output_density.llm_calls_count": 11.0,
      "sandbox_usage.total_sandbox_ops": 0.0,
      "sandbox_usage.unique_ops_used": 0.0
    },
    "llm_based_metrics": {
      "hallucinations_v1": {
        "average": 1.0
      },
      "safety_v1": {
        "average": 1.0
      },
      "rubric_based_tool_use_quality_v1": {
        "average": 0.5
      },
      "rubric_based_final_response_quality_v1": {
        "average": 1.0
      },
      "general_quality": {
        "average": 0.083333336,
        "score_range": {
          "min": 0,
          "max": 1,
          "description": "Passing rate for general quality rubrics"
        }
      },
      "tool_use_quality": {
        "average": 5.0,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Poor tool usage, 5=Excellent tool usage"
        }
      },
      "trajectory_accuracy": {
        "average": 4.0,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Completely wrong, 5=Perfect trajectory"
        }
      },
      "text_quality": {
        "average": 0.2,
        "score_range": {
          "min": 0,
          "max": 1,
          "description": "Passing rate for text quality rubrics"
        }
      },
      "pipeline_integrity": {
        "average": 2.0,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Major integrity issues, 5=Perfect integrity"
        }
      }
    }
  },
  "per_question_summary": [
    {
      "question_id": "seattle_coffee_001",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 11,
          "models_used": [
            "gemini-2.5-pro"
          ],
          "total_tokens": 0,
          "prompt_tokens": 135448,
          "completion_tokens": 16156,
          "cached_tokens": 0,
          "estimated_cost_usd": 0.0
        },
        "latency_metrics": {
          "total_latency_seconds": 325.4837,
          "average_turn_latency_seconds": 325.4837,
          "llm_latency_seconds": 19.0,
          "tool_latency_seconds": 10.0,
          "time_to_first_response_seconds": 13.811457024
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.0,
          "total_cached_tokens": 0,
          "total_fresh_prompt_tokens": 135448,
          "total_input_tokens": 135448
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.37177789891752977,
          "total_thinking_tokens": 9548,
          "total_candidate_tokens": 16134,
          "total_output_tokens": 25682,
          "turns_with_thinking": 10
        },
        "tool_utilization": {
          "total_tool_calls": 10,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 2,
            "transfer_to_agent": 2,
            "search_places": 2,
            "generate_html_report": 2,
            "generate_infographic": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 5,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 11
        },
        "context_saturation": {
          "max_total_tokens": 63350,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 5,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "app",
            "transfer_to_agent",
            "IntakeAgent"
          ]
        },
        "output_density": {
          "average_output_tokens": 1468.7272727272727,
          "total_output_tokens": 16156,
          "llm_calls_count": 11
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "safety_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.5,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "general_quality": {
          "score": 0.083333336,
          "rubric_verdicts": [
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The response is in English."
                  }
                },
                "type": "LANGUAGE:PRIMARY_RESPONSE_LANGUAGE",
                "importance": "HIGH"
              },
              "verdict": true,
              "reasoning": "The entire response is written in the English language."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The response provides an analysis of location viability."
                  }
                },
                "type": "ANALYSIS:LOCATION_VIABILITY",
                "importance": "HIGH"
              },
              "reasoning": "The response only indicates that an analysis was generated as an artifact, but it does not present the analysis within the response itself."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis is specifically tailored to a coffee shop business."
                  }
                },
                "type": "CONTENT_CONSTRAINT:BUSINESS_TYPE",
                "importance": "HIGH"
              },
              "reasoning": "The response does not provide any analysis, nor does it specify that the mentioned \"location intelligence analysis\" is tailored to a coffee shop business."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis is specifically focused on Seattle, Washington."
                  }
                },
                "type": "CONTENT_CONSTRAINT:GEOGRAPHIC_AREA",
                "importance": "HIGH"
              },
              "reasoning": "The response does not provide any analysis, nor does it specify that the mentioned \"location intelligence analysis\" is focused on Seattle, Washington."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis considers relevant demographic factors for a coffee shop in Seattle (e.g., target customer base, population density, income levels)."
                  }
                },
                "type": "ANALYSIS_CRITERION:DEMOGRAPHICS",
                "importance": "HIGH"
              },
              "reasoning": "The response does not provide any analysis, so it cannot consider demographic factors."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis examines the competitive landscape for coffee shops in Seattle (e.g., number of existing shops, major chains vs. independents)."
                  }
                },
                "type": "ANALYSIS_CRITERION:COMPETITION",
                "importance": "HIGH"
              },
              "reasoning": "The response does not provide any analysis, so it cannot examine the competitive landscape."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis discusses factors related to physical location and accessibility (e.g., foot traffic, visibility, public transport access, parking)."
                  }
                },
                "type": "ANALYSIS_CRITERION:ACCESSIBILITY_AND_VISIBILITY",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, so it cannot discuss factors related to physical location and accessibility."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis addresses economic considerations such as commercial rental costs or real estate availability in Seattle."
                  }
                },
                "type": "ANALYSIS_CRITERION:COSTS_AND_REAL_ESTATE",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, so it cannot address economic considerations."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis mentions any relevant local regulations, permits, or licensing requirements for opening a coffee shop in Seattle."
                  }
                },
                "type": "ANALYSIS_CRITERION:REGULATORY_ENVIRONMENT",
                "importance": "LOW"
              },
              "reasoning": "The response does not provide any analysis, so it cannot mention local regulations, permits, or licensing requirements."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis considers Seattle's local culture and consumer preferences regarding coffee."
                  }
                },
                "type": "ANALYSIS_CRITERION:CULTURAL_FIT",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, so it cannot consider Seattle's local culture and consumer preferences."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis discusses potential opportunities or challenges specific to operating a coffee shop in Seattle."
                  }
                },
                "type": "ANALYSIS_CRITERION:OPPORTUNITIES_AND_CHALLENGES",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, so it cannot discuss potential opportunities or challenges."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The response offers a conclusion or recommendation regarding the viability of opening a coffee shop in Seattle."
                  }
                },
                "type": "ANALYSIS_CONCLUSION:VIABILITY_STATEMENT",
                "importance": "HIGH"
              },
              "reasoning": "The response does not provide any analysis, and therefore does not offer a conclusion or recommendation."
            }
          ],
          "input": {
            "request": {
              "contents": [
                {
                  "role": "user",
                  "parts": [
                    {
                      "text": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability."
                    }
                  ]
                },
                {
                  "role": "model",
                  "parts": [
                    {
                      "text": "Hello! I can help with that. First, I need to process your request.\n"
                    }
                  ]
                }
              ]
            },
            "response": {
              "candidates": [
                {
                  "content": {
                    "role": "model",
                    "parts": [
                      {
                        "text": "The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`."
                      }
                    ]
                  }
                }
              ]
            }
          }
        },
        "tool_use_quality": {
          "score": 5.0,
          "explanation": "The agent used all relevant available tools (`IntakeAgent`, `search_places`, `generate_html_report`, `generate_infographic`) correctly to address the user request. The `IntakeAgent` parsed the request accurately. `search_places` was used to gather competitor data, which is crucial for location analysis. The `generate_html_report` and `generate_infographic` tools were used to present a comprehensive analysis and summary, respectively. The content for these reports, assumed to be mock data as per the prompt's instructions, was properly processed by the agent. The `transfer_to_agent` call is slightly ambiguous but did not impede the successful execution and completion of the core request.",
          "input": {
            "prompt": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability.",
            "response": "The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"I want to open a coffee shop in Seattle, WA. Please analyze the location viability.\"}, \"call_id\": \"adk-e260b8c9-ff84-4e2e-b0ec-ce1b7563a44b\", \"output_result\": {\"targetLocation\": \"Seattle, WA\", \"businessType\": \"coffee shop\", \"additionalContext\": \"Analyze the location viability.\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-a4f164ea-0c12-42b4-b71f-8410a5a97009\", \"outpu... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        },
        "trajectory_accuracy": {
          "score": 4.0,
          "explanation": "The agent followed a logical path, starting with intake, then using `search_places` for data collection, and finally generating both an HTML report and an infographic. While key stages like market research and gap analysis from the expected pipeline were not performed, this is due to the unavailability of `google_search` and `code execution` tools, which is not a flaw in the agent's execution but rather a limitation of its toolset. The agent gracefully handled this limitation by using the tools it had to deliver a relevant output.",
          "input": {
            "prompt": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability.",
            "response": "agent:app\nsub-agent:IntakeAgent\nsub-agent:transfer_to_agent\ntool:search_places\ntool:generate_html_report\ntool:generate_infographic",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}",
            "final_response": "The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`."
          }
        },
        "text_quality": {
          "score": 0.2,
          "rubric_verdicts": [
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response provide accurate, factual information specific to Seattle, WA, relevant to coffee shop location viability?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response provide accurate, factual information specific to Seattle, WA, relevant to coffee shop location viability?\nSTEP 1: Does the response provide accurate, factual information specific to Seattle, WA, relevant to coffee shop location viability?\nSTEP 2:\n1. Read the response.\n2. Identify if any information specific to Seattle, WA is present.\n3. Determine if this information is factual and relevant to coffee shop location viability.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" This response does not contain any specific information about Seattle, WA, nor does it provide any factual information relevant to coffee shop location viability. It only indicates that an infographic was generated.\nSTEP 4: The response does not provide any specific, factual information about Seattle, WA, or coffee shop location viability.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the analysis of location viability logically sound and based on valid business principles?"
                  }
                },
                "type": ""
              },
              "reasoning": "Is the analysis of location viability logically sound and based on valid business principles?\nSTEP 1: Is the analysis of location viability logically sound and based on valid business principles?\nSTEP 2:\n1. Read the response.\n2. Identify if any analysis of location viability is present.\n3. If analysis is present, evaluate its logical soundness and adherence to valid business principles.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response does not contain any analysis of location viability. It only mentions that an infographic summarizing such an analysis was generated. Therefore, there is no analysis to evaluate for logical soundness or business principles.\nSTEP 4: The response does not provide any analysis of location viability.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the response well-organized and easy to read, presenting the analysis in a coherent manner?"
                  }
                },
                "type": ""
              },
              "reasoning": "Is the response well-organized and easy to read, presenting the analysis in a coherent manner?\nSTEP 1: Is the response well-organized and easy to read, presenting the analysis in a coherent manner?\nSTEP 2:\n1. Read the response.\n2. Determine if there is any analysis presented.\n3. If analysis is present, evaluate its organization, readability, and coherence.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response itself is a single sentence and is easy to read. However, it does not present any analysis. It merely states that an infographic containing analysis was generated. Therefore, it cannot be evaluated for presenting analysis in a coherent manner.\nSTEP 4: While the response is easy to read, it does not present any analysis, so it cannot be judged on how well it organizes or coherently presents analysis.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response analyze key factors influencing coffee shop location viability (e.g., demographics, competition, regulations, real estate)?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response analyze key factors influencing coffee shop location viability (e.g., demographics, competition, regulations, real estate)?\nSTEP 1: Does the response analyze key factors influencing coffee shop location viability (e.g., demographics, competition, regulations, real estate)?\nSTEP 2:\n1. Read the response.\n2. Identify if any analysis of key factors influencing coffee shop location viability is present.\n3. Check for specific examples like demographics, competition, regulations, or real estate.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response does not contain any analysis of key factors influencing coffee shop location viability. It only mentions that an infographic summarizing such an analysis was generated.\nSTEP 4: The response does not analyze any key factors influencing coffee shop location viability.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response provide sufficient depth and detail for each analyzed factor to support an informed understanding of location viability?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response provide sufficient depth and detail for each analyzed factor to support an informed understanding of location viability?\nSTEP 1: Does the response provide sufficient depth and detail for each analyzed factor to support an informed understanding of location viability?\nSTEP 2:\n1. Read the response.\n2. Determine if any factors are analyzed.\n3. If factors are analyzed, assess the depth and detail provided for each.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response does not analyze any factors. Therefore, it cannot provide depth and detail for them.\nSTEP 4: The response does not analyze any factors, so it cannot provide depth and detail for them.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is all content in the response directly relevant to the viability of a coffee shop location in Seattle, WA?"
                  }
                },
                "type": ""
              },
              "reasoning": "Is all content in the response directly relevant to the viability of a coffee shop location in Seattle, WA?\nSTEP 1: Is all content in the response directly relevant to the viability of a coffee shop location in Seattle, WA?\nSTEP 2:\n1. Read the response.\n2. Identify the content of the response.\n3. Determine if this content directly addresses the viability of a coffee shop location in Seattle, WA.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The content is about the generation of an infographic. While the infographic *might* be relevant, the response itself (the statement about the infographic's generation) does not directly discuss or analyze the viability of a coffee shop location in Seattle, WA. It's a meta-response about an action taken, not an analysis itself.\nSTEP 4: The response describes an action (generating an infographic) rather than providing content directly relevant to the viability of a coffee shop location in Seattle, WA.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the response concise and free from irrelevant or verbose information?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Is the response concise and free from irrelevant or verbose information?\nSTEP 1: Is the response concise and free from irrelevant or verbose information?\nSTEP 2:\n1. Read the response.\n2. Evaluate if the response is brief and to the point.\n3. Determine if there is any information that is unnecessary or overly wordy given the user's prompt.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response is very concise (17 words, 1 sentence). It does not contain verbose information. However, given the user's prompt was to \"analyze the location viability,\" the response about generating an infographic is irrelevant to *providing* that analysis directly in the text. While concise, its content is not what the user asked for. But the question asks if it's concise and free from *irrelevant or verbose information*. It is concise and free from verbose information. The information itself is not the analysis, but the statement about the infographic is not verbose.\nSTEP 4: The response is short and to the point. It does not contain any unnecessary words or overly detailed descriptions.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response specifically focus the analysis on a 'coffee shop'?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response specifically focus the analysis on a 'coffee shop'?\nSTEP 1: Does the response specifically focus the analysis on a 'coffee shop'?\nSTEP 2:\n1. Read the response.\n2. Identify if any analysis is present.\n3. If analysis is present, check if it is specifically focused on a 'coffee shop'.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response does not contain any analysis. It mentions \"location intelligence analysis\" generally, but does not specify if this analysis is focused on a 'coffee shop' within the response itself.\nSTEP 4: The response does not provide any analysis, and therefore cannot focus it on a 'coffee shop'.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response specifically focus the analysis on 'Seattle, WA'?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response specifically focus the analysis on 'Seattle, WA'?\nSTEP 1: Does the response specifically focus the analysis on 'Seattle, WA'?\nSTEP 2:\n1. Read the response.\n2. Identify if any analysis is present.\n3. If analysis is present, check if it is specifically focused on 'Seattle, WA'.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" The response does not contain any analysis. It mentions \"location intelligence analysis\" generally, but does not specify if this analysis is focused on 'Seattle, WA' within the response itself.\nSTEP 4: The response does not provide any analysis, and therefore cannot focus it on 'Seattle, WA'.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the response free of harmful, unethical, or inappropriate content?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Is the response free of harmful, unethical, or inappropriate content?\nSTEP 1: Is the response free of harmful, unethical, or inappropriate content?\nSTEP 2:\n1. Read the response.\n2. Check for any language or implications that could be considered harmful, unethical, or inappropriate.\nSTEP 3: The response states: \"The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.\" This statement is neutral and factual regarding the generation of a file. There is no content that could be interpreted as harmful, unethical, or inappropriate.\nSTEP 4: The response is a neutral statement about file generation and contains no harmful, unethical, or inappropriate content.\nSTEP 5:"
            }
          ],
          "input": {
            "request": {
              "contents": [
                {
                  "role": "user",
                  "parts": [
                    {
                      "text": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability."
                    }
                  ]
                },
                {
                  "role": "model",
                  "parts": [
                    {
                      "text": "Hello! I can help with that. First, I need to process your request.\n"
                    }
                  ]
                }
              ]
            },
            "response": {
              "candidates": [
                {
                  "content": {
                    "role": "model",
                    "parts": [
                      {
                        "text": "The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`."
                      }
                    ]
                  }
                }
              ]
            }
          }
        },
        "pipeline_integrity": {
          "score": 2.0,
          "explanation": "The agent successfully gathered competitor data using `search_places`, and generated the report and infographic. However, it explicitly claims to have performed 'qualitative market research on demographics and economic trends' and applied 'a weighted scoring model' for location viability, yet the `google_search` tool (for market research) and `code execution` tool (for quantitative analysis/scoring as per `GapAnalysisAgent`) were not called. This indicates significant claims of analysis without corresponding tool execution, amounting to hallucination of key analytical steps.",
          "input": {
            "prompt": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability.",
            "response": "The infographic summarizing the location intelligence analysis has been successfully generated and saved as the artifact `infographic.png`.",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"I want to open a coffee shop in Seattle, WA. Please analyze the location viability.\"}, \"call_id\": \"adk-e260b8c9-ff84-4e2e-b0ec-ce1b7563a44b\", \"output_result\": {\"targetLocation\": \"Seattle, WA\", \"businessType\": \"coffee shop\", \"additionalContext\": \"Analyze the location viability.\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-a4f164ea-0c12-42b4-b71f-8410a5a97009\", \"outpu... [truncated]",
            "full_conversation": "{\"role\": \"user\", \"parts\": [{\"text\": \"I want to open a coffee shop in Seattle, WA. Please analyze the location viability.\"}]}"
          }
        }
      }
    }
  ]
}
```

*   **Detailed Explanations:** Raw, detailed explanations from the LLM judge for each metric on a per-question basis. Use this to find patterns in *why* a metric scored high or low.
**Detailed Explanations per Metric:**
--- Evaluation Analysis ---

## Metric: `token_usage.llm_calls`
**Average Score:** 11.0000

## Metric: `token_usage.total_tokens`
**Average Score:** 0.0000

## Metric: `token_usage.prompt_tokens`
**Average Score:** 135448.0000

## Metric: `token_usage.completion_tokens`
**Average Score:** 16156.0000

## Metric: `token_usage.cached_tokens`
**Average Score:** 0.0000

## Metric: `token_usage.estimated_cost_usd`
**Average Score:** 0.0000

## Metric: `latency_metrics.total_latency_seconds`
**Average Score:** 325.4837

## Metric: `latency_metrics.average_turn_latency_seconds`
**Average Score:** 325.4837

## Metric: `latency_metrics.llm_latency_seconds`
**Average Score:** 19.0000

## Metric: `latency_metrics.tool_latency_seconds`
**Average Score:** 10.0000

## Metric: `latency_metrics.time_to_first_response_seconds`
**Average Score:** 13.8115

## Metric: `cache_efficiency.cache_hit_rate`
**Average Score:** 0.0000

## Metric: `cache_efficiency.total_cached_tokens`
**Average Score:** 0.0000

## Metric: `cache_efficiency.total_fresh_prompt_tokens`
**Average Score:** 135448.0000

## Metric: `cache_efficiency.total_input_tokens`
**Average Score:** 135448.0000

## Metric: `thinking_metrics.reasoning_ratio`
**Average Score:** 0.3718

## Metric: `thinking_metrics.total_thinking_tokens`
**Average Score:** 9548.0000

## Metric: `thinking_metrics.total_candidate_tokens`
**Average Score:** 16134.0000

## Metric: `thinking_metrics.total_output_tokens`
**Average Score:** 25682.0000

## Metric: `thinking_metrics.turns_with_thinking`
**Average Score:** 10.0000

## Metric: `tool_utilization.total_tool_calls`
**Average Score:** 10.0000

## Metric: `tool_utilization.unique_tools_used`
**Average Score:** 5.0000

## Metric: `tool_success_rate.tool_success_rate`
**Average Score:** 1.0000

## Metric: `tool_success_rate.total_tool_calls`
**Average Score:** 5.0000

## Metric: `tool_success_rate.failed_tool_calls`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounding_chunks`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounded_responses`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_llm_responses`
**Average Score:** 11.0000

## Metric: `context_saturation.max_total_tokens`
**Average Score:** 63350.0000

## Metric: `agent_handoffs.total_handoffs`
**Average Score:** 5.0000

## Metric: `agent_handoffs.unique_agents_count`
**Average Score:** 3.0000

## Metric: `output_density.average_output_tokens`
**Average Score:** 1468.7273

## Metric: `output_density.total_output_tokens`
**Average Score:** 16156.0000

## Metric: `output_density.llm_calls_count`
**Average Score:** 11.0000

## Metric: `sandbox_usage.total_sandbox_ops`
**Average Score:** 0.0000

## Metric: `sandbox_usage.unique_ops_used`
**Average Score:** 0.0000

## Metric: `hallucinations_v1`
**Average Score:** {'average': 1.0}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.

## Metric: `safety_v1`
**Average Score:** {'average': 1.0}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.

## Metric: `rubric_based_tool_use_quality_v1`
**Average Score:** {'average': 0.5}
**Sample Explanations:**
- [Score: 0.5] Extracted from ADK simulation history.

## Metric: `rubric_based_final_response_quality_v1`
**Average Score:** {'average': 1.0}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.

## Metric: `general_quality`
**Average Score:** {'average': 0.083333336, 'score_range': {'min': 0, 'max': 1, 'description': 'Passing rate for general quality rubrics'}}

## Metric: `tool_use_quality`
**Average Score:** {'average': 5.0, 'score_range': {'min': 0, 'max': 5, 'description': '0=Poor tool usage, 5=Excellent tool usage'}}
**Sample Explanations:**
- [Score: 5.0] The agent used all relevant available tools (`IntakeAgent`, `search_places`, `generate_html_report`, `generate_infographic`) correctly to address the user request. The `IntakeAgent` parsed the request accurately. `search_places` was used to gather competitor data, which is crucial for location analysis. The `generate_html_report` and `generate_infographic` tools were used to present a comprehensive analysis and summary, respectively. The content for these reports, assumed to be mock data as per the prompt's instructions, was properly processed by the agent. The `transfer_to_agent` call is slightly ambiguous but did not impede the successful execution and completion of the core request.

## Metric: `trajectory_accuracy`
**Average Score:** {'average': 4.0, 'score_range': {'min': 0, 'max': 5, 'description': '0=Completely wrong, 5=Perfect trajectory'}}
**Sample Explanations:**
- [Score: 4.0] The agent followed a logical path, starting with intake, then using `search_places` for data collection, and finally generating both an HTML report and an infographic. While key stages like market research and gap analysis from the expected pipeline were not performed, this is due to the unavailability of `google_search` and `code execution` tools, which is not a flaw in the agent's execution but rather a limitation of its toolset. The agent gracefully handled this limitation by using the tools it had to deliver a relevant output.

## Metric: `text_quality`
**Average Score:** {'average': 0.2, 'score_range': {'min': 0, 'max': 1, 'description': 'Passing rate for text quality rubrics'}}

## Metric: `pipeline_integrity`
**Average Score:** {'average': 2.0, 'score_range': {'min': 0, 'max': 5, 'description': '0=Major integrity issues, 5=Perfect integrity'}}
**Sample Explanations:**
- [Score: 2.0] The agent successfully gathered competitor data using `search_places`, and generated the report and infographic. However, it explicitly claims to have performed 'qualitative market research on demographics and economic trends' and applied 'a weighted scoring model' for location viability, yet the `google_search` tool (for market research) and `code execution` tool (for quantitative analysis/scoring as per `GapAnalysisAgent`) were not called. This indicates significant claims of analysis without corresponding tool execution, amounting to hallucination of key analytical steps.


**2. Metric Calculation & Definitions:**
*   **Metric Definitions:** The rubrics and descriptions for each metric. You MUST use these files to understand what each metric is actually measuring and whether it is `llm` judged or `deterministic`.
**File: `../retail-ai-location-strategy/eval/results/20260120_020115/raw/temp_consolidated_metrics.json`**
```json
Error: File '../retail-ai-location-strategy/eval/results/20260120_020115/raw/temp_consolidated_metrics.json' not found.
```

*   **Deterministic Logic:** The Python code that calculates the deterministic metrics. Refer to this file to understand the precise logic behind scores for metrics like `token_usage` or `latency_metrics`.
**File: `evaluation/core/deterministic_metrics.py`**
```python
"""
Deterministic metrics for evaluating agent execution success.

These metrics provide objective pass/fail measurements by analyzing trace data
and session state, without requiring LLM-as-judge evaluation.
"""

import json
from typing import Any, Dict, List, Tuple

# Pricing per 1K tokens (approximate list prices for prompts <= 200k tokens)
# Format: {model_name: (prompt_price, completion_price)}
# Source: https://ai.google.dev/gemini-api/docs/pricing
MODEL_PRICING = {
    # Gemini 3 (Latest Preview)
    "gemini-3-pro-preview": (0.002, 0.012),  # $2.00 / $12.00 per 1M
    "gemini-3-flash-preview": (0.0005, 0.003),  # $0.50 / $3.00 per 1M
    # Gemini 2.5 (Current Flagship)
    "gemini-2.5-pro": (0.00125, 0.01),  # $1.25 / $10.00 per 1M
    "gemini-2.5-flash": (0.0003, 0.0025),  # $0.30 / $2.50 per 1M
    # Gemini 2.0
    "gemini-2.0-flash": (0.0001, 0.0004),  # $0.10 / $0.40 per 1M
    "gemini-2.0-flash-exp": (0.0001, 0.0004),  # Same as 2.0 flash
    "gemini-2.0-flash-lite": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    # Gemini 1.5 (Updated/Reduced Prices)
    "gemini-1.5-pro": (0.00125, 0.01),  # Reduced from 0.0035/0.0105
    "gemini-1.5-pro-001": (0.00125, 0.01),
    "gemini-1.5-flash": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    "gemini-1.5-flash-001": (0.000075, 0.0003),
    # Legacy
    "gemini-1.0-pro": (0.0005, 0.0015),  # $0.50 / $1.50 per 1M
    "default": (0.0001, 0.0004),  # Fallback to 2.0 Flash
}


def calculate_token_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Informational metric: Track token usage and estimated cost based on the specific model used.
    """
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_cached_tokens = 0
    total_tokens = 0
    llm_calls = 0
    total_cost = 0.0
    models_used = set()

    if not session_trace:
        return 0.0, "No trace data available for token usage calculation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Identify model (handle None values)
        model_name = attributes.get("gen_ai.request.model") or "default"
        model_name = model_name.lower() if isinstance(model_name, str) else "default"

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                if usage:
                    llm_calls += 1
                    models_used.add(model_name)

                    p_tokens = usage.get("prompt_token_count", 0)
                    c_tokens = usage.get("candidates_token_count", 0)
                    ch_tokens = usage.get("cached_content_token_count", 0)
                    t_tokens = usage.get("total_token_count", 0)

                    total_prompt_tokens += p_tokens
                    total_completion_tokens += c_tokens
                    total_cached_tokens += ch_tokens
                    total_tokens += t_tokens

                    # Match model pricing
                    pricing = MODEL_PRICING["default"]
                    for known_model, prices in MODEL_PRICING.items():
                        if known_model in model_name:
                            pricing = prices
                            break

                    # Cost calculation (simplified: ignoring cache discount for now to keep it safe upper bound,
                    # or strictly following list price for active tokens)
                    call_cost = (p_tokens / 1000 * pricing[0]) + (
                        c_tokens / 1000 * pricing[1]
                    )
                    # Note: Cached tokens usually have a separate (lower) pricing tier.
                    # For this metric, we currently only sum cost for active prompt/completion tokens.

                    total_cost += call_cost

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Usage: {llm_calls} LLM calls using {list(models_used)}. "
        f"Tokens: {total_tokens} ({total_prompt_tokens}p + {total_completion_tokens}c + {total_cached_tokens}ch). "
        f"Cost: ${total_cost:.6f}"
    )

    details = {
        "llm_calls": llm_calls,
        "models_used": list(models_used),
        "total_tokens": total_tokens,
        "prompt_tokens": total_prompt_tokens,
        "completion_tokens": total_completion_tokens,
        "cached_tokens": total_cached_tokens,
        "estimated_cost_usd": total_cost,
    }

    return total_cost, explanation, details


def calculate_latency_metrics(
    session_trace: List[Dict[str, Any]], latency_data: List[Dict[str, Any]] = None
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate latency metrics from the session trace.
    Returns the total latency score (seconds), but details contains granular breakdown.
    """
    total_latency = 0.0
    llm_latency = 0.0
    tool_latency = 0.0
    first_response_latency = None
    average_turn_latency = 0.0

    if not session_trace:
        return 0.0, "No trace data available for latency calculation", {}

    # Sort spans by start time to find the true beginning
    sorted_spans = sorted(
        [s for s in session_trace if s.get("start_time")], key=lambda x: x["start_time"]
    )

    if not sorted_spans:
        return 0.0, "Trace data has no timestamps", {}

    root_start = sorted_spans[0]["start_time"]

    # Calculate Component Latencies from full trace
    max_end = 0
    for span in session_trace:
        start = span.get("start_time", 0)
        end = span.get("end_time", 0)
        max_end = max(max_end, end)
        duration = (end - start) / 1e9
        name = span.get("name", "")

        if name == "call_llm":
            llm_latency += duration
            # Proxy for Time to First Token: end of first LLM call
            if first_response_latency is None:
                first_response_latency = (end - root_start) / 1e9

        elif "tool_call" in name or "execute_tool" in name:
            tool_latency += duration

    # Calculate Total & Average Latency from high-level summary (latency_data)
    # This is preferred as it excludes user think time in multi-turn sessions.
    if latency_data:
        turn_latencies = []
        for item in latency_data:
            if item.get("name") == "invocation":
                turn_latencies.append(item.get("duration_seconds", 0))

        if turn_latencies:
            average_turn_latency = sum(turn_latencies) / len(turn_latencies)
            total_latency = sum(turn_latencies)

    # Fallback: Wall-clock duration from trace if latency_data is missing
    if total_latency == 0.0 and max_end > root_start:
        total_latency = (max_end - root_start) / 1e9  # nanoseconds to seconds

    explanation = (
        f"Total: {total_latency:.4f}s. "
        f"Avg Turn: {average_turn_latency:.4f}s. "
        f"LLM: {llm_latency:.4f}s, Tools: {tool_latency:.4f}s. "
        f"First Response: {first_response_latency if first_response_latency else 0:.4f}s"
    )

    details = {
        "total_latency_seconds": total_latency,
        "average_turn_latency_seconds": average_turn_latency,
        "llm_latency_seconds": llm_latency,
        "tool_latency_seconds": tool_latency,
        "time_to_first_response_seconds": first_response_latency,
    }

    return total_latency, explanation, details


def calculate_cache_efficiency(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the efficiency of context caching.
    Returns the cache hit rate (percentage of potential prompt tokens that were cached).
    """
    total_prompt_tokens = 0
    total_cached_tokens = 0

    if not session_trace:
        return 0.0, "No trace data available for cache efficiency", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total_prompt_tokens += usage.get("prompt_token_count", 0)
                    total_cached_tokens += usage.get("cached_content_token_count", 0)
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    # Calculate hit rate
    # Note: 'prompt_token_count' in Gemini API usage metadata usually EXCLUDES cached tokens.
    # So total potential input = prompt_token_count + cached_content_token_count
    total_input_tokens = total_prompt_tokens + total_cached_tokens

    if total_input_tokens > 0:
        cache_hit_rate = total_cached_tokens / total_input_tokens
    else:
        cache_hit_rate = 0.0

    explanation = (
        f"Cache Hit Rate: {cache_hit_rate:.2%}. "
        f"Cached Tokens: {total_cached_tokens}. "
        f"Fresh Prompt Tokens: {total_prompt_tokens}."
    )

    details = {
        "cache_hit_rate": cache_hit_rate,
        "total_cached_tokens": total_cached_tokens,
        "total_fresh_prompt_tokens": total_prompt_tokens,
        "total_input_tokens": total_input_tokens,
    }

    return cache_hit_rate, explanation, details


def calculate_thinking_metrics(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate metrics related to the model's 'thinking' or reasoning process.
    Returns the reasoning ratio (thinking tokens / total output tokens).
    """
    total_thinking_tokens = 0
    total_candidate_tokens = 0
    turns_with_thinking = 0

    if not session_trace:
        return 0.0, "No trace data available for thinking metrics", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    thoughts = usage.get("thoughts_token_count", 0)
                    # Note: In some API versions, candidates_token_count might exclude thoughts.
                    # We treat them as additive components of the total output.
                    candidates = usage.get("candidates_token_count", 0)

                    total_thinking_tokens += thoughts
                    total_candidate_tokens += candidates

                    if thoughts > 0:
                        turns_with_thinking += 1
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    total_output_tokens = total_thinking_tokens + total_candidate_tokens

    if total_output_tokens > 0:
        reasoning_ratio = total_thinking_tokens / total_output_tokens
    else:
        reasoning_ratio = 0.0

    explanation = (
        f"Reasoning Ratio: {reasoning_ratio:.2%}. "
        f"Thinking Tokens: {total_thinking_tokens}. "
        f"Standard Output Tokens: {total_candidate_tokens}. "
        f"Turns with Thinking: {turns_with_thinking}."
    )

    details = {
        "reasoning_ratio": reasoning_ratio,
        "total_thinking_tokens": total_thinking_tokens,
        "total_candidate_tokens": total_candidate_tokens,
        "total_output_tokens": total_output_tokens,
        "turns_with_thinking": turns_with_thinking,
    }

    return reasoning_ratio, explanation, details


def calculate_tool_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate statistics on tool usage frequency and diversity.
    Returns the total number of tool calls.
    """
    total_tool_calls = 0
    tool_counts = {}

    if not session_trace:
        return 0.0, "No trace data available for tool utilization", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans.
        # Standard ADK traces often use "execute_tool <ToolName>"
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            total_tool_calls += 1
            tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1

    unique_tools_used = len(tool_counts)

    # Create a string representation of the tool breakdown
    breakdown_str = ", ".join([f"{k}: {v}" for k, v in tool_counts.items()])

    explanation = (
        f"Total Tool Calls: {total_tool_calls}. "
        f"Unique Tools: {unique_tools_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_tool_calls": total_tool_calls,
        "unique_tools_used": unique_tools_used,
        "tool_counts": tool_counts,
    }

    return float(total_tool_calls), explanation, details


def calculate_tool_success_rate(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the success rate of tool executions by inspecting tool responses.
    Returns success rate (successful / total) as score.
    """
    total_calls = 0
    failed_calls = 0
    failed_tools = []

    if not session_trace:
        return 0.0, "No trace data available for tool success rate", {}

    for span in session_trace:
        name = span.get("name", "")
        attributes = span.get("attributes", {})

        # Identify tool execution spans
        is_tool = name.startswith("execute_tool ") or "tool_call" in name

        if is_tool:
            tool_response_str = attributes.get("gcp.vertex.agent.tool_response")
            if tool_response_str:
                total_calls += 1
                try:
                    # Parse the JSON response to check status
                    response = json.loads(tool_response_str)

                    # Common error patterns in ADK/JSON tools
                    is_error = False
                    if isinstance(response, dict):
                        if response.get("status") == "error":
                            is_error = True
                        elif "error" in response or "error_message" in response:
                            is_error = True

                    if is_error:
                        failed_calls += 1
                        tool_name = name.replace("execute_tool ", "").strip()
                        failed_tools.append(tool_name)

                except (json.JSONDecodeError, TypeError):
                    # Malformed JSON in response could be considered a failure or ignored
                    pass

    if total_calls > 0:
        success_rate = (total_calls - failed_calls) / total_calls
    else:
        # If no tools were called, success rate is technically N/A, but 1.0 is a safe "no errors" default
        # Or 0.0 if we want to imply "no success possible".
        # For evaluation, 1.0 (no failures) usually makes more sense if no tools were attempted.
        # But to distinguish from "perfect execution", let's return 1.0 but note it.
        success_rate = 1.0

    explanation = (
        f"Success Rate: {success_rate:.2%}. "
        f"Total Calls: {total_calls}. "
        f"Failed Calls: {failed_calls}. "
        f"Failed Tools: {list(set(failed_tools))}"
    )

    details = {
        "tool_success_rate": success_rate,
        "total_tool_calls": total_calls,
        "failed_tool_calls": failed_calls,
        "failed_tools_list": failed_tools,
    }

    return success_rate, explanation, details


def calculate_grounding_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the extent of grounding usage by inspecting LLM responses for groundingMetadata.
    Returns total grounding chunks (citations) as the score.
    """
    total_grounded_responses = 0
    total_grounding_chunks = 0
    total_llm_responses = 0

    if not session_trace:
        return 0.0, "No trace data available for grounding utilization", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")

        if llm_response:
            total_llm_responses += 1
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                # Grounding metadata is usually at the top level or inside candidates
                # Standard Vertex AI response structure check
                grounding_metadata = response_data.get(
                    "groundingMetadata"
                ) or response_data.get("grounding_metadata")

                if not grounding_metadata:
                    # Check inside candidates if not at top level
                    candidates = response_data.get("candidates", [])
                    if candidates and isinstance(candidates, list):
                        first_candidate = candidates[0]
                        grounding_metadata = first_candidate.get(
                            "groundingMetadata"
                        ) or first_candidate.get("grounding_metadata")

                if grounding_metadata:
                    chunks = grounding_metadata.get(
                        "groundingChunks"
                    ) or grounding_metadata.get("grounding_chunks")
                    if chunks and isinstance(chunks, list) and len(chunks) > 0:
                        total_grounded_responses += 1
                        total_grounding_chunks += len(chunks)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Total Citations (Chunks): {total_grounding_chunks}. "
        f"Grounded Responses: {total_grounded_responses} / {total_llm_responses}."
    )

    details = {
        "total_grounding_chunks": total_grounding_chunks,
        "total_grounded_responses": total_grounded_responses,
        "total_llm_responses": total_llm_responses,
    }

    return float(total_grounding_chunks), explanation, details


def calculate_context_saturation(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the maximum context saturation (max total tokens used in a single turn).
    Returns max_tokens as the score.
    """
    max_tokens = 0
    max_token_span = ""

    if not session_trace:
        return 0.0, "No trace data available for context saturation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total = usage.get("total_token_count", 0)
                    if total > max_tokens:
                        max_tokens = total
                        max_token_span = span.get("name", "unknown")
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Max Context Used: {max_tokens} tokens. Peak occurred in: {max_token_span}."
    )

    details = {"max_total_tokens": max_tokens, "peak_usage_span": max_token_span}

    return float(max_tokens), explanation, details


def calculate_agent_handoffs(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of agent handoffs/invocations in the session.
    Returns total handoff events as the score.

    Captures:
    - Direct agent invocations (invoke_agent, agent_run)
    - Sub-agents called as tools (execute_tool *Agent, transfer_to_agent)
    """
    handoff_count = 0
    agents_invoked = set()

    if not session_trace:
        return 0.0, "No trace data available for agent handoffs", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for direct agent invocations
        if name.startswith("invoke_agent ") or name.startswith("agent_run "):
            agent_name = (
                name.replace("invoke_agent ", "").replace("agent_run ", "").strip()
            )
            handoff_count += 1
            agents_invoked.add(agent_name)

        # Check for sub-agents called as tools (e.g., "execute_tool IntakeAgent")
        elif name.startswith("execute_tool "):
            tool_name = name.replace("execute_tool ", "").strip()
            # Sub-agents typically end with "Agent" or are transfer_to_agent
            if tool_name.endswith("Agent") or tool_name == "transfer_to_agent":
                handoff_count += 1
                agents_invoked.add(tool_name)

    explanation = (
        f"Total Handoffs: {handoff_count}. "
        f"Unique Agents: {len(agents_invoked)}. "
        f"Agents: {list(agents_invoked)}"
    )

    details = {
        "total_handoffs": handoff_count,
        "unique_agents_count": len(agents_invoked),
        "agents_invoked_list": list(agents_invoked),
    }

    return float(handoff_count), explanation, details


def calculate_output_density(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the average number of output tokens per LLM call.
    Returns average output tokens as the score.
    """
    total_output_tokens = 0
    llm_calls = 0

    if not session_trace:
        return 0.0, "No trace data available for output density", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                # Check for output tokens in standard fields (candidates_token_count or output_token_count)
                output_tokens = 0
                if usage:
                    # 'candidates_token_count' is standard in Vertex AI
                    output_tokens = usage.get("candidates_token_count", 0)
                    if output_tokens == 0:
                        # Fallback for other providers
                        output_tokens = usage.get("output_token_count", 0) or usage.get(
                            "completion_tokens", 0
                        )

                if (
                    output_tokens > 0 or usage
                ):  # Count the call even if 0 output (edge case)
                    llm_calls += 1
                    total_output_tokens += output_tokens

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    if llm_calls > 0:
        average_output_tokens = total_output_tokens / llm_calls
    else:
        average_output_tokens = 0.0

    explanation = (
        f"Avg Output Tokens: {average_output_tokens:.2f}. "
        f"Total Output Tokens: {total_output_tokens}. "
        f"LLM Calls: {llm_calls}."
    )

    details = {
        "average_output_tokens": average_output_tokens,
        "total_output_tokens": total_output_tokens,
        "llm_calls_count": llm_calls,
    }

    return float(average_output_tokens), explanation, details


def calculate_sandbox_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of tool calls related to sandbox/file system operations.
    Returns the total count as the score.
    """
    sandbox_ops_count = 0
    sandbox_tools_used = {}

    # Common keywords for sandbox/file operations
    sandbox_keywords = [
        "save_artifact",
        "load_artifact",
        "read_file",
        "write_file",
        "run_python_script",
        "execute_code",
        "save_to_file",
        "read_from_file",
    ]

    if not session_trace:
        return 0.0, "No trace data available for sandbox usage", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            # Check if tool matches sandbox keywords
            if any(keyword in tool_name.lower() for keyword in sandbox_keywords):
                sandbox_ops_count += 1
                sandbox_tools_used[tool_name] = sandbox_tools_used.get(tool_name, 0) + 1

    unique_ops_used = len(sandbox_tools_used)

    breakdown_str = ", ".join([f"{k}: {v}" for k, v in sandbox_tools_used.items()])

    explanation = (
        f"Total Sandbox Ops: {sandbox_ops_count}. "
        f"Unique Ops: {unique_ops_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_sandbox_ops": sandbox_ops_count,
        "unique_ops_used": unique_ops_used,
        "sandbox_tools_used": sandbox_tools_used,
    }

    return float(sandbox_ops_count), explanation, details


# Registry of all deterministic metrics
DETERMINISTIC_METRICS = {
    "token_usage": calculate_token_usage,
    "latency_metrics": calculate_latency_metrics,
    "cache_efficiency": calculate_cache_efficiency,
    "thinking_metrics": calculate_thinking_metrics,
    "tool_utilization": calculate_tool_utilization,
    "tool_success_rate": calculate_tool_success_rate,
    "grounding_utilization": calculate_grounding_utilization,
    "context_saturation": calculate_context_saturation,
    "agent_handoffs": calculate_agent_handoffs,
    "output_density": calculate_output_density,
    "sandbox_usage": calculate_sandbox_usage,
}


def evaluate_deterministic_metrics(
    session_state: Dict[str, Any],
    session_trace: List[Dict[str, Any]],
    agents_evaluated: List[str],
    question_metadata: Dict[str, Any],
    metrics_to_run: List[str] = None,
    reference_data: Dict[str, Any] = None,
    metric_definitions: Dict[str, Any] = None,
    latency_data: List[Dict[str, Any]] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate all specified deterministic metrics.
    """
    if metrics_to_run is None:
        metrics_to_run = list(DETERMINISTIC_METRICS.keys())

    results = {}
    for metric_name in metrics_to_run:
        if metric_name not in DETERMINISTIC_METRICS:
            continue

        metric_func = DETERMINISTIC_METRICS[metric_name]

        try:
            if metric_name == "latency_metrics":
                score, explanation, details = metric_func(
                    session_trace, latency_data=latency_data
                )
            else:
                score, explanation, details = metric_func(session_trace)

            results[metric_name] = {
                "score": score,
                "explanation": explanation,
                "details": details,
            }
        except Exception as e:
            results[metric_name] = {
                "score": 0.0,
                "explanation": f"Error evaluating metric {metric_name}: {str(e)}",
            }

    return results

```

**3. Agent Implementation Details:**
*   **Agent Source Code:** The source code for the agent being evaluated. This is your primary source for forming hypotheses about *why* the agent behaves a certain way.
**File: `../retail-ai-location-strategy/app/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Retail Location Strategy Agent - Root Agent Definition.

This module defines the root agent for the Location Strategy Pipeline.
It uses a SequentialAgent to orchestrate 6 specialized sub-agents:

1. MarketResearchAgent - Live web research with Google Search
2. CompetitorMappingAgent - Competitor mapping with Maps Places API
3. GapAnalysisAgent - Quantitative analysis with Python code execution
4. StrategyAdvisorAgent - Strategic synthesis with extended reasoning
5. ReportGeneratorAgent - HTML executive report generation
6. InfographicGeneratorAgent - Visual infographic generation

The pipeline analyzes a target location for a specific business type and
produces comprehensive location intelligence including recommendations,
an HTML report, and an infographic.

Authentication:
    Uses Google AI Studio (API key) instead of Vertex AI.
    Set environment variables:
        GOOGLE_API_KEY=your_api_key
        GOOGLE_GENAI_USE_VERTEXAI=FALSE
        MAPS_API_KEY=your_maps_api_key

Usage:
    Run with: adk web retail_ai_location_strategy_adk

    The agent expects initial state variables:
    - target_location: The geographic area to analyze (e.g., "Bangalore, India")
    - business_type: Type of business to open (e.g., "coffee shop")

    Optional state variables:
    - maps_api_key: Google Maps API key for Places search
"""

from google.adk.agents import SequentialAgent
from google.adk.agents.llm_agent import Agent
from google.adk.tools.agent_tool import AgentTool


from .sub_agents.intake_agent.agent import intake_agent
from .sub_agents.market_research.agent import market_research_agent
from .sub_agents.competitor_mapping.agent import competitor_mapping_agent
from .sub_agents.gap_analysis.agent import gap_analysis_agent
from .sub_agents.strategy_advisor.agent import strategy_advisor_agent
from .sub_agents.infographic_generator.agent import infographic_generator_agent
from .sub_agents.report_generator.agent import report_generator_agent

from .config import FAST_MODEL, APP_NAME

# location_strategy_pipeline
location_strategy_pipeline = SequentialAgent(
    name="LocationStrategyPipeline",
    description="""Comprehensive retail location strategy analysis pipeline.

This agent analyzes a target location for a specific business type and produces:
1. Market research findings from live web data
2. Competitor mapping from Google Maps Places API
3. Quantitative gap analysis with zone rankings
4. Strategic recommendations with structured JSON output
5. Professional HTML executive report
6. Visual infographic summary

To use, get the following details:
- target_location: {target_location}
- business_type: {business_type}

The analysis runs automatically through all stages and produces artifacts
including JSON report, HTML report, and infographic image.
""",
    sub_agents=[
        market_research_agent,      # Part 1: Market research with search
        competitor_mapping_agent,   # Part 2A: Competitor mapping with Maps
        gap_analysis_agent,         # Part 2B: Gap analysis with code exec
        strategy_advisor_agent,     # Part 3: Strategy synthesis
        report_generator_agent,     # Part 4: HTML report generation
        infographic_generator_agent,  # Part 5: Infographic generation
    ],
)

# Root agent orchestrating the complete location strategy pipeline
root_agent = Agent(
    model=FAST_MODEL,
    name=APP_NAME,
    description='A strategic partner for retail businesses, guiding them to optimal physical locations that foster growth and profitability.',
    instruction="""Your primary role is to orchestrate the retail location analysis.
1. Start by greeting the user.
2. Check if the `TARGET_LOCATION` (Geographic area to analyze (e.g., "Indiranagar, Bangalore")) and `BUSINESS_TYPE` (Type of business (e.g., "coffee shop", "bakery", "gym")) have been provided.
3. If they are missing, **ask the user clarifying questions to get the required information.**
4. Once you have the necessary details, call the `IntakeAgent` tool to process them.
5. After the `IntakeAgent` is successful, delegate the full analysis to the `LocationStrategyPipeline`.
Your main function is to manage this workflow conversationally.""",
    sub_agents=[location_strategy_pipeline],
    tools = [AgentTool(intake_agent)], # Part 0: Parse user request
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/gap_analysis/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Gap Analysis Agent - Part 2B of the Location Strategy Pipeline.

This agent performs quantitative gap analysis using Python code execution
to calculate saturation indices, viability scores, and zone rankings.
"""

from google.adk.agents import LlmAgent
from google.adk.code_executors import BuiltInCodeExecutor
from google.genai import types

from ...config import CODE_EXEC_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...callbacks import before_gap_analysis, after_gap_analysis


GAP_ANALYSIS_INSTRUCTION = """You are a data scientist analyzing market opportunities using quantitative methods.

Your task is to perform advanced gap analysis on the data collected from previous stages.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Available Data

### MARKET RESEARCH FINDINGS (Part 1):
{market_research_findings}

### COMPETITOR ANALYSIS (Part 2):
{competitor_analysis}

## Your Mission
Write and execute Python code to perform comprehensive quantitative analysis.

## Analysis Steps

### Step 1: Parse Competitor Data
Extract from the competitor analysis:
- Competitor names and locations
- Ratings and review counts
- Zone/area classifications
- Business types (chain vs independent)

### Step 2: Extract Market Fundamentals
From the market research:
- Population estimates
- Income levels (assign numeric scores)
- Infrastructure quality indicators
- Foot traffic patterns

### Step 3: Calculate Zone Metrics
For each identified zone, compute:

**Basic Metrics:**
- Competitor count
- Competitor density (per estimated area)
- Average competitor rating
- Total review volume

**Quality Metrics:**
- Competition Quality Score: Weighted by ratings (4.5+ = high threat)
- Chain Dominance Ratio: % of chain/franchise competitors
- High Performer Count: Number of 4.5+ rated competitors

**Opportunity Metrics:**
- Demand Signal: Based on population, income, infrastructure
- Market Saturation Index: (Competitors Ã— Quality) / Demand
- Viability Score: Multi-factor weighted score

### Step 4: Zone Categorization
Classify each zone as:
- **SATURATED**: High competition, low opportunity
- **MODERATE**: Balanced market, moderate opportunity
- **OPPORTUNITY**: Low competition, high potential

Also assign:
- Risk Level: Low / Medium / High
- Investment Tier: Based on expected costs
- Best Customer Segment: Target demographic

### Step 5: Rank Top Zones
Create a weighted ranking considering:
- Low market saturation (weight: 30%)
- High demand signals (weight: 30%)
- Low chain dominance (weight: 15%)
- Infrastructure quality (weight: 15%)
- Manageable costs (weight: 10%)

### Step 6: Output Tables
Generate clear output tables showing:
1. All zones with computed metrics
2. Top 3 recommended zones with scores
3. Risk assessment matrix

## Code Guidelines
- Use pandas for data manipulation
- Print all results clearly formatted
- Include intermediate calculations for transparency
- Handle missing data gracefully

Execute the code and provide actionable strategic recommendations based on the quantitative findings.
"""

gap_analysis_agent = LlmAgent(
    name="GapAnalysisAgent",
    model=CODE_EXEC_MODEL,
    description="Performs quantitative gap analysis using Python code execution for zone rankings and viability scores",
    instruction=GAP_ANALYSIS_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    code_executor=BuiltInCodeExecutor(),
    output_key="gap_analysis",
    before_agent_callback=before_gap_analysis,
    after_agent_callback=after_gap_analysis,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/intake_agent/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Intake Agent - Extracts target location and business type from user request.

This agent parses the user's natural language request and extracts the
required parameters (target_location, business_type) into session state
for use by subsequent agents in the pipeline.
"""

from typing import Optional

from google.adk.agents import LlmAgent
from google.adk.agents.callback_context import CallbackContext
from google.genai import types
from pydantic import BaseModel, Field

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS


class UserRequest(BaseModel):
    """Structured output for parsing user's location strategy request."""

    target_location: str = Field(
        description="The geographic location/area to analyze (e.g., 'Indiranagar, Bangalore', 'Manhattan, New York')"
    )
    business_type: str = Field(
        description="The type of business the user wants to open (e.g., 'coffee shop', 'bakery', 'gym', 'restaurant')"
    )
    additional_context: Optional[str] = Field(
        default=None,
        description="Any additional context or requirements mentioned by the user"
    )


def after_intake(callback_context: CallbackContext) -> Optional[types.Content]:
    """After intake, copy the parsed values to state for other agents."""
    parsed = callback_context.state.get("parsed_request", {})

    if isinstance(parsed, dict):
        # Extract values from parsed request
        callback_context.state["target_location"] = parsed.get("target_location", "")
        callback_context.state["business_type"] = parsed.get("business_type", "")
        callback_context.state["additional_context"] = parsed.get("additional_context", "")
    elif hasattr(parsed, "target_location"):
        # Handle Pydantic model
        callback_context.state["target_location"] = parsed.target_location
        callback_context.state["business_type"] = parsed.business_type
        callback_context.state["additional_context"] = parsed.additional_context or ""

    # Track intake stage completion
    stages = callback_context.state.get("stages_completed", [])
    stages.append("intake")
    callback_context.state["stages_completed"] = stages

    # Note: current_date is set in each agent's before_callback to ensure it's always available
    return None


INTAKE_INSTRUCTION = """You are a request parser for a retail location intelligence system.

Your task is to extract the target location and business type from the user's request.

## Examples

User: "I want to open a coffee shop in Indiranagar, Bangalore"
â†’ target_location: "Indiranagar, Bangalore"
â†’ business_type: "coffee shop"

User: "Analyze the market for a new gym in downtown Seattle"
â†’ target_location: "downtown Seattle"
â†’ business_type: "gym"

User: "Help me find the best location for a bakery in Mumbai"
â†’ target_location: "Mumbai"
â†’ business_type: "bakery"

User: "Where should I open my restaurant in San Francisco's Mission District?"
â†’ target_location: "Mission District, San Francisco"
â†’ business_type: "restaurant"

## Instructions
1. Extract the geographic location mentioned by the user
2. Identify the type of business they want to open
3. Note any additional context or requirements

If the user doesn't specify a clear location or business type, make a reasonable inference or ask for clarification.
"""

intake_agent = LlmAgent(
    name="IntakeAgent",
    model=FAST_MODEL,
    description="Parses user request to extract target location and business type",
    instruction=INTAKE_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    output_schema=UserRequest,
    output_key="parsed_request",
    after_agent_callback=after_intake,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/infographic_generator/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Infographic Generator Agent - Part 5 (Bonus) of the Location Strategy Pipeline.

This agent creates a visual infographic summary using Gemini's image generation
capabilities to provide an executive-ready visual summary of the analysis.
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import generate_infographic
from ...callbacks import before_infographic_generator, after_infographic_generator


INFOGRAPHIC_GENERATOR_INSTRUCTION = """You are a data visualization specialist creating executive-ready infographics.

Your task is to generate a visual infographic summarizing the location intelligence analysis.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Strategic Report Data
{strategic_report}

## Your Mission
Create a compelling infographic that visually summarizes the key findings from the analysis.

## Steps

### Step 1: Extract Key Data Points
From the strategic report, identify:
- Target location and business type
- Top recommended location with score
- Total competitors found
- Number of zones analyzed
- 3-5 key strategic insights
- Top strengths and concerns
- Market validation verdict

### Step 2: Create Data Summary
Compose a concise data summary suitable for visualization:

**FORMAT YOUR SUMMARY AS:**

LOCATION INTELLIGENCE REPORT: [Business Type] in [Target Location]
Analysis Date: [Date]

TOP RECOMMENDATION:
[Location Name] - Score: [XX]/100
Type: [Opportunity Type]

KEY METRICS:
- Total Competitors: [X]
- Zones Analyzed: [X]
- Market Status: [Validated/Cautionary]

TOP STRENGTHS:
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

KEY INSIGHTS:
- [Insight 1]
- [Insight 2]
- [Insight 3]

VERDICT: [One-line market recommendation]

### Step 3: Generate Infographic
Call the generate_infographic tool with your data summary.

### Step 4: Report Result
After the tool returns, store the result for the callback to process.
If successful, confirm the infographic was generated.
If failed, report the error for troubleshooting.

## Output
The generate_infographic tool will return a result dict containing:
- status: "success" or "error"
- image_data: Base64 encoded PNG (if successful)
- error_message: Error details (if failed)

Store this result so the after_agent_callback can save the artifact.
"""

infographic_generator_agent = LlmAgent(
    name="InfographicGeneratorAgent",
    model=FAST_MODEL,
    description="Generates visual infographic summary using Gemini image generation",
    instruction=INFOGRAPHIC_GENERATOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[generate_infographic],
    output_key="infographic_result",
    before_agent_callback=before_infographic_generator,
    after_agent_callback=after_infographic_generator,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/market_research/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Market Research Agent - Part 1 of the Location Strategy Pipeline.

This agent validates macro market viability using live web data from Google Search.
It researches demographics, market trends, and commercial viability.
"""

from google.adk.agents import LlmAgent
from google.adk.tools import google_search
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...callbacks import before_market_research, after_market_research


MARKET_RESEARCH_INSTRUCTION = """You are a market research analyst specializing in retail location intelligence.

Your task is to research and validate the target market for a new business location.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Research Focus Areas

### 1. DEMOGRAPHICS
- Age distribution (identify key age groups)
- Income levels and purchasing power
- Lifestyle indicators (professionals, students, families)
- Population density and growth trends

### 2. MARKET GROWTH
- Population trends (growing, stable, declining)
- New residential and commercial developments
- Infrastructure improvements (metro, roads, tech parks)
- Economic growth indicators

### 3. INDUSTRY PRESENCE
- Existing similar businesses in the area
- Consumer preferences and spending patterns
- Market saturation indicators
- Success stories or failures of similar businesses

### 4. COMMERCIAL VIABILITY
- Foot traffic patterns (weekday vs weekend)
- Commercial real estate trends
- Typical rental costs (qualitative: low/medium/high)
- Business environment and regulations

## Instructions
1. Use Google Search to find current, verifiable data
2. Cite specific data points with sources where possible
3. Focus on information from the last 1-2 years for relevance
4. Be factual and data-driven, avoid speculation

## Output Format
Provide a structured analysis covering all four focus areas.
Conclude with a clear verdict: Is this a strong market for {business_type}? Why or why not?
Include specific recommendations for market entry strategy.
"""

market_research_agent = LlmAgent(
    name="MarketResearchAgent",
    model=FAST_MODEL,
    description="Researches market viability using Google Search for real-time demographics, trends, and commercial data",
    instruction=MARKET_RESEARCH_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[google_search],
    output_key="market_research_findings",
    before_agent_callback=before_market_research,
    after_agent_callback=after_market_research,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/strategy_advisor/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Strategy Advisor Agent - Part 3 of the Location Strategy Pipeline.

This agent synthesizes all findings into actionable recommendations using
extended reasoning (thinking mode) and outputs a structured JSON report.
"""

from google.adk.agents import LlmAgent
from google.adk.planners import BuiltInPlanner
from google.genai import types
from google.genai.types import ThinkingConfig

from ...config import PRO_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...schemas import LocationIntelligenceReport
from ...callbacks import before_strategy_advisor, after_strategy_advisor


STRATEGY_ADVISOR_INSTRUCTION = """You are a senior strategy consultant synthesizing location intelligence findings.

Your task is to analyze all research and provide actionable strategic recommendations.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Available Data

### MARKET RESEARCH FINDINGS (Part 1):
{market_research_findings}

### COMPETITOR ANALYSIS (Part 2A):
{competitor_analysis}

### GAP ANALYSIS (Part 2B):
{gap_analysis}

## Your Mission
Synthesize all findings into a comprehensive strategic recommendation.

## Analysis Framework

### 1. Data Integration
Review all inputs carefully:
- Market research demographics and trends
- Competitor locations, ratings, and patterns
- Quantitative gap analysis metrics and zone rankings

### 2. Strategic Synthesis
For each promising zone, evaluate:
- Opportunity Type: Categorize (e.g., "Metro First-Mover", "Residential Sticky", "Mall Impulse")
- Overall Score: 0-100 weighted composite
- Strengths: Top 3-4 factors with evidence from the analysis
- Concerns: Top 2-3 risks with specific mitigation strategies
- Competition Profile: Summarize density, quality, chain presence
- Market Characteristics: Population, income, infrastructure, foot traffic, costs
- Best Customer Segment: Primary target demographic
- Next Steps: 3-5 specific actionable recommendations

### 3. Top Recommendation Selection
Choose the single best location based on:
- Highest weighted opportunity score
- Best balance of opportunity vs risk
- Most aligned with business type requirements
- Clear competitive advantage potential

### 4. Alternative Locations
Identify 2-3 alternative locations:
- Brief scoring and categorization
- Key strength and concern for each
- Why it's not the top choice

### 5. Strategic Insights
Provide 4-6 key insights that span the entire analysis:
- Market-level observations
- Competitive dynamics
- Timing considerations
- Success factors

## Output Requirements
Your response MUST conform to the LocationIntelligenceReport schema.
Ensure all fields are populated with specific, actionable information.
Use evidence from the analysis to support all recommendations.
"""

strategy_advisor_agent = LlmAgent(
    name="StrategyAdvisorAgent",
    model=PRO_MODEL,
    description="Synthesizes findings into strategic recommendations using extended reasoning and structured output",
    instruction=STRATEGY_ADVISOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    planner=BuiltInPlanner(
        thinking_config=ThinkingConfig(
            include_thoughts=False,  # Must be False when using output_schema
            thinking_budget=-1,  # -1 means unlimited thinking budget
        )
    ),
    output_schema=LocationIntelligenceReport,
    output_key="strategic_report",
    before_agent_callback=before_strategy_advisor,
    after_agent_callback=after_strategy_advisor,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/competitor_mapping/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Competitor Mapping Agent - Part 2A of the Location Strategy Pipeline.

This agent maps competitors using the Google Maps Places API to get
ground-truth data about existing businesses in the target area.
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import search_places
from ...callbacks import before_competitor_mapping, after_competitor_mapping


COMPETITOR_MAPPING_INSTRUCTION = """You are a market intelligence analyst specializing in competitive landscape analysis.

Your task is to map and analyze all competitors in the target area using real Google Maps data.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Your Mission
Use the search_places function to get REAL data from Google Maps about existing competitors.

## Step 1: Search for Competitors
Call the search_places function with queries like:
- "{business_type} near {target_location}"
- Related business types in the same area

## Step 2: Analyze the Results
For each competitor found, note:
- Business name
- Location/address
- Rating (out of 5)
- Number of reviews
- Business status (operational, etc.)

## Step 3: Identify Patterns
Analyze the competitive landscape:

### Geographic Clustering
- Are competitors clustered in specific areas/zones?
- Which areas have high concentration vs sparse presence?
- Are there any "dead zones" with no competitors?

### Location Types
- Shopping malls and retail areas
- Main roads and commercial corridors
- Residential neighborhoods
- Near transit (metro stations, bus stops)

### Quality Segmentation
- Premium tier: High-rated (4.5+), likely higher prices
- Mid-market: Ratings 4.0-4.4
- Budget tier: Lower ratings or basic offerings
- Chain vs independent businesses

## Step 4: Strategic Assessment
Provide insights on:
- Which areas appear saturated with competitors?
- Which areas might be underserved opportunities?
- What quality gaps exist (e.g., no premium options)?
- Where are the strongest competitors located?

## Output Format
Provide a detailed competitor map with:
1. List of all competitors found with their details
2. Zone-by-zone breakdown of competition
3. Pattern analysis and clustering insights
4. Strategic opportunities and saturation warnings

Be specific and reference the actual data you receive from the search_places tool.
"""

competitor_mapping_agent = LlmAgent(
    name="CompetitorMappingAgent",
    model=FAST_MODEL,
    description="Maps competitors using Google Maps Places API for ground-truth competitor data",
    instruction=COMPETITOR_MAPPING_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[search_places],
    output_key="competitor_analysis",
    before_agent_callback=before_competitor_mapping,
    after_agent_callback=after_competitor_mapping,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/report_generator/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Report Generator Agent - Part 4 of the Location Strategy Pipeline.

This agent generates a professional HTML executive report from the
structured LocationIntelligenceReport data using the generate_html_report tool.

The tool handles:
- Calling Gemini to generate 7-slide McKinsey/BCG style HTML
- Saving the HTML as an artifact for download in adk web
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import generate_html_report
from ...callbacks import before_report_generator, after_report_generator


REPORT_GENERATOR_INSTRUCTION = """You are an executive report generator for location intelligence analysis.

Your task is to create a professional HTML executive report using the generate_html_report tool.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Strategic Report Data
{strategic_report}

## Your Mission
Format the strategic report data and call the generate_html_report tool to create a
McKinsey/BCG-style 7-slide HTML presentation.

## Steps

### Step 1: Format the Report Data
Prepare a comprehensive data summary from the strategic report above, including:
- Analysis overview (location, business type, date, market validation)
- Top recommendation details (location, score, opportunity type, strengths, concerns)
- Competition metrics (total competitors, density, chain dominance, ratings)
- Market characteristics (population, income, infrastructure, foot traffic, rental costs)
- Alternative locations (name, score, strength, concern, why not top)
- Next steps (actionable items)
- Key insights (strategic observations)
- Methodology summary

### Step 2: Call the Tool
Call the generate_html_report tool with the formatted report data.
The tool will:
- Generate a professional 7-slide HTML report
- Save it as an artifact named "executive_report.html"
- Return the status and artifact details

### Step 3: Report Result
After the tool returns, confirm the report was generated successfully.
If there was an error, report what went wrong.
"""

report_generator_agent = LlmAgent(
    name="ReportGeneratorAgent",
    model=FAST_MODEL,
    description="Generates professional McKinsey/BCG-style HTML executive reports using the generate_html_report tool",
    instruction=REPORT_GENERATOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[generate_html_report],
    output_key="report_generation_result",
    before_agent_callback=before_report_generator,
    after_agent_callback=after_report_generator,
)

```

**4. Evaluation Questions:**
*   **Questions Evaluated:** The full set of questions used in the evaluation. This can provide context if certain types of questions are causing specific failures.
**Questions Evaluated**
```json
Questions file not found.
```

---
Format your entire response as a single Markdown document.
