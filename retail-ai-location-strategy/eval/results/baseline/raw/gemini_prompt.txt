
You are an expert AI evaluation analyst. Your task is to produce a deep technical diagnosis of an AI agent's performance for technical stakeholders.

**Tone:** objective, analytical, and professional
**Report Length:** comprehensive



**CRITICAL INSTRUCTIONS:**
1.  **Focus on Diagnosis, Not Recommendations:** Your primary goal is to explain *why* the metrics are what they are. Do not provide a future-looking action plan unless the Strategic Framework above explicitly guides it.
2.  **Synthesize, Don't Summarize:** Do not simply repeat the scores. Your value is in synthesizing insights by connecting the metric scores, the metric definitions, the source code, and the raw explanations.
3.  **Reference Your Sources:** When you make a claim or analyze a metric, you MUST reference the specific source file (e.g., `metric_definitions.json`, `deterministic_metrics.py`, `agent.py`).
4.  **Analyze Calculation Methods:** For each metric you discuss, you MUST explain how its calculation method (deterministic vs. LLM-judged) influences its interpretation.

---

**Context for Your Analysis**

You are provided with the following context files to perform your diagnosis. Use them to connect the agent's behavior (the metrics) to its underlying implementation (the code).

**1. Overall Performance Data:**
*   **Evaluation Summary:** High-level average scores for all metrics. Use this to identify the most significant areas of success and failure.
**Evaluation Summary**
```json
{
  "experiment_id": "eval-20260119_183419",
  "run_type": "manual",
  "test_description": "Automated evaluation",
  "interaction_datetime": "2026-01-19T18:34:21.097737",
  "overall_summary": {
    "deterministic_metrics": {
      "token_usage.llm_calls": 11.0,
      "token_usage.total_tokens": 62085.0,
      "token_usage.prompt_tokens": 94371.0,
      "token_usage.completion_tokens": 11334.0,
      "token_usage.cached_tokens": 31166.0,
      "token_usage.estimated_cost_usd": 0.08751375,
      "latency_metrics.total_latency_seconds": 295.3988,
      "latency_metrics.average_turn_latency_seconds": 295.3988,
      "latency_metrics.llm_latency_seconds": 19.0,
      "latency_metrics.tool_latency_seconds": 10.0,
      "latency_metrics.time_to_first_response_seconds": 14.356017152,
      "cache_efficiency.cache_hit_rate": 0.24826146873033447,
      "cache_efficiency.total_cached_tokens": 31166.0,
      "cache_efficiency.total_fresh_prompt_tokens": 94371.0,
      "cache_efficiency.total_input_tokens": 125537.0,
      "thinking_metrics.reasoning_ratio": 0.4229131393976707,
      "thinking_metrics.total_thinking_tokens": 8243.0,
      "thinking_metrics.total_candidate_tokens": 11248.0,
      "thinking_metrics.total_output_tokens": 19491.0,
      "thinking_metrics.turns_with_thinking": 9.0,
      "tool_utilization.total_tool_calls": 10.0,
      "tool_utilization.unique_tools_used": 5.0,
      "tool_success_rate.tool_success_rate": 1.0,
      "tool_success_rate.total_tool_calls": 5.0,
      "tool_success_rate.failed_tool_calls": 0.0,
      "grounding_utilization.total_grounding_chunks": 0.0,
      "grounding_utilization.total_grounded_responses": 0.0,
      "grounding_utilization.total_llm_responses": 11.0,
      "context_saturation.max_total_tokens": 33081.0,
      "agent_handoffs.total_handoffs": 5.0,
      "agent_handoffs.unique_agents_count": 3.0,
      "output_density.average_output_tokens": 1030.3636363636363,
      "output_density.total_output_tokens": 11334.0,
      "output_density.llm_calls_count": 11.0,
      "sandbox_usage.total_sandbox_ops": 0.0,
      "sandbox_usage.unique_ops_used": 0.0
    },
    "llm_based_metrics": {
      "hallucinations_v1": {
        "average": 1.0
      },
      "safety_v1": {
        "average": 1.0
      },
      "rubric_based_tool_use_quality_v1": {
        "average": 0.5
      },
      "rubric_based_final_response_quality_v1": {
        "average": 1.0
      },
      "general_quality": {
        "average": 0.1,
        "score_range": {
          "min": 0,
          "max": 1,
          "description": "Passing rate for general quality rubrics"
        }
      },
      "tool_use_quality": {
        "average": 5.0,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Poor tool usage, 5=Excellent tool usage"
        }
      },
      "trajectory_accuracy": {
        "average": 5.0,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Completely wrong, 5=Perfect trajectory"
        }
      },
      "pipeline_integrity": {
        "average": 1.0,
        "score_range": {
          "min": 0,
          "max": 5,
          "description": "0=Major integrity issues, 5=Perfect integrity"
        }
      },
      "text_quality": {
        "average": 0.3846154,
        "score_range": {
          "min": 0,
          "max": 1,
          "description": "Passing rate for text quality rubrics"
        }
      }
    }
  },
  "per_question_summary": [
    {
      "question_id": "seattle_coffee_001",
      "runs": 1,
      "deterministic_metrics": {
        "token_usage": {
          "llm_calls": 11,
          "models_used": [
            "gemini-2.5-pro"
          ],
          "total_tokens": 62085,
          "prompt_tokens": 94371,
          "completion_tokens": 11334,
          "cached_tokens": 31166,
          "estimated_cost_usd": 0.08751375
        },
        "latency_metrics": {
          "total_latency_seconds": 295.3988,
          "average_turn_latency_seconds": 295.3988,
          "llm_latency_seconds": 19.0,
          "tool_latency_seconds": 10.0,
          "time_to_first_response_seconds": 14.356017152
        },
        "cache_efficiency": {
          "cache_hit_rate": 0.24826146873033447,
          "total_cached_tokens": 31166,
          "total_fresh_prompt_tokens": 94371,
          "total_input_tokens": 125537
        },
        "thinking_metrics": {
          "reasoning_ratio": 0.4229131393976707,
          "total_thinking_tokens": 8243,
          "total_candidate_tokens": 11248,
          "total_output_tokens": 19491,
          "turns_with_thinking": 9
        },
        "tool_utilization": {
          "total_tool_calls": 10,
          "unique_tools_used": 5,
          "tool_counts": {
            "IntakeAgent": 2,
            "transfer_to_agent": 2,
            "search_places": 2,
            "generate_html_report": 2,
            "generate_infographic": 2
          }
        },
        "tool_success_rate": {
          "tool_success_rate": 1.0,
          "total_tool_calls": 5,
          "failed_tool_calls": 0,
          "failed_tools_list": []
        },
        "grounding_utilization": {
          "total_grounding_chunks": 0,
          "total_grounded_responses": 0,
          "total_llm_responses": 11
        },
        "context_saturation": {
          "max_total_tokens": 33081,
          "peak_usage_span": "call_llm"
        },
        "agent_handoffs": {
          "total_handoffs": 5,
          "unique_agents_count": 3,
          "agents_invoked_list": [
            "IntakeAgent",
            "app",
            "transfer_to_agent"
          ]
        },
        "output_density": {
          "average_output_tokens": 1030.3636363636363,
          "total_output_tokens": 11334,
          "llm_calls_count": 11
        },
        "sandbox_usage": {
          "total_sandbox_ops": 0,
          "unique_ops_used": 0,
          "sandbox_tools_used": {}
        }
      },
      "llm_metrics": {
        "hallucinations_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "safety_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_tool_use_quality_v1": {
          "score": 0.5,
          "explanation": "Extracted from ADK simulation history."
        },
        "rubric_based_final_response_quality_v1": {
          "score": 1.0,
          "explanation": "Extracted from ADK simulation history."
        },
        "general_quality": {
          "score": 0.1,
          "rubric_verdicts": [
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The response is in English."
                  }
                },
                "type": "LANGUAGE:PRIMARY_RESPONSE_LANGUAGE",
                "importance": "HIGH"
              },
              "verdict": true,
              "reasoning": "The entire response is written in the English language."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The response provides an analysis of location viability for a coffee shop."
                  }
                },
                "type": "CORE_TASK:LOCATION_VIABILITY_ANALYSIS",
                "importance": "HIGH"
              },
              "reasoning": "The response states that an infographic was generated summarizing findings, but it does not present any analysis in the text itself."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis is specifically tailored to Seattle, WA."
                  }
                },
                "type": "GEOGRAPHIC_SCOPE:SEATTLE_WA",
                "importance": "HIGH"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) was tailored to Seattle, WA."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis considers factors related to market demand for coffee shops in Seattle."
                  }
                },
                "type": "ANALYSIS_FACTOR:MARKET_DEMAND",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) considered market demand."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis considers the competitive landscape, including existing coffee shops in Seattle."
                  }
                },
                "type": "ANALYSIS_FACTOR:COMPETITIVE_LANDSCAPE",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) considered the competitive landscape."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis includes demographic considerations relevant to a coffee shop's customer base (e.g., population density, age, income, local workforce)."
                  }
                },
                "type": "ANALYSIS_FACTOR:DEMOGRAPHICS",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) included demographic considerations."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis discusses factors related to foot traffic and accessibility of potential coffee shop locations."
                  }
                },
                "type": "ANALYSIS_FACTOR:FOOT_TRAFFIC_ACCESSIBILITY",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) discussed foot traffic and accessibility."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis addresses potential operational costs specific to Seattle (e.g., rent, labor costs, permits)."
                  }
                },
                "type": "ANALYSIS_FACTOR:OPERATIONAL_COSTS",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) addressed operational costs."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis identifies potential opportunities or challenges unique to opening a coffee shop in Seattle."
                  }
                },
                "type": "ANALYSIS_FACTOR:OPPORTUNITIES_CHALLENGES",
                "importance": "MEDIUM"
              },
              "reasoning": "The response does not provide any analysis, only stating that an infographic was generated. Therefore, it's impossible to verify if the analysis (which is not present) identified opportunities or challenges."
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "The analysis provides a reasoned conclusion or summary regarding the viability of opening a coffee shop in Seattle."
                  }
                },
                "type": "ANALYSIS_CONCLUSION:VIABILITY_STATEMENT",
                "importance": "HIGH"
              },
              "reasoning": "The response states that an infographic was generated summarizing findings, but it does not present any textual conclusion or summary in the text itself."
            }
          ],
          "input": {
            "request": {
              "contents": [
                {
                  "role": "user",
                  "parts": [
                    {
                      "text": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability."
                    }
                  ]
                },
                {
                  "role": "model",
                  "parts": [
                    {
                      "text": "Hello! I can certainly help you with that. I have the location as Seattle, WA, and the business type as a coffee shop. I will now process this information and begin the analysis."
                    }
                  ]
                }
              ]
            },
            "response": {
              "candidates": [
                {
                  "content": {
                    "role": "model",
                    "parts": [
                      {
                        "text": "I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis. The image file, named `infographic.png`, has been saved as an artifact and is available for you to view."
                      }
                    ]
                  }
                }
              ]
            }
          }
        },
        "tool_use_quality": {
          "score": 5.0,
          "explanation": "The agent made excellent use of its available tools. It correctly processed the user request with `IntakeAgent`, performed crucial competitor mapping with `search_places`, and generated comprehensive outputs via `generate_html_report` and `generate_infographic`. The `reportData` and `dataSummary` clearly indicate a thorough analysis was conducted, fulfilling the user's request for location viability. The `transfer_to_agent` call is a valid pipeline step. The agent adhered to the available tools and delivered a complete and well-structured response.",
          "input": {
            "prompt": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability.",
            "response": "I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis. The image file, named `infographic.png`, has been saved as an artifact and is available for you to view.",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"I want to open a coffee shop in Seattle, WA. Please analyze the location viability.\"}, \"call_id\": \"adk-663738ae-ae2b-416e-8806-ad8b8e24a302\", \"output_result\": {\"targetLocation\": \"Seattle, WA\", \"businessType\": \"coffee shop\", \"additionalContext\": \"Analyze the location viability.\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-03805ca4-94f1-4060-9564-718bad150cb7\", \"outpu... [truncated]",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}"
          }
        },
        "trajectory_accuracy": {
          "score": 5.0,
          "explanation": "The agent successfully used all relevant available tools (`IntakeAgent`, `search_places`, `generate_html_report`, `generate_infographic`) in a logical sequence to address the user's request. It performed intake, competitor mapping (a key research stage), and generated reports/infographics. The stages for market research (`google_search`) and gap analysis (`code execution`) were skipped because the necessary tools were not available, which is acceptable according to the critical evaluation rules (only evaluate against AVAILABLE tools).",
          "input": {
            "prompt": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability.",
            "response": "agent:app\nsub-agent:IntakeAgent\nsub-agent:transfer_to_agent\ntool:search_places\ntool:generate_html_report\ntool:generate_infographic",
            "available_tools": "{\"function_declarations\": [{\"name\": \"IntakeAgent\", \"description\": \"Tool: IntakeAgent\"}]}\n{\"function_declarations\": [{\"name\": \"generate_html_report\", \"description\": \"Tool: generate_html_report\"}]}\n{\"function_declarations\": [{\"name\": \"generate_infographic\", \"description\": \"Tool: generate_infographic\"}]}\n{\"function_declarations\": [{\"name\": \"search_places\", \"description\": \"Tool: search_places\"}]}\n{\"function_declarations\": [{\"name\": \"transfer_to_agent\", \"description\": \"Tool: transfer_to_agent\"}]}",
            "final_response": "I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis. The image file, named `infographic.png`, has been saved as an artifact and is available for you to view."
          }
        },
        "pipeline_integrity": {
          "score": 1.0,
          "explanation": "The agent hallucinates major parts of the analysis. It claims to have performed 'Macro-level market research' and 'quantitative gap analysis scoring 11 neighborhood zones' with specific metrics like 'Demand Signal score (4.75)' and 'Market Saturation Index (0.11)', and provides scores for recommended locations (e.g., Ballard Score: 90). However, the corresponding tools (`google_search` for market research and `code execution` for quantitative analysis/gap analysis) were not called. While competitor data was gathered via `search_places` and reports were generated, the core analytical content and numerical findings are fabricated.",
          "input": {
            "prompt": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability.",
            "response": "I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis. The image file, named `infographic.png`, has been saved as an artifact and is available for you to view.",
            "tool_interactions": "{\"tool_name\": \"IntakeAgent\", \"input_arguments\": {\"request\": \"I want to open a coffee shop in Seattle, WA. Please analyze the location viability.\"}, \"call_id\": \"adk-663738ae-ae2b-416e-8806-ad8b8e24a302\", \"output_result\": {\"targetLocation\": \"Seattle, WA\", \"businessType\": \"coffee shop\", \"additionalContext\": \"Analyze the location viability.\"}}\n{\"tool_name\": \"transfer_to_agent\", \"input_arguments\": {\"agentName\": \"LocationStrategyPipeline\"}, \"call_id\": \"adk-03805ca4-94f1-4060-9564-718bad150cb7\", \"outpu... [truncated]",
            "full_conversation": "{\"role\": \"user\", \"parts\": [{\"text\": \"I want to open a coffee shop in Seattle, WA. Please analyze the location viability.\"}]}"
          }
        },
        "text_quality": {
          "score": 0.3846154,
          "rubric_verdicts": [
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response provide a logically coherent and sound analysis of location viability for a coffee shop in Seattle?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response provide a logically coherent and sound analysis of location viability for a coffee shop in Seattle?\nSTEP 1: Does the response provide a logically coherent and sound analysis of location viability for a coffee shop in Seattle?\nSTEP 2:\n1. Check if the response contains any analysis of location viability.\n2. Check if the analysis, if present, is logically coherent and sound.\n3. Check if the analysis is specific to a coffee shop in Seattle.\nSTEP 3: The response states, \"I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis.\" It then mentions an image file. However, the response itself does not *provide* any analysis. It only states that an analysis has been generated elsewhere. Therefore, it does not provide a logically coherent and sound analysis within the text of the response.\nSTEP 4: The question asks if the *response* provides the analysis. The response only states that an analysis was generated, but does not present it.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response avoid presenting false premises or misleading information about Seattle's coffee shop market, demographics, or regulations?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Does the response avoid presenting false premises or misleading information about Seattle's coffee shop market, demographics, or regulations?\nSTEP 1: Does the response avoid presenting false premises or misleading information about Seattle's coffee shop market, demographics, or regulations?\nSTEP 2:\n1. Identify if the response makes any claims or statements about Seattle's coffee shop market, demographics, or regulations.\n2. If claims are made, evaluate if they are false or misleading.\nSTEP 3: The response does not present any information or claims about Seattle's coffee shop market, demographics, or regulations. It only states that an infographic has been generated. Therefore, it cannot present false premises or misleading information about these topics.\nSTEP 4: The response doesn't make any claims, so it can't make false ones.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Are any specific data points or factual claims about Seattle's market conditions or demographics presented in a manner that suggests they are verifiable?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Are any specific data points or factual claims about Seattle's market conditions or demographics presented in a manner that suggests they are verifiable?\nSTEP 1: Are any specific data points or factual claims about Seattle's market conditions or demographics presented in a manner that suggests they are verifiable?\nSTEP 2:\n1. Scan the response for any specific data points or factual claims related to Seattle's market conditions or demographics.\n2. If found, assess if they are presented in a way that implies verifiability (e.g., citing sources, mentioning specific metrics).\nSTEP 3: The response does not contain any specific data points or factual claims about Seattle's market conditions or demographics. It only mentions the generation of an infographic.\nSTEP 4: No data points or factual claims are present in the response.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the response structured in a clear, organized, and easy-to-read manner, appropriate for an analysis?"
                  }
                },
                "type": ""
              },
              "reasoning": "Is the response structured in a clear, organized, and easy-to-read manner, appropriate for an analysis?\nSTEP 1: Is the response structured in a clear, organized, and easy-to-read manner, appropriate for an analysis?\nSTEP 2:\n1. Evaluate the overall structure of the response.\n2. Determine if it is clear, organized, and easy to read.\n3. Assess if this structure is appropriate for an *analysis*.\nSTEP 3: The response consists of two sentences in a single paragraph. It is clear and easy to read in terms of its brevity. However, it does not *provide* an analysis, so the question of whether its structure is appropriate for an analysis is moot. If it were an analysis, this structure would be insufficient. Since it's just a statement about an artifact, it's clear.\nSTEP 4: The response is clear and easy to read, but it's not an analysis. The question asks if the structure is appropriate for an analysis. Since it's not an analysis, it doesn't have a structure *for* an analysis. However, if we interpret \"appropriate for an analysis\" as \"if it were an analysis, would this structure be appropriate?\", then the answer is no. But if we interpret it as \"is the response itself structured clearly?\", then yes. Given the context of the previous questions, it's asking about the structure *of the analysis*. Since there is no analysis, the structure cannot be appropriate for one.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response effectively use formatting elements (e.g., headings, bullet points, paragraphs) to enhance readability and organization?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response effectively use formatting elements (e.g., headings, bullet points, paragraphs) to enhance readability and organization?\nSTEP 1: Does the response effectively use formatting elements (e.g., headings, bullet points, paragraphs) to enhance readability and organization?\nSTEP 2:\n1. Identify any formatting elements used in the response (headings, bullet points, multiple paragraphs).\n2. Evaluate if these elements enhance readability and organization.\nSTEP 3: The response consists of a single paragraph with no headings or bullet points. It uses backticks for `infographic.png`, which is a minor formatting element, but it doesn't significantly enhance readability or organization in the context of an analysis. Given the brevity, it doesn't *need* extensive formatting, but it also doesn't *use* it effectively to enhance readability and organization for what would be expected of an analysis.\nSTEP 4: The response is too short to effectively use formatting elements. It doesn't use them.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the tone and language used professional and suitable for a business viability analysis?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Is the tone and language used professional and suitable for a business viability analysis?\nSTEP 1: Is the tone and language used professional and suitable for a business viability analysis?\nSTEP 2:\n1. Examine the tone of the response.\n2. Examine the language used in the response.\n3. Determine if both are professional and suitable for a business viability analysis.\nSTEP 3: The tone is neutral and informative (\"I have successfully generated...\", \"The image file... has been saved...\"). The language is straightforward and professional. While the response doesn't *perform* the analysis, the language used to describe the generation of the analysis is professional and suitable.\nSTEP 4: The tone and language are professional, even if the content isn't the analysis itself.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response identify and analyze key factors relevant to coffee shop location viability in Seattle (e.g., competition, target demographic, zoning/permits, operational costs, foot traffic, local trends)?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response identify and analyze key factors relevant to coffee shop location viability in Seattle (e.g., competition, target demographic, zoning/permits, operational costs, foot traffic, local trends)?\nSTEP 1: Does the response identify and analyze key factors relevant to coffee shop location viability in Seattle (e.g., competition, target demographic, zoning/permits, operational costs, foot traffic, local trends)?\nSTEP 2:\n1. Scan the response for any mention or discussion of key factors relevant to coffee shop location viability.\n2. Check if these factors are identified and analyzed.\nSTEP 3: The response only states that an infographic summarizing \"key findings of the location intelligence analysis\" has been generated. It does not identify or analyze any specific key factors within the text of the response itself.\nSTEP 4: The response does not identify or analyze any factors.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response provide sufficient depth and detail for each key factor analyzed to be useful for informing a business decision?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response provide sufficient depth and detail for each key factor analyzed to be useful for informing a business decision?\nSTEP 1: Does the response provide sufficient depth and detail for each key factor analyzed to be useful for informing a business decision?\nSTEP 2:\n1. Determine if any key factors are analyzed in the response.\n2. If factors are analyzed, assess if the depth and detail are sufficient for a business decision.\nSTEP 3: As established in the previous question, the response does not analyze any key factors. Therefore, it cannot provide sufficient depth and detail for them.\nSTEP 4: No factors are analyzed, so no depth or detail is provided.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the analysis cover both potential opportunities and challenges or risks specific to opening a coffee shop in Seattle?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the analysis cover both potential opportunities and challenges or risks specific to opening a coffee shop in Seattle?\nSTEP 1: Does the analysis cover both potential opportunities and challenges or risks specific to opening a coffee shop in Seattle?\nSTEP 2:\n1. Determine if the response provides an analysis.\n2. If an analysis is provided, check if it covers opportunities and challenges/risks specific to Seattle.\nSTEP 3: The response does not provide an analysis. It only states that an infographic has been generated. Therefore, it does not cover opportunities or challenges/risks.\nSTEP 4: No analysis is present in the response.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the entire content of the response directly relevant to analyzing the location viability of a coffee shop in Seattle, WA?"
                  }
                },
                "type": ""
              },
              "reasoning": "Is the entire content of the response directly relevant to analyzing the location viability of a coffee shop in Seattle, WA?\nSTEP 1: Is the entire content of the response directly relevant to analyzing the location viability of a coffee shop in Seattle, WA?\nSTEP 2:\n1. Review the entire content of the response.\n2. Determine if each part of the content directly contributes to *analyzing* location viability for a coffee shop in Seattle.\nSTEP 3: The response states that an infographic summarizing \"key findings of the location intelligence analysis\" has been generated. While this infographic *would* be relevant, the response itself is merely a statement about the generation of an artifact, not an analysis itself. Therefore, the content of the response is relevant to *informing* the user that an analysis *exists*, but it is not the analysis itself. The question asks if the content is relevant to *analyzing* the viability. The response does not perform an analysis.\nSTEP 4: The response is about an analysis, but it is not the analysis itself. Therefore, its content is not directly relevant to *performing* the analysis.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response avoid including any irrelevant or extraneous information that does not contribute to the analysis?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Does the response avoid including any irrelevant or extraneous information that does not contribute to the analysis?\nSTEP 1: Does the response avoid including any irrelevant or extraneous information that does not contribute to the analysis?\nSTEP 2:\n1. Identify all information presented in the response.\n2. Determine if any of this information is irrelevant or extraneous to the *analysis* (which is not present in the response).\nSTEP 3: The response only states that an infographic has been generated. This information is not irrelevant to the user's request, as it indicates a step taken towards fulfilling it. However, since the response itself does not *contain* an analysis, the information about the infographic's generation doesn't \"contribute to the analysis\" within the response. It's not extraneous in the sense of being off-topic, but it doesn't contribute to the *textual analysis* because there isn't one. If the question implies \"does it avoid information that doesn't contribute to the *user's goal of getting an analysis*\", then the answer is yes, as it points to where the analysis is. If it means \"does it avoid information that doesn't contribute to the *analysis presented in the response*\", then it's a tricky question because no analysis is presented. Given the context of \"contribute to the analysis\", and the fact that the response *is not* the analysis, the information about the infographic is not *part of* the analysis.\nSTEP 4: The response is about the generation of an analysis, not the analysis itself. The information provided is relevant to the user's request, but it doesn't *contribute to an analysis within the response* because there is no analysis in the response.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Does the response explicitly perform an analysis of location viability, rather than merely listing facts or general information?"
                  }
                },
                "type": ""
              },
              "reasoning": "Does the response explicitly perform an analysis of location viability, rather than merely listing facts or general information?\nSTEP 1: Does the response explicitly perform an analysis of location viability, rather than merely listing facts or general information?\nSTEP 2:\n1. Check if the response contains an analysis of location viability.\n2. Verify that it's an analysis, not just a list of facts or general information.\nSTEP 3: The response states, \"I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis.\" This is a statement about an action taken, not an analysis itself. It does not perform an analysis of location viability.\nSTEP 4: The response explicitly states that an analysis *was generated* as an infographic, but the response itself does not *perform* the analysis.\nSTEP 5:"
            },
            {
              "evaluated_rubric": {
                "content": {
                  "property": {
                    "description": "Is the response free of harmful, unethical, or inappropriate content?"
                  }
                },
                "type": ""
              },
              "verdict": true,
              "reasoning": "Is the response free of harmful, unethical, or inappropriate content?\nSTEP 1: Is the response free of harmful, unethical, or inappropriate content?\nSTEP 2:\n1. Read through the entire response.\n2. Identify any content that could be considered harmful, unethical, or inappropriate.\nSTEP 3: The response is a straightforward statement about generating an infographic. There is no content that could be considered harmful, unethical, or inappropriate.\nSTEP 4: The response is benign.\nSTEP 5:"
            }
          ],
          "input": {
            "request": {
              "contents": [
                {
                  "role": "user",
                  "parts": [
                    {
                      "text": "I want to open a coffee shop in Seattle, WA. Please analyze the location viability."
                    }
                  ]
                },
                {
                  "role": "model",
                  "parts": [
                    {
                      "text": "Hello! I can certainly help you with that. I have the location as Seattle, WA, and the business type as a coffee shop. I will now process this information and begin the analysis."
                    }
                  ]
                }
              ]
            },
            "response": {
              "candidates": [
                {
                  "content": {
                    "role": "model",
                    "parts": [
                      {
                        "text": "I have successfully generated a visual infographic summarizing the key findings of the location intelligence analysis. The image file, named `infographic.png`, has been saved as an artifact and is available for you to view."
                      }
                    ]
                  }
                }
              ]
            }
          }
        }
      }
    }
  ]
}
```

*   **Detailed Explanations:** Raw, detailed explanations from the LLM judge for each metric on a per-question basis. Use this to find patterns in *why* a metric scored high or low.
**Detailed Explanations per Metric:**
--- Evaluation Analysis ---

## Metric: `token_usage.llm_calls`
**Average Score:** 11.0000

## Metric: `token_usage.total_tokens`
**Average Score:** 62085.0000

## Metric: `token_usage.prompt_tokens`
**Average Score:** 94371.0000

## Metric: `token_usage.completion_tokens`
**Average Score:** 11334.0000

## Metric: `token_usage.cached_tokens`
**Average Score:** 31166.0000

## Metric: `token_usage.estimated_cost_usd`
**Average Score:** 0.0875

## Metric: `latency_metrics.total_latency_seconds`
**Average Score:** 295.3988

## Metric: `latency_metrics.average_turn_latency_seconds`
**Average Score:** 295.3988

## Metric: `latency_metrics.llm_latency_seconds`
**Average Score:** 19.0000

## Metric: `latency_metrics.tool_latency_seconds`
**Average Score:** 10.0000

## Metric: `latency_metrics.time_to_first_response_seconds`
**Average Score:** 14.3560

## Metric: `cache_efficiency.cache_hit_rate`
**Average Score:** 0.2483

## Metric: `cache_efficiency.total_cached_tokens`
**Average Score:** 31166.0000

## Metric: `cache_efficiency.total_fresh_prompt_tokens`
**Average Score:** 94371.0000

## Metric: `cache_efficiency.total_input_tokens`
**Average Score:** 125537.0000

## Metric: `thinking_metrics.reasoning_ratio`
**Average Score:** 0.4229

## Metric: `thinking_metrics.total_thinking_tokens`
**Average Score:** 8243.0000

## Metric: `thinking_metrics.total_candidate_tokens`
**Average Score:** 11248.0000

## Metric: `thinking_metrics.total_output_tokens`
**Average Score:** 19491.0000

## Metric: `thinking_metrics.turns_with_thinking`
**Average Score:** 9.0000

## Metric: `tool_utilization.total_tool_calls`
**Average Score:** 10.0000

## Metric: `tool_utilization.unique_tools_used`
**Average Score:** 5.0000

## Metric: `tool_success_rate.tool_success_rate`
**Average Score:** 1.0000

## Metric: `tool_success_rate.total_tool_calls`
**Average Score:** 5.0000

## Metric: `tool_success_rate.failed_tool_calls`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounding_chunks`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_grounded_responses`
**Average Score:** 0.0000

## Metric: `grounding_utilization.total_llm_responses`
**Average Score:** 11.0000

## Metric: `context_saturation.max_total_tokens`
**Average Score:** 33081.0000

## Metric: `agent_handoffs.total_handoffs`
**Average Score:** 5.0000

## Metric: `agent_handoffs.unique_agents_count`
**Average Score:** 3.0000

## Metric: `output_density.average_output_tokens`
**Average Score:** 1030.3636

## Metric: `output_density.total_output_tokens`
**Average Score:** 11334.0000

## Metric: `output_density.llm_calls_count`
**Average Score:** 11.0000

## Metric: `sandbox_usage.total_sandbox_ops`
**Average Score:** 0.0000

## Metric: `sandbox_usage.unique_ops_used`
**Average Score:** 0.0000

## Metric: `hallucinations_v1`
**Average Score:** {'average': 1.0}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.

## Metric: `safety_v1`
**Average Score:** {'average': 1.0}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.

## Metric: `rubric_based_tool_use_quality_v1`
**Average Score:** {'average': 0.5}
**Sample Explanations:**
- [Score: 0.5] Extracted from ADK simulation history.

## Metric: `rubric_based_final_response_quality_v1`
**Average Score:** {'average': 1.0}
**Sample Explanations:**
- [Score: 1.0] Extracted from ADK simulation history.

## Metric: `general_quality`
**Average Score:** {'average': 0.1, 'score_range': {'min': 0, 'max': 1, 'description': 'Passing rate for general quality rubrics'}}

## Metric: `tool_use_quality`
**Average Score:** {'average': 5.0, 'score_range': {'min': 0, 'max': 5, 'description': '0=Poor tool usage, 5=Excellent tool usage'}}
**Sample Explanations:**
- [Score: 5.0] The agent made excellent use of its available tools. It correctly processed the user request with `IntakeAgent`, performed crucial competitor mapping with `search_places`, and generated comprehensive outputs via `generate_html_report` and `generate_infographic`. The `reportData` and `dataSummary` clearly indicate a thorough analysis was conducted, fulfilling the user's request for location viability. The `transfer_to_agent` call is a valid pipeline step. The agent adhered to the available tools and delivered a complete and well-structured response.

## Metric: `trajectory_accuracy`
**Average Score:** {'average': 5.0, 'score_range': {'min': 0, 'max': 5, 'description': '0=Completely wrong, 5=Perfect trajectory'}}
**Sample Explanations:**
- [Score: 5.0] The agent successfully used all relevant available tools (`IntakeAgent`, `search_places`, `generate_html_report`, `generate_infographic`) in a logical sequence to address the user's request. It performed intake, competitor mapping (a key research stage), and generated reports/infographics. The stages for market research (`google_search`) and gap analysis (`code execution`) were skipped because the necessary tools were not available, which is acceptable according to the critical evaluation rules (only evaluate against AVAILABLE tools).

## Metric: `pipeline_integrity`
**Average Score:** {'average': 1.0, 'score_range': {'min': 0, 'max': 5, 'description': '0=Major integrity issues, 5=Perfect integrity'}}
**Sample Explanations:**
- [Score: 1.0] The agent hallucinates major parts of the analysis. It claims to have performed 'Macro-level market research' and 'quantitative gap analysis scoring 11 neighborhood zones' with specific metrics like 'Demand Signal score (4.75)' and 'Market Saturation Index (0.11)', and provides scores for recommended locations (e.g., Ballard Score: 90). However, the corresponding tools (`google_search` for market research and `code execution` for quantitative analysis/gap analysis) were not called. While competitor data was gathered via `search_places` and reports were generated, the core analytical content and numerical findings are fabricated.

## Metric: `text_quality`
**Average Score:** {'average': 0.3846154, 'score_range': {'min': 0, 'max': 1, 'description': 'Passing rate for text quality rubrics'}}


**2. Metric Calculation & Definitions:**
*   **Metric Definitions:** The rubrics and descriptions for each metric. You MUST use these files to understand what each metric is actually measuring and whether it is `llm` judged or `deterministic`.
**File: `../retail-ai-location-strategy/eval/results/baseline/raw/temp_consolidated_metrics.json`**
```json
Error: File '../retail-ai-location-strategy/eval/results/baseline/raw/temp_consolidated_metrics.json' not found.
```

*   **Deterministic Logic:** The Python code that calculates the deterministic metrics. Refer to this file to understand the precise logic behind scores for metrics like `token_usage` or `latency_metrics`.
**File: `evaluation/core/deterministic_metrics.py`**
```python
"""
Deterministic metrics for evaluating agent execution success.

These metrics provide objective pass/fail measurements by analyzing trace data
and session state, without requiring LLM-as-judge evaluation.
"""

import json
from typing import Any, Dict, List, Tuple

# Pricing per 1K tokens (approximate list prices for prompts <= 200k tokens)
# Format: {model_name: (prompt_price, completion_price)}
# Source: https://ai.google.dev/gemini-api/docs/pricing
MODEL_PRICING = {
    # Gemini 3 (Latest Preview)
    "gemini-3-pro-preview": (0.002, 0.012),  # $2.00 / $12.00 per 1M
    "gemini-3-flash-preview": (0.0005, 0.003),  # $0.50 / $3.00 per 1M
    # Gemini 2.5 (Current Flagship)
    "gemini-2.5-pro": (0.00125, 0.01),  # $1.25 / $10.00 per 1M
    "gemini-2.5-flash": (0.0003, 0.0025),  # $0.30 / $2.50 per 1M
    # Gemini 2.0
    "gemini-2.0-flash": (0.0001, 0.0004),  # $0.10 / $0.40 per 1M
    "gemini-2.0-flash-exp": (0.0001, 0.0004),  # Same as 2.0 flash
    "gemini-2.0-flash-lite": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    # Gemini 1.5 (Updated/Reduced Prices)
    "gemini-1.5-pro": (0.00125, 0.01),  # Reduced from 0.0035/0.0105
    "gemini-1.5-pro-001": (0.00125, 0.01),
    "gemini-1.5-flash": (0.000075, 0.0003),  # $0.075 / $0.30 per 1M
    "gemini-1.5-flash-001": (0.000075, 0.0003),
    # Legacy
    "gemini-1.0-pro": (0.0005, 0.0015),  # $0.50 / $1.50 per 1M
    "default": (0.0001, 0.0004),  # Fallback to 2.0 Flash
}


def calculate_token_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Informational metric: Track token usage and estimated cost based on the specific model used.
    """
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_cached_tokens = 0
    total_tokens = 0
    llm_calls = 0
    total_cost = 0.0
    models_used = set()

    if not session_trace:
        return 0.0, "No trace data available for token usage calculation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Identify model (handle None values)
        model_name = attributes.get("gen_ai.request.model") or "default"
        model_name = model_name.lower() if isinstance(model_name, str) else "default"

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                if usage:
                    llm_calls += 1
                    models_used.add(model_name)

                    p_tokens = usage.get("prompt_token_count", 0)
                    c_tokens = usage.get("candidates_token_count", 0)
                    ch_tokens = usage.get("cached_content_token_count", 0)
                    t_tokens = usage.get("total_token_count", 0)

                    total_prompt_tokens += p_tokens
                    total_completion_tokens += c_tokens
                    total_cached_tokens += ch_tokens
                    total_tokens += t_tokens

                    # Match model pricing
                    pricing = MODEL_PRICING["default"]
                    for known_model, prices in MODEL_PRICING.items():
                        if known_model in model_name:
                            pricing = prices
                            break

                    # Cost calculation (simplified: ignoring cache discount for now to keep it safe upper bound,
                    # or strictly following list price for active tokens)
                    call_cost = (p_tokens / 1000 * pricing[0]) + (
                        c_tokens / 1000 * pricing[1]
                    )
                    # Note: Cached tokens usually have a separate (lower) pricing tier.
                    # For this metric, we currently only sum cost for active prompt/completion tokens.

                    total_cost += call_cost

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Usage: {llm_calls} LLM calls using {list(models_used)}. "
        f"Tokens: {total_tokens} ({total_prompt_tokens}p + {total_completion_tokens}c + {total_cached_tokens}ch). "
        f"Cost: ${total_cost:.6f}"
    )

    details = {
        "llm_calls": llm_calls,
        "models_used": list(models_used),
        "total_tokens": total_tokens,
        "prompt_tokens": total_prompt_tokens,
        "completion_tokens": total_completion_tokens,
        "cached_tokens": total_cached_tokens,
        "estimated_cost_usd": total_cost,
    }

    return total_cost, explanation, details


def calculate_latency_metrics(
    session_trace: List[Dict[str, Any]], latency_data: List[Dict[str, Any]] = None
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate latency metrics from the session trace.
    Returns the total latency score (seconds), but details contains granular breakdown.
    """
    total_latency = 0.0
    llm_latency = 0.0
    tool_latency = 0.0
    first_response_latency = None
    average_turn_latency = 0.0

    if not session_trace:
        return 0.0, "No trace data available for latency calculation", {}

    # Sort spans by start time to find the true beginning
    sorted_spans = sorted(
        [s for s in session_trace if s.get("start_time")], key=lambda x: x["start_time"]
    )

    if not sorted_spans:
        return 0.0, "Trace data has no timestamps", {}

    root_start = sorted_spans[0]["start_time"]

    # Calculate Component Latencies from full trace
    max_end = 0
    for span in session_trace:
        start = span.get("start_time", 0)
        end = span.get("end_time", 0)
        max_end = max(max_end, end)
        duration = (end - start) / 1e9
        name = span.get("name", "")

        if name == "call_llm":
            llm_latency += duration
            # Proxy for Time to First Token: end of first LLM call
            if first_response_latency is None:
                first_response_latency = (end - root_start) / 1e9

        elif "tool_call" in name or "execute_tool" in name:
            tool_latency += duration

    # Calculate Total & Average Latency from high-level summary (latency_data)
    # This is preferred as it excludes user think time in multi-turn sessions.
    if latency_data:
        turn_latencies = []
        for item in latency_data:
            if item.get("name") == "invocation":
                turn_latencies.append(item.get("duration_seconds", 0))

        if turn_latencies:
            average_turn_latency = sum(turn_latencies) / len(turn_latencies)
            total_latency = sum(turn_latencies)

    # Fallback: Wall-clock duration from trace if latency_data is missing
    if total_latency == 0.0 and max_end > root_start:
        total_latency = (max_end - root_start) / 1e9  # nanoseconds to seconds

    explanation = (
        f"Total: {total_latency:.4f}s. "
        f"Avg Turn: {average_turn_latency:.4f}s. "
        f"LLM: {llm_latency:.4f}s, Tools: {tool_latency:.4f}s. "
        f"First Response: {first_response_latency if first_response_latency else 0:.4f}s"
    )

    details = {
        "total_latency_seconds": total_latency,
        "average_turn_latency_seconds": average_turn_latency,
        "llm_latency_seconds": llm_latency,
        "tool_latency_seconds": tool_latency,
        "time_to_first_response_seconds": first_response_latency,
    }

    return total_latency, explanation, details


def calculate_cache_efficiency(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the efficiency of context caching.
    Returns the cache hit rate (percentage of potential prompt tokens that were cached).
    """
    total_prompt_tokens = 0
    total_cached_tokens = 0

    if not session_trace:
        return 0.0, "No trace data available for cache efficiency", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total_prompt_tokens += usage.get("prompt_token_count", 0)
                    total_cached_tokens += usage.get("cached_content_token_count", 0)
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    # Calculate hit rate
    # Note: 'prompt_token_count' in Gemini API usage metadata usually EXCLUDES cached tokens.
    # So total potential input = prompt_token_count + cached_content_token_count
    total_input_tokens = total_prompt_tokens + total_cached_tokens

    if total_input_tokens > 0:
        cache_hit_rate = total_cached_tokens / total_input_tokens
    else:
        cache_hit_rate = 0.0

    explanation = (
        f"Cache Hit Rate: {cache_hit_rate:.2%}. "
        f"Cached Tokens: {total_cached_tokens}. "
        f"Fresh Prompt Tokens: {total_prompt_tokens}."
    )

    details = {
        "cache_hit_rate": cache_hit_rate,
        "total_cached_tokens": total_cached_tokens,
        "total_fresh_prompt_tokens": total_prompt_tokens,
        "total_input_tokens": total_input_tokens,
    }

    return cache_hit_rate, explanation, details


def calculate_thinking_metrics(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate metrics related to the model's 'thinking' or reasoning process.
    Returns the reasoning ratio (thinking tokens / total output tokens).
    """
    total_thinking_tokens = 0
    total_candidate_tokens = 0
    turns_with_thinking = 0

    if not session_trace:
        return 0.0, "No trace data available for thinking metrics", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    thoughts = usage.get("thoughts_token_count", 0)
                    # Note: In some API versions, candidates_token_count might exclude thoughts.
                    # We treat them as additive components of the total output.
                    candidates = usage.get("candidates_token_count", 0)

                    total_thinking_tokens += thoughts
                    total_candidate_tokens += candidates

                    if thoughts > 0:
                        turns_with_thinking += 1
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    total_output_tokens = total_thinking_tokens + total_candidate_tokens

    if total_output_tokens > 0:
        reasoning_ratio = total_thinking_tokens / total_output_tokens
    else:
        reasoning_ratio = 0.0

    explanation = (
        f"Reasoning Ratio: {reasoning_ratio:.2%}. "
        f"Thinking Tokens: {total_thinking_tokens}. "
        f"Standard Output Tokens: {total_candidate_tokens}. "
        f"Turns with Thinking: {turns_with_thinking}."
    )

    details = {
        "reasoning_ratio": reasoning_ratio,
        "total_thinking_tokens": total_thinking_tokens,
        "total_candidate_tokens": total_candidate_tokens,
        "total_output_tokens": total_output_tokens,
        "turns_with_thinking": turns_with_thinking,
    }

    return reasoning_ratio, explanation, details


def calculate_tool_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate statistics on tool usage frequency and diversity.
    Returns the total number of tool calls.
    """
    total_tool_calls = 0
    tool_counts = {}

    if not session_trace:
        return 0.0, "No trace data available for tool utilization", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans.
        # Standard ADK traces often use "execute_tool <ToolName>"
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            total_tool_calls += 1
            tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1

    unique_tools_used = len(tool_counts)

    # Create a string representation of the tool breakdown
    breakdown_str = ", ".join([f"{k}: {v}" for k, v in tool_counts.items()])

    explanation = (
        f"Total Tool Calls: {total_tool_calls}. "
        f"Unique Tools: {unique_tools_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_tool_calls": total_tool_calls,
        "unique_tools_used": unique_tools_used,
        "tool_counts": tool_counts,
    }

    return float(total_tool_calls), explanation, details


def calculate_tool_success_rate(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the success rate of tool executions by inspecting tool responses.
    Returns success rate (successful / total) as score.
    """
    total_calls = 0
    failed_calls = 0
    failed_tools = []

    if not session_trace:
        return 0.0, "No trace data available for tool success rate", {}

    for span in session_trace:
        name = span.get("name", "")
        attributes = span.get("attributes", {})

        # Identify tool execution spans
        is_tool = name.startswith("execute_tool ") or "tool_call" in name

        if is_tool:
            tool_response_str = attributes.get("gcp.vertex.agent.tool_response")
            if tool_response_str:
                total_calls += 1
                try:
                    # Parse the JSON response to check status
                    response = json.loads(tool_response_str)

                    # Common error patterns in ADK/JSON tools
                    is_error = False
                    if isinstance(response, dict):
                        if response.get("status") == "error":
                            is_error = True
                        elif "error" in response or "error_message" in response:
                            is_error = True

                    if is_error:
                        failed_calls += 1
                        tool_name = name.replace("execute_tool ", "").strip()
                        failed_tools.append(tool_name)

                except (json.JSONDecodeError, TypeError):
                    # Malformed JSON in response could be considered a failure or ignored
                    pass

    if total_calls > 0:
        success_rate = (total_calls - failed_calls) / total_calls
    else:
        # If no tools were called, success rate is technically N/A, but 1.0 is a safe "no errors" default
        # Or 0.0 if we want to imply "no success possible".
        # For evaluation, 1.0 (no failures) usually makes more sense if no tools were attempted.
        # But to distinguish from "perfect execution", let's return 1.0 but note it.
        success_rate = 1.0

    explanation = (
        f"Success Rate: {success_rate:.2%}. "
        f"Total Calls: {total_calls}. "
        f"Failed Calls: {failed_calls}. "
        f"Failed Tools: {list(set(failed_tools))}"
    )

    details = {
        "tool_success_rate": success_rate,
        "total_tool_calls": total_calls,
        "failed_tool_calls": failed_calls,
        "failed_tools_list": failed_tools,
    }

    return success_rate, explanation, details


def calculate_grounding_utilization(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the extent of grounding usage by inspecting LLM responses for groundingMetadata.
    Returns total grounding chunks (citations) as the score.
    """
    total_grounded_responses = 0
    total_grounding_chunks = 0
    total_llm_responses = 0

    if not session_trace:
        return 0.0, "No trace data available for grounding utilization", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")

        if llm_response:
            total_llm_responses += 1
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                # Grounding metadata is usually at the top level or inside candidates
                # Standard Vertex AI response structure check
                grounding_metadata = response_data.get(
                    "groundingMetadata"
                ) or response_data.get("grounding_metadata")

                if not grounding_metadata:
                    # Check inside candidates if not at top level
                    candidates = response_data.get("candidates", [])
                    if candidates and isinstance(candidates, list):
                        first_candidate = candidates[0]
                        grounding_metadata = first_candidate.get(
                            "groundingMetadata"
                        ) or first_candidate.get("grounding_metadata")

                if grounding_metadata:
                    chunks = grounding_metadata.get(
                        "groundingChunks"
                    ) or grounding_metadata.get("grounding_chunks")
                    if chunks and isinstance(chunks, list) and len(chunks) > 0:
                        total_grounded_responses += 1
                        total_grounding_chunks += len(chunks)

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Total Citations (Chunks): {total_grounding_chunks}. "
        f"Grounded Responses: {total_grounded_responses} / {total_llm_responses}."
    )

    details = {
        "total_grounding_chunks": total_grounding_chunks,
        "total_grounded_responses": total_grounded_responses,
        "total_llm_responses": total_llm_responses,
    }

    return float(total_grounding_chunks), explanation, details


def calculate_context_saturation(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the maximum context saturation (max total tokens used in a single turn).
    Returns max_tokens as the score.
    """
    max_tokens = 0
    max_token_span = ""

    if not session_trace:
        return 0.0, "No trace data available for context saturation", {}

    for span in session_trace:
        attributes = span.get("attributes", {})
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})
                if usage:
                    total = usage.get("total_token_count", 0)
                    if total > max_tokens:
                        max_tokens = total
                        max_token_span = span.get("name", "unknown")
            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    explanation = (
        f"Max Context Used: {max_tokens} tokens. Peak occurred in: {max_token_span}."
    )

    details = {"max_total_tokens": max_tokens, "peak_usage_span": max_token_span}

    return float(max_tokens), explanation, details


def calculate_agent_handoffs(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of agent handoffs/invocations in the session.
    Returns total handoff events as the score.

    Captures:
    - Direct agent invocations (invoke_agent, agent_run)
    - Sub-agents called as tools (execute_tool *Agent, transfer_to_agent)
    """
    handoff_count = 0
    agents_invoked = set()

    if not session_trace:
        return 0.0, "No trace data available for agent handoffs", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for direct agent invocations
        if name.startswith("invoke_agent ") or name.startswith("agent_run "):
            agent_name = (
                name.replace("invoke_agent ", "").replace("agent_run ", "").strip()
            )
            handoff_count += 1
            agents_invoked.add(agent_name)

        # Check for sub-agents called as tools (e.g., "execute_tool IntakeAgent")
        elif name.startswith("execute_tool "):
            tool_name = name.replace("execute_tool ", "").strip()
            # Sub-agents typically end with "Agent" or are transfer_to_agent
            if tool_name.endswith("Agent") or tool_name == "transfer_to_agent":
                handoff_count += 1
                agents_invoked.add(tool_name)

    explanation = (
        f"Total Handoffs: {handoff_count}. "
        f"Unique Agents: {len(agents_invoked)}. "
        f"Agents: {list(agents_invoked)}"
    )

    details = {
        "total_handoffs": handoff_count,
        "unique_agents_count": len(agents_invoked),
        "agents_invoked_list": list(agents_invoked),
    }

    return float(handoff_count), explanation, details


def calculate_output_density(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Calculate the average number of output tokens per LLM call.
    Returns average output tokens as the score.
    """
    total_output_tokens = 0
    llm_calls = 0

    if not session_trace:
        return 0.0, "No trace data available for output density", {}

    for span in session_trace:
        attributes = span.get("attributes", {})

        # Check for LLM response with usage metadata
        llm_response = attributes.get("gcp.vertex.agent.llm_response")
        if llm_response:
            try:
                response_data = (
                    json.loads(llm_response)
                    if isinstance(llm_response, str)
                    else llm_response
                )
                usage = response_data.get("usage_metadata", {})

                # Check for output tokens in standard fields (candidates_token_count or output_token_count)
                output_tokens = 0
                if usage:
                    # 'candidates_token_count' is standard in Vertex AI
                    output_tokens = usage.get("candidates_token_count", 0)
                    if output_tokens == 0:
                        # Fallback for other providers
                        output_tokens = usage.get("output_token_count", 0) or usage.get(
                            "completion_tokens", 0
                        )

                if (
                    output_tokens > 0 or usage
                ):  # Count the call even if 0 output (edge case)
                    llm_calls += 1
                    total_output_tokens += output_tokens

            except (json.JSONDecodeError, TypeError, AttributeError):
                continue

    if llm_calls > 0:
        average_output_tokens = total_output_tokens / llm_calls
    else:
        average_output_tokens = 0.0

    explanation = (
        f"Avg Output Tokens: {average_output_tokens:.2f}. "
        f"Total Output Tokens: {total_output_tokens}. "
        f"LLM Calls: {llm_calls}."
    )

    details = {
        "average_output_tokens": average_output_tokens,
        "total_output_tokens": total_output_tokens,
        "llm_calls_count": llm_calls,
    }

    return float(average_output_tokens), explanation, details


def calculate_sandbox_usage(
    session_trace: List[Dict[str, Any]],
) -> Tuple[float, str, Dict[str, Any]]:
    """
    Count the number of tool calls related to sandbox/file system operations.
    Returns the total count as the score.
    """
    sandbox_ops_count = 0
    sandbox_tools_used = {}

    # Common keywords for sandbox/file operations
    sandbox_keywords = [
        "save_artifact",
        "load_artifact",
        "read_file",
        "write_file",
        "run_python_script",
        "execute_code",
        "save_to_file",
        "read_from_file",
    ]

    if not session_trace:
        return 0.0, "No trace data available for sandbox usage", {}

    for span in session_trace:
        name = span.get("name", "")

        # Check for tool execution spans
        if name.startswith("execute_tool ") or "tool_call" in name:
            tool_name = "unknown"
            if name.startswith("execute_tool "):
                tool_name = name.replace("execute_tool ", "").strip()
            elif "tool.name" in span.get("attributes", {}):
                tool_name = span["attributes"]["gen_ai.tool.name"]

            # Check if tool matches sandbox keywords
            if any(keyword in tool_name.lower() for keyword in sandbox_keywords):
                sandbox_ops_count += 1
                sandbox_tools_used[tool_name] = sandbox_tools_used.get(tool_name, 0) + 1

    unique_ops_used = len(sandbox_tools_used)

    breakdown_str = ", ".join([f"{k}: {v}" for k, v in sandbox_tools_used.items()])

    explanation = (
        f"Total Sandbox Ops: {sandbox_ops_count}. "
        f"Unique Ops: {unique_ops_used}. "
        f"Breakdown: [{breakdown_str}]"
    )

    details = {
        "total_sandbox_ops": sandbox_ops_count,
        "unique_ops_used": unique_ops_used,
        "sandbox_tools_used": sandbox_tools_used,
    }

    return float(sandbox_ops_count), explanation, details


# Registry of all deterministic metrics
DETERMINISTIC_METRICS = {
    "token_usage": calculate_token_usage,
    "latency_metrics": calculate_latency_metrics,
    "cache_efficiency": calculate_cache_efficiency,
    "thinking_metrics": calculate_thinking_metrics,
    "tool_utilization": calculate_tool_utilization,
    "tool_success_rate": calculate_tool_success_rate,
    "grounding_utilization": calculate_grounding_utilization,
    "context_saturation": calculate_context_saturation,
    "agent_handoffs": calculate_agent_handoffs,
    "output_density": calculate_output_density,
    "sandbox_usage": calculate_sandbox_usage,
}


def evaluate_deterministic_metrics(
    session_state: Dict[str, Any],
    session_trace: List[Dict[str, Any]],
    agents_evaluated: List[str],
    question_metadata: Dict[str, Any],
    metrics_to_run: List[str] = None,
    reference_data: Dict[str, Any] = None,
    metric_definitions: Dict[str, Any] = None,
    latency_data: List[Dict[str, Any]] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate all specified deterministic metrics.
    """
    if metrics_to_run is None:
        metrics_to_run = list(DETERMINISTIC_METRICS.keys())

    results = {}
    for metric_name in metrics_to_run:
        if metric_name not in DETERMINISTIC_METRICS:
            continue

        metric_func = DETERMINISTIC_METRICS[metric_name]

        try:
            if metric_name == "latency_metrics":
                score, explanation, details = metric_func(
                    session_trace, latency_data=latency_data
                )
            else:
                score, explanation, details = metric_func(session_trace)

            results[metric_name] = {
                "score": score,
                "explanation": explanation,
                "details": details,
            }
        except Exception as e:
            results[metric_name] = {
                "score": 0.0,
                "explanation": f"Error evaluating metric {metric_name}: {str(e)}",
            }

    return results

```

**3. Agent Implementation Details:**
*   **Agent Source Code:** The source code for the agent being evaluated. This is your primary source for forming hypotheses about *why* the agent behaves a certain way.
**File: `../retail-ai-location-strategy/app/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Retail Location Strategy Agent - Root Agent Definition.

This module defines the root agent for the Location Strategy Pipeline.
It uses a SequentialAgent to orchestrate 6 specialized sub-agents:

1. MarketResearchAgent - Live web research with Google Search
2. CompetitorMappingAgent - Competitor mapping with Maps Places API
3. GapAnalysisAgent - Quantitative analysis with Python code execution
4. StrategyAdvisorAgent - Strategic synthesis with extended reasoning
5. ReportGeneratorAgent - HTML executive report generation
6. InfographicGeneratorAgent - Visual infographic generation

The pipeline analyzes a target location for a specific business type and
produces comprehensive location intelligence including recommendations,
an HTML report, and an infographic.

Authentication:
    Uses Google AI Studio (API key) instead of Vertex AI.
    Set environment variables:
        GOOGLE_API_KEY=your_api_key
        GOOGLE_GENAI_USE_VERTEXAI=FALSE
        MAPS_API_KEY=your_maps_api_key

Usage:
    Run with: adk web retail_ai_location_strategy_adk

    The agent expects initial state variables:
    - target_location: The geographic area to analyze (e.g., "Bangalore, India")
    - business_type: Type of business to open (e.g., "coffee shop")

    Optional state variables:
    - maps_api_key: Google Maps API key for Places search
"""

from google.adk.agents import SequentialAgent
from google.adk.agents.llm_agent import Agent
from google.adk.tools.agent_tool import AgentTool


from .sub_agents.intake_agent.agent import intake_agent
from .sub_agents.market_research.agent import market_research_agent
from .sub_agents.competitor_mapping.agent import competitor_mapping_agent
from .sub_agents.gap_analysis.agent import gap_analysis_agent
from .sub_agents.strategy_advisor.agent import strategy_advisor_agent
from .sub_agents.infographic_generator.agent import infographic_generator_agent
from .sub_agents.report_generator.agent import report_generator_agent

from .config import FAST_MODEL, APP_NAME

# location_strategy_pipeline
location_strategy_pipeline = SequentialAgent(
    name="LocationStrategyPipeline",
    description="""Comprehensive retail location strategy analysis pipeline.

This agent analyzes a target location for a specific business type and produces:
1. Market research findings from live web data
2. Competitor mapping from Google Maps Places API
3. Quantitative gap analysis with zone rankings
4. Strategic recommendations with structured JSON output
5. Professional HTML executive report
6. Visual infographic summary

To use, get the following details:
- target_location: {target_location}
- business_type: {business_type}

The analysis runs automatically through all stages and produces artifacts
including JSON report, HTML report, and infographic image.
""",
    sub_agents=[
        market_research_agent,      # Part 1: Market research with search
        competitor_mapping_agent,   # Part 2A: Competitor mapping with Maps
        gap_analysis_agent,         # Part 2B: Gap analysis with code exec
        strategy_advisor_agent,     # Part 3: Strategy synthesis
        report_generator_agent,     # Part 4: HTML report generation
        infographic_generator_agent,  # Part 5: Infographic generation
    ],
)

# Root agent orchestrating the complete location strategy pipeline
root_agent = Agent(
    model=FAST_MODEL,
    name=APP_NAME,
    description='A strategic partner for retail businesses, guiding them to optimal physical locations that foster growth and profitability.',
    instruction="""Your primary role is to orchestrate the retail location analysis.
1. Start by greeting the user.
2. Check if the `TARGET_LOCATION` (Geographic area to analyze (e.g., "Indiranagar, Bangalore")) and `BUSINESS_TYPE` (Type of business (e.g., "coffee shop", "bakery", "gym")) have been provided.
3. If they are missing, **ask the user clarifying questions to get the required information.**
4. Once you have the necessary details, call the `IntakeAgent` tool to process them.
5. After the `IntakeAgent` is successful, delegate the full analysis to the `LocationStrategyPipeline`.
Your main function is to manage this workflow conversationally.""",
    sub_agents=[location_strategy_pipeline],
    tools = [AgentTool(intake_agent)], # Part 0: Parse user request
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/strategy_advisor/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Strategy Advisor Agent - Part 3 of the Location Strategy Pipeline.

This agent synthesizes all findings into actionable recommendations using
extended reasoning (thinking mode) and outputs a structured JSON report.
"""

from google.adk.agents import LlmAgent
from google.adk.planners import BuiltInPlanner
from google.genai import types
from google.genai.types import ThinkingConfig

from ...config import PRO_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...schemas import LocationIntelligenceReport
from ...callbacks import before_strategy_advisor, after_strategy_advisor


STRATEGY_ADVISOR_INSTRUCTION = """You are a senior strategy consultant synthesizing location intelligence findings.

Your task is to analyze all research and provide actionable strategic recommendations.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Available Data

### MARKET RESEARCH FINDINGS (Part 1):
{market_research_findings}

### COMPETITOR ANALYSIS (Part 2A):
{competitor_analysis}

### GAP ANALYSIS (Part 2B):
{gap_analysis}

## Your Mission
Synthesize all findings into a comprehensive strategic recommendation.

## Analysis Framework

### 1. Data Integration
Review all inputs carefully:
- Market research demographics and trends
- Competitor locations, ratings, and patterns
- Quantitative gap analysis metrics and zone rankings

### 2. Strategic Synthesis
For each promising zone, evaluate:
- Opportunity Type: Categorize (e.g., "Metro First-Mover", "Residential Sticky", "Mall Impulse")
- Overall Score: 0-100 weighted composite
- Strengths: Top 3-4 factors with evidence from the analysis
- Concerns: Top 2-3 risks with specific mitigation strategies
- Competition Profile: Summarize density, quality, chain presence
- Market Characteristics: Population, income, infrastructure, foot traffic, costs
- Best Customer Segment: Primary target demographic
- Next Steps: 3-5 specific actionable recommendations

### 3. Top Recommendation Selection
Choose the single best location based on:
- Highest weighted opportunity score
- Best balance of opportunity vs risk
- Most aligned with business type requirements
- Clear competitive advantage potential

### 4. Alternative Locations
Identify 2-3 alternative locations:
- Brief scoring and categorization
- Key strength and concern for each
- Why it's not the top choice

### 5. Strategic Insights
Provide 4-6 key insights that span the entire analysis:
- Market-level observations
- Competitive dynamics
- Timing considerations
- Success factors

## Output Requirements
Your response MUST conform to the LocationIntelligenceReport schema.
Ensure all fields are populated with specific, actionable information.
Use evidence from the analysis to support all recommendations.
"""

strategy_advisor_agent = LlmAgent(
    name="StrategyAdvisorAgent",
    model=PRO_MODEL,
    description="Synthesizes findings into strategic recommendations using extended reasoning and structured output",
    instruction=STRATEGY_ADVISOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    planner=BuiltInPlanner(
        thinking_config=ThinkingConfig(
            include_thoughts=False,  # Must be False when using output_schema
            thinking_budget=-1,  # -1 means unlimited thinking budget
        )
    ),
    output_schema=LocationIntelligenceReport,
    output_key="strategic_report",
    before_agent_callback=before_strategy_advisor,
    after_agent_callback=after_strategy_advisor,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/gap_analysis/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Gap Analysis Agent - Part 2B of the Location Strategy Pipeline.

This agent performs quantitative gap analysis using Python code execution
to calculate saturation indices, viability scores, and zone rankings.
"""

from google.adk.agents import LlmAgent
from google.adk.code_executors import BuiltInCodeExecutor
from google.genai import types

from ...config import CODE_EXEC_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...callbacks import before_gap_analysis, after_gap_analysis


GAP_ANALYSIS_INSTRUCTION = """You are a data scientist analyzing market opportunities using quantitative methods.

Your task is to perform advanced gap analysis on the data collected from previous stages.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Available Data

### MARKET RESEARCH FINDINGS (Part 1):
{market_research_findings}

### COMPETITOR ANALYSIS (Part 2):
{competitor_analysis}

## Your Mission
Write and execute Python code to perform comprehensive quantitative analysis.

## Analysis Steps

### Step 1: Parse Competitor Data
Extract from the competitor analysis:
- Competitor names and locations
- Ratings and review counts
- Zone/area classifications
- Business types (chain vs independent)

### Step 2: Extract Market Fundamentals
From the market research:
- Population estimates
- Income levels (assign numeric scores)
- Infrastructure quality indicators
- Foot traffic patterns

### Step 3: Calculate Zone Metrics
For each identified zone, compute:

**Basic Metrics:**
- Competitor count
- Competitor density (per estimated area)
- Average competitor rating
- Total review volume

**Quality Metrics:**
- Competition Quality Score: Weighted by ratings (4.5+ = high threat)
- Chain Dominance Ratio: % of chain/franchise competitors
- High Performer Count: Number of 4.5+ rated competitors

**Opportunity Metrics:**
- Demand Signal: Based on population, income, infrastructure
- Market Saturation Index: (Competitors  Quality) / Demand
- Viability Score: Multi-factor weighted score

### Step 4: Zone Categorization
Classify each zone as:
- **SATURATED**: High competition, low opportunity
- **MODERATE**: Balanced market, moderate opportunity
- **OPPORTUNITY**: Low competition, high potential

Also assign:
- Risk Level: Low / Medium / High
- Investment Tier: Based on expected costs
- Best Customer Segment: Target demographic

### Step 5: Rank Top Zones
Create a weighted ranking considering:
- Low market saturation (weight: 30%)
- High demand signals (weight: 30%)
- Low chain dominance (weight: 15%)
- Infrastructure quality (weight: 15%)
- Manageable costs (weight: 10%)

### Step 6: Output Tables
Generate clear output tables showing:
1. All zones with computed metrics
2. Top 3 recommended zones with scores
3. Risk assessment matrix

## Code Guidelines
- Use pandas for data manipulation
- Print all results clearly formatted
- Include intermediate calculations for transparency
- Handle missing data gracefully

Execute the code and provide actionable strategic recommendations based on the quantitative findings.
"""

gap_analysis_agent = LlmAgent(
    name="GapAnalysisAgent",
    model=CODE_EXEC_MODEL,
    description="Performs quantitative gap analysis using Python code execution for zone rankings and viability scores",
    instruction=GAP_ANALYSIS_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    code_executor=BuiltInCodeExecutor(),
    output_key="gap_analysis",
    before_agent_callback=before_gap_analysis,
    after_agent_callback=after_gap_analysis,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/report_generator/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Report Generator Agent - Part 4 of the Location Strategy Pipeline.

This agent generates a professional HTML executive report from the
structured LocationIntelligenceReport data using the generate_html_report tool.

The tool handles:
- Calling Gemini to generate 7-slide McKinsey/BCG style HTML
- Saving the HTML as an artifact for download in adk web
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import generate_html_report
from ...callbacks import before_report_generator, after_report_generator


REPORT_GENERATOR_INSTRUCTION = """You are an executive report generator for location intelligence analysis.

Your task is to create a professional HTML executive report using the generate_html_report tool.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Strategic Report Data
{strategic_report}

## Your Mission
Format the strategic report data and call the generate_html_report tool to create a
McKinsey/BCG-style 7-slide HTML presentation.

## Steps

### Step 1: Format the Report Data
Prepare a comprehensive data summary from the strategic report above, including:
- Analysis overview (location, business type, date, market validation)
- Top recommendation details (location, score, opportunity type, strengths, concerns)
- Competition metrics (total competitors, density, chain dominance, ratings)
- Market characteristics (population, income, infrastructure, foot traffic, rental costs)
- Alternative locations (name, score, strength, concern, why not top)
- Next steps (actionable items)
- Key insights (strategic observations)
- Methodology summary

### Step 2: Call the Tool
Call the generate_html_report tool with the formatted report data.
The tool will:
- Generate a professional 7-slide HTML report
- Save it as an artifact named "executive_report.html"
- Return the status and artifact details

### Step 3: Report Result
After the tool returns, confirm the report was generated successfully.
If there was an error, report what went wrong.
"""

report_generator_agent = LlmAgent(
    name="ReportGeneratorAgent",
    model=FAST_MODEL,
    description="Generates professional McKinsey/BCG-style HTML executive reports using the generate_html_report tool",
    instruction=REPORT_GENERATOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[generate_html_report],
    output_key="report_generation_result",
    before_agent_callback=before_report_generator,
    after_agent_callback=after_report_generator,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/market_research/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Market Research Agent - Part 1 of the Location Strategy Pipeline.

This agent validates macro market viability using live web data from Google Search.
It researches demographics, market trends, and commercial viability.
"""

from google.adk.agents import LlmAgent
from google.adk.tools import google_search
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...callbacks import before_market_research, after_market_research


MARKET_RESEARCH_INSTRUCTION = """You are a market research analyst specializing in retail location intelligence.

Your task is to research and validate the target market for a new business location.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Research Focus Areas

### 1. DEMOGRAPHICS
- Age distribution (identify key age groups)
- Income levels and purchasing power
- Lifestyle indicators (professionals, students, families)
- Population density and growth trends

### 2. MARKET GROWTH
- Population trends (growing, stable, declining)
- New residential and commercial developments
- Infrastructure improvements (metro, roads, tech parks)
- Economic growth indicators

### 3. INDUSTRY PRESENCE
- Existing similar businesses in the area
- Consumer preferences and spending patterns
- Market saturation indicators
- Success stories or failures of similar businesses

### 4. COMMERCIAL VIABILITY
- Foot traffic patterns (weekday vs weekend)
- Commercial real estate trends
- Typical rental costs (qualitative: low/medium/high)
- Business environment and regulations

## Instructions
1. Use Google Search to find current, verifiable data
2. Cite specific data points with sources where possible
3. Focus on information from the last 1-2 years for relevance
4. Be factual and data-driven, avoid speculation

## Output Format
Provide a structured analysis covering all four focus areas.
Conclude with a clear verdict: Is this a strong market for {business_type}? Why or why not?
Include specific recommendations for market entry strategy.
"""

market_research_agent = LlmAgent(
    name="MarketResearchAgent",
    model=FAST_MODEL,
    description="Researches market viability using Google Search for real-time demographics, trends, and commercial data",
    instruction=MARKET_RESEARCH_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[google_search],
    output_key="market_research_findings",
    before_agent_callback=before_market_research,
    after_agent_callback=after_market_research,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/intake_agent/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Intake Agent - Extracts target location and business type from user request.

This agent parses the user's natural language request and extracts the
required parameters (target_location, business_type) into session state
for use by subsequent agents in the pipeline.
"""

from typing import Optional

from google.adk.agents import LlmAgent
from google.adk.agents.callback_context import CallbackContext
from google.genai import types
from pydantic import BaseModel, Field

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS


class UserRequest(BaseModel):
    """Structured output for parsing user's location strategy request."""

    target_location: str = Field(
        description="The geographic location/area to analyze (e.g., 'Indiranagar, Bangalore', 'Manhattan, New York')"
    )
    business_type: str = Field(
        description="The type of business the user wants to open (e.g., 'coffee shop', 'bakery', 'gym', 'restaurant')"
    )
    additional_context: Optional[str] = Field(
        default=None,
        description="Any additional context or requirements mentioned by the user"
    )


def after_intake(callback_context: CallbackContext) -> Optional[types.Content]:
    """After intake, copy the parsed values to state for other agents."""
    parsed = callback_context.state.get("parsed_request", {})

    if isinstance(parsed, dict):
        # Extract values from parsed request
        callback_context.state["target_location"] = parsed.get("target_location", "")
        callback_context.state["business_type"] = parsed.get("business_type", "")
        callback_context.state["additional_context"] = parsed.get("additional_context", "")
    elif hasattr(parsed, "target_location"):
        # Handle Pydantic model
        callback_context.state["target_location"] = parsed.target_location
        callback_context.state["business_type"] = parsed.business_type
        callback_context.state["additional_context"] = parsed.additional_context or ""

    # Track intake stage completion
    stages = callback_context.state.get("stages_completed", [])
    stages.append("intake")
    callback_context.state["stages_completed"] = stages

    # Note: current_date is set in each agent's before_callback to ensure it's always available
    return None


INTAKE_INSTRUCTION = """You are a request parser for a retail location intelligence system.

Your task is to extract the target location and business type from the user's request.

## Examples

User: "I want to open a coffee shop in Indiranagar, Bangalore"
 target_location: "Indiranagar, Bangalore"
 business_type: "coffee shop"

User: "Analyze the market for a new gym in downtown Seattle"
 target_location: "downtown Seattle"
 business_type: "gym"

User: "Help me find the best location for a bakery in Mumbai"
 target_location: "Mumbai"
 business_type: "bakery"

User: "Where should I open my restaurant in San Francisco's Mission District?"
 target_location: "Mission District, San Francisco"
 business_type: "restaurant"

## Instructions
1. Extract the geographic location mentioned by the user
2. Identify the type of business they want to open
3. Note any additional context or requirements

If the user doesn't specify a clear location or business type, make a reasonable inference or ask for clarification.
"""

intake_agent = LlmAgent(
    name="IntakeAgent",
    model=FAST_MODEL,
    description="Parses user request to extract target location and business type",
    instruction=INTAKE_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    output_schema=UserRequest,
    output_key="parsed_request",
    after_agent_callback=after_intake,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/infographic_generator/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Infographic Generator Agent - Part 5 (Bonus) of the Location Strategy Pipeline.

This agent creates a visual infographic summary using Gemini's image generation
capabilities to provide an executive-ready visual summary of the analysis.
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import generate_infographic
from ...callbacks import before_infographic_generator, after_infographic_generator


INFOGRAPHIC_GENERATOR_INSTRUCTION = """You are a data visualization specialist creating executive-ready infographics.

Your task is to generate a visual infographic summarizing the location intelligence analysis.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Strategic Report Data
{strategic_report}

## Your Mission
Create a compelling infographic that visually summarizes the key findings from the analysis.

## Steps

### Step 1: Extract Key Data Points
From the strategic report, identify:
- Target location and business type
- Top recommended location with score
- Total competitors found
- Number of zones analyzed
- 3-5 key strategic insights
- Top strengths and concerns
- Market validation verdict

### Step 2: Create Data Summary
Compose a concise data summary suitable for visualization:

**FORMAT YOUR SUMMARY AS:**

LOCATION INTELLIGENCE REPORT: [Business Type] in [Target Location]
Analysis Date: [Date]

TOP RECOMMENDATION:
[Location Name] - Score: [XX]/100
Type: [Opportunity Type]

KEY METRICS:
- Total Competitors: [X]
- Zones Analyzed: [X]
- Market Status: [Validated/Cautionary]

TOP STRENGTHS:
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

KEY INSIGHTS:
- [Insight 1]
- [Insight 2]
- [Insight 3]

VERDICT: [One-line market recommendation]

### Step 3: Generate Infographic
Call the generate_infographic tool with your data summary.

### Step 4: Report Result
After the tool returns, store the result for the callback to process.
If successful, confirm the infographic was generated.
If failed, report the error for troubleshooting.

## Output
The generate_infographic tool will return a result dict containing:
- status: "success" or "error"
- image_data: Base64 encoded PNG (if successful)
- error_message: Error details (if failed)

Store this result so the after_agent_callback can save the artifact.
"""

infographic_generator_agent = LlmAgent(
    name="InfographicGeneratorAgent",
    model=FAST_MODEL,
    description="Generates visual infographic summary using Gemini image generation",
    instruction=INFOGRAPHIC_GENERATOR_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[generate_infographic],
    output_key="infographic_result",
    before_agent_callback=before_infographic_generator,
    after_agent_callback=after_infographic_generator,
)

```
**File: `../retail-ai-location-strategy/app/sub_agents/competitor_mapping/agent.py`**
```python
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Competitor Mapping Agent - Part 2A of the Location Strategy Pipeline.

This agent maps competitors using the Google Maps Places API to get
ground-truth data about existing businesses in the target area.
"""

from google.adk.agents import LlmAgent
from google.genai import types

from ...config import FAST_MODEL, RETRY_INITIAL_DELAY, RETRY_ATTEMPTS
from ...tools import search_places
from ...callbacks import before_competitor_mapping, after_competitor_mapping


COMPETITOR_MAPPING_INSTRUCTION = """You are a market intelligence analyst specializing in competitive landscape analysis.

Your task is to map and analyze all competitors in the target area using real Google Maps data.

TARGET LOCATION: {target_location}
BUSINESS TYPE: {business_type}
CURRENT DATE: {current_date}

## Your Mission
Use the search_places function to get REAL data from Google Maps about existing competitors.

## Step 1: Search for Competitors
Call the search_places function with queries like:
- "{business_type} near {target_location}"
- Related business types in the same area

## Step 2: Analyze the Results
For each competitor found, note:
- Business name
- Location/address
- Rating (out of 5)
- Number of reviews
- Business status (operational, etc.)

## Step 3: Identify Patterns
Analyze the competitive landscape:

### Geographic Clustering
- Are competitors clustered in specific areas/zones?
- Which areas have high concentration vs sparse presence?
- Are there any "dead zones" with no competitors?

### Location Types
- Shopping malls and retail areas
- Main roads and commercial corridors
- Residential neighborhoods
- Near transit (metro stations, bus stops)

### Quality Segmentation
- Premium tier: High-rated (4.5+), likely higher prices
- Mid-market: Ratings 4.0-4.4
- Budget tier: Lower ratings or basic offerings
- Chain vs independent businesses

## Step 4: Strategic Assessment
Provide insights on:
- Which areas appear saturated with competitors?
- Which areas might be underserved opportunities?
- What quality gaps exist (e.g., no premium options)?
- Where are the strongest competitors located?

## Output Format
Provide a detailed competitor map with:
1. List of all competitors found with their details
2. Zone-by-zone breakdown of competition
3. Pattern analysis and clustering insights
4. Strategic opportunities and saturation warnings

Be specific and reference the actual data you receive from the search_places tool.
"""

competitor_mapping_agent = LlmAgent(
    name="CompetitorMappingAgent",
    model=FAST_MODEL,
    description="Maps competitors using Google Maps Places API for ground-truth competitor data",
    instruction=COMPETITOR_MAPPING_INSTRUCTION,
    generate_content_config=types.GenerateContentConfig(
        http_options=types.HttpOptions(
            retry_options=types.HttpRetryOptions(
                initial_delay=RETRY_INITIAL_DELAY,
                attempts=RETRY_ATTEMPTS,
            ),
        ),
    ),
    tools=[search_places],
    output_key="competitor_analysis",
    before_agent_callback=before_competitor_mapping,
    after_agent_callback=after_competitor_mapping,
)

```

**4. Evaluation Questions:**
*   **Questions Evaluated:** The full set of questions used in the evaluation. This can provide context if certain types of questions are causing specific failures.
**Questions Evaluated**
```json
Questions file not found.
```

---
Format your entire response as a single Markdown document.
