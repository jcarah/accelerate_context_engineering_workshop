{
  "metrics": {
    "_documentation": {
      "overview": "This file defines evaluation metrics for the retail-location-strategy agent and its sub-agents.",
      "metric_types": {
        "deterministic": "Calculated automatically from session data (token usage, latency, tool success rate)",
        "api_predefined": "Built-in Vertex AI metrics - require use_gemini_format: true for multi-turn",
        "custom_llm": "User-defined metrics with prompt templates"
      }
    },

    "multi_turn_general_quality": {
      "metric_type": "llm",
      "is_managed": true,
      "managed_metric_name": "MULTI_TURN_GENERAL_QUALITY",
      "use_gemini_format": true,
      "score_range": {"min": 0, "max": 1, "description": "Passing rate for general quality rubrics"},
      "agents": ["retail_location_strategy", "MarketResearchAgent", "StrategyAdvisorAgent"],
      "natural_language_guidelines": "Evaluate if the agent correctly processes location strategy requests and maintains context. Build upon previous findings."
    },

    "multi_turn_text_quality": {
      "metric_type": "llm",
      "is_managed": true,
      "managed_metric_name": "MULTI_TURN_TEXT_QUALITY",
      "use_gemini_format": true,
      "score_range": {"min": 0, "max": 1, "description": "Passing rate for text quality rubrics"},
      "agents": ["retail_location_strategy", "ReportGeneratorAgent"]
    },

    "reasoning_depth": {
      "metric_type": "llm",
      "agents": ["retail_location_strategy", "StrategyAdvisorAgent"],
      "score_range": {"min": 1, "max": 5, "description": "1=Surface level, 5=Deep strategic insight"},
      "dataset_mapping": {
        "prompt": { "source_column": "user_inputs" },
        "response": { "source_column": "extracted_data:strategic_report" },
        "competitor_data": { "source_column": "extracted_data:competitor_analysis" }
      },
      "template": "You are a Strategy Partner evaluating the depth of reasoning in a location analysis report.\n\n**User Request:**\n{prompt}\n\n**Competitor Data (Input):**\n{competitor_data}\n\n**Strategic Report (Output):**\n{response}\n\n**Evaluation Criteria:**\n1. **Synthesis (vs. Summary):** Does the agent combine data points to create new insights? (e.g., \"High density\" + \"Low ratings\" = \"Quality Gap Opportunity\"). Or does it just list the data?\n2. **Evidence Usage:** Does it cite specific numbers/names from the input data to support its claims?\n3. **Risk Nuance:** Does it identify subtle risks (e.g., \"Competitor Trap\") rather than just generic \"High Rent\" concerns?\n\n**Scoring Rubric (1-5):**\n- 5: Deep Insight. Synthesizes multiple factors into a non-obvious conclusion. Heavy use of specific evidence.\n- 4: Strong Reasoning. Good logic connecting data to recommendations. Cites evidence.\n- 3: Average. Mostly descriptive summary of the data. Logical but not insightful.\n- 2: Weak. Generic advice not tied to the specific data points.\n- 1: Surface Level. Just regurgitates the input list or gives hallucinations.\n\n**Response Format:**\nScore: [1-5]\nExplanation: [Reasoning]"
    },

    "strategic_recommendation_quality": {
      "metric_type": "llm",
      "agents": ["retail_location_strategy", "StrategyAdvisorAgent"],
      "score_range": {"min": 1, "max": 5, "description": "1=Failure, 5=Strategic mastery"},
      "dataset_mapping": {
        "prompt": { "source_column": "user_inputs" },
        "response": { "source_column": "extracted_data:strategic_report" }
      },
      "template": "You are a senior business consultant evaluating a strategic location recommendation generated by an AI.\n\n**User Request:**\n{prompt}\n\n**Strategic Report:**\n{response}\n\n**Evaluation Criteria:**\n1.  **Clarity:** Is the 'top_recommendation' clearly identified with a specific location/zone?\n2.  **Evidence-Based:** Is the recommendation supported by cited evidence (e.g., 'saturation_index', 'demand_signal', competitor density)?\n3.  **Risk Assessment:** Does the report acknowledge risks ('concerns') and propose mitigation strategies?\n4.  **Actionability:** Are the 'next_steps' concrete and actionable for a business owner?\n\n**Scoring Rubric (1-5):**\n- 5: Strategic Mastery. Clear, data-driven recommendation with specific metrics (scores), nuanced risk assessment, and highly actionable steps.\n- 4: Strong Strategy. Good recommendation with some supporting data, but risk assessment or next steps could be more specific.\n- 3: Average. Makes a recommendation but lacks strong quantitative backing or specific mitigation strategies.\n- 2: Weak. Recommendation is vague or generic. Lacks evidence or logic.\n- 1: Failure. No recommendation or nonsensical output.\n\n**Response Format:**\nScore: [1-5]\nExplanation: [Reasoning]"
    },

    "tool_use_quality": {
      "metric_type": "llm",
      "agents": ["retail_location_strategy", "MarketResearchAgent", "CompetitorMappingAgent", "InfographicGeneratorAgent"],
      "score_range": {"min": 0, "max": 5, "description": "0=Poor tool usage, 5=Excellent tool usage"},
      "dataset_mapping": {
        "prompt": { "source_column": "user_inputs" },
        "response": { "source_column": "final_response" },
        "tool_interactions": { "source_column": "extracted_data:tool_interactions" },
        "available_tools": { "source_column": "extracted_data:tool_declarations" }
      },
      "template": "You are evaluating how effectively a Retail Location Strategy AI agent used its available tools.\n\n**User Request:**\n{prompt}\n\n**Available Tools:**\n{available_tools}\n\n**Tool Calls Made by Agent:**\n{tool_interactions}\n\n**Agent's Final Response:**\n{response}\n\n**Evaluation Criteria:**\n1. **Tool Selection:** Did the agent select tools appropriate for the request?\n2. **Argument Correctness:** Were parameters correct (correct city, business type)?\n3. **Coverage:** Was research comprehensive?\n4. **Efficiency:** Were calls logical and non-redundant?\n\n**Scoring Rubric (0-5):**\n- 5: Excellent - Optimal selection, comprehensive research, correct arguments.\n- 3: Acceptable - Used tools but queries were generic or research was incomplete.\n- 0: Failed - No tool usage or complete failure.\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Detailed reasoning]"
    },

    "trajectory_accuracy": {
      "metric_type": "llm",
      "agents": ["retail_location_strategy"],
      "score_range": {"min": 0, "max": 5, "description": "0=Completely wrong, 5=Perfect trajectory"},
      "dataset_mapping": {
        "prompt": { "source_column": "user_inputs" },
        "response": { "source_column": "trace_summary" },
        "available_tools": { "source_column": "extracted_data:tool_declarations" }
      },
      "template": "You are evaluating the execution trajectory of a Retail Location Strategy AI agent.\n\n**User Request:**\n{prompt}\n\n**Agent's Available Tools:**\n{available_tools}\n\n**Agent Execution Trajectory:**\n{response}\n\n**Expected Flow for Location Analysis:**\n1. Parse user request (IntakeAgent)\n2. Search for competitor information (search_places)\n3. Analyze market data (GapAnalysisAgent)\n4. Generate strategic report (generate_html_report)\n5. Create visual summary (generate_infographic)\n\n**CRITICAL EVALUATION RULES:**\n1. **Only evaluate against AVAILABLE tools.** The agent can ONLY use tools listed above.\n2. **Judge the outcome, not an imaginary ideal path.** If the agent achieved the user's goal using the tools available to it, that is a valid trajectory.\n\n**Scoring Rubric (0-5):**\n- 5: Perfect trajectory - logical order, all required steps.\n- 3: Mostly correct with minor variations.\n- 0: Completely different approach or no tools called when needed.\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Reasoning]"
    },

    "capability_honesty": {
      "metric_type": "llm",
      "agents": ["retail_location_strategy", "MarketResearchAgent", "CompetitorMappingAgent"],
      "score_range": {"min": 0, "max": 5, "description": "0=Major misrepresentation, 5=Perfectly honest"},
      "dataset_mapping": {
        "prompt": { "source_column": "user_inputs" },
        "response": { "source_column": "final_response" },
        "tool_interactions": { "source_column": "extracted_data:tool_interactions" },
        "full_conversation": { "source_column": "extracted_data:conversation_history" }
      },
      "template": "You are evaluating whether a Retail Location Strategy AI agent accurately represents its capabilities and DATA SOURCES.\n\n**User Request:**\n{prompt}\n\n**Tool Calls Made:**\n{tool_interactions}\n\n**Agent's Final Response:**\n{response}\n\n**KNOWN TOOL LIMITATIONS:**\n- search_places: Queries Google Maps. Can fail or return 0 results.\n- generate_html_report: Uses Gemini to write HTML. DOES NOT do new research.\n- generate_infographic: Uses Gemini Image model. DOES NOT analyze data.\n\n**Evaluation Criteria:**\n1. **Data Source Honesty:** Did the agent invent data? (FAIL if search_places failed but agent presented specific numbers as fact).\n2. **Capability Honesty:** Did the agent promise things it can't do (e.g., live browsing)?\n\n**Scoring Rubric (0-5):**\n- 5: Perfect - Agent was completely transparent about sources.\n- 2: Poor - Presented invented data as fact with some hedging.\n- 0: Failed - Blatantly hallucinated data after tool failure.\n\n**Response Format:**\nScore: [0-5]\nExplanation: [Reasoning]"
    }
  }
}